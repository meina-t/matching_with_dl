{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Strategy Proofness - First Order Stochastic Dominance)\n",
    "$$\n",
    "\\forall i\\in W\\cup F \\ \\forall \\succ_i \\forall \\succ_{-i} \\forall \\succ'_i \\forall j \\\\\n",
    "\\sum_{j'\\succeq j}(g_{ij'}(\\succ'_i,\\succ_{-i})-g_{ij'}(\\succ_i,\\succ_{-i})) \\leq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Ex-ante Stability)\n",
    "$\\nexists (w,f)\\in W\\times F$ s.t. $\\exist f'\\ [g_{wf'}(\\succ)>0\\land f\\succ_w f']\\ \\exist w'\\ [g_{w'f}(\\succ)>0\\land w\\succ_f w']$\n",
    "\n",
    "## (Stability of Deterministic Matching)\n",
    "$$\n",
    "\\forall (w,f)\\in W\\times F \\ g_{wf}+\\sum_{f'\\succ_w f}g_{wf'}+\\sum_{w'\\succ_f w}g_{w'f}\\geq 1\n",
    "$$\n",
    "\n",
    "## (Ex-post Stability)\n",
    "A randomized matching is **ex-post stable** iff it can be decomposed into deterministic stable matchings.\n",
    "\n",
    "## (Fractionally Stable)\n",
    "$$\n",
    "\\forall (w,f)\\in W\\times F \\ g_{wf}+\\sum_{f'\\succ_w f}g_{wf'}+\\sum_{w'\\succ_f w}g_{w'f}\\geq 1\n",
    "$$\n",
    "\n",
    "### (Violation of Fractionally Stability)\n",
    "$$\n",
    "\\sum_\\succ\\sum_w\\sum_f\\max\\left\\{0,1-g_{wf}(\\succ)-\\sum_{w'\\succ_f w}g_{w'f}(\\succ)-\\sum_{f'\\succ_w f}g_{wf'}(\\succ)\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Primal)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min & \\sum_\\succ\\sum_w\\sum_f t_{wf}(\\succ)\\\\\n",
    "    \\text{s.t.} & \\sum_f g_{wf}(\\succ)\\leq 1 & \\forall\\succ\\forall w \\\\\n",
    "    & \\sum_w g_{wf}(\\succ)\\leq 1 & \\forall \\succ\\forall f\\\\\n",
    "    & t_{wf}(\\succ)\\geq 1-g_{wf}(\\succ)-\\sum_{w'\\succ_f w}g_{w'f}(\\succ)-\\sum_{f'\\succ_w f}g_{wf'}(\\succ) & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & \\sum_{f'\\succ_wf}(g_{wf'}(\\succ_w',\\succ_{-w})-g_{wf'}(\\succ))\\leq 0 & \\forall\\succ\\forall w\\forall\\succ_{w}'\\forall f\\\\\n",
    "    & \\sum_{w'\\succ_fw}(g_{w'f}(\\succ_f',\\succ_{-f})-g_{w'f}(\\succ))\\leq 0 & \\forall\\succ\\forall f\\forall\\succ_{f}'\\forall w\\\\\n",
    "    & g_{wf}(\\succ)\\geq 0,\\ t_{wf}(\\succ)\\geq 0 & \\forall\\succ\\forall w \\forall y\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficient_loss:\n",
    "\n",
    "$M = 効率的なマッチングの集合$ とした時、そのマッチング$\\mu$におけるefficiency_lossは\n",
    "$$\n",
    "\\min_{\\nu \\in M} \\sum_{p \\in P} \\sum_{q \\in Q}  (\\sum{}\\nu())\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Dual)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min & \\sum_\\succ\\left(\\sum_wx_w(\\succ)+\\sum_fy_f(\\succ)-\\sum_w\\sum_fz_{wf}(\\succ)\\right)\\\\\n",
    "    \\text{s.t.}  \\\\\n",
    "    & \\forall \\succ \\forall w \\forall f\\\\\n",
    "    & x_w(\\succ)+y_f(\\succ)-z_{wf}(\\succ)-\\sum_{f'\\prec_wf}z_{wf'}(\\succ)-\\sum_{w'\\prec_fw}z_{w'f}(\\succ)-\\sum_{\\succ_w'}\\left(\\sum_{f'\\prec_w f}u_{wf'}(\\succ_w',\\succ_w,\\succ_{-w})-\\sum_{f'\\prec_w'f}u_{wf'}(\\succ_w,\\succ_w',\\succ_{-w})\\right)-\\sum_{\\succ_f'}\\left(\\sum_{w'\\prec_fw}v_{w'f}(\\succ_f',\\succ_f,\\succ_{-f})-\\sum_{w'\\prec_f'w}v_{w'f}(\\succ_f,\\succ_f',\\succ_{-f})\\right)\\geq 0 & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & x_w(\\succ)\\geq 0,\\ y_f(\\succ)\\geq 0,\\ 0\\leq z_{wf}(\\succ)\\leq 1 & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & u_{wf}(\\succ'_w,\\succ_w,\\succ_{-w})\\geq 0 & \\forall\\succ\\forall w\\forall\\succ_w'\\forall f\\\\\n",
    "    & v_{wf}(\\succ'_f,\\succ_f,\\succ_{-f})\\geq 0 & \\forall\\succ\\forall f\\forall\\succ_f'\\forall w\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"primal_dual_matching.ipynb\").resolve().parent.parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import Data\n",
    "\n",
    "from primal_net import PrimalNet\n",
    "from primal_loss import *\n",
    "from primal_train import *\n",
    "\n",
    "#from dual_net import DualNet\n",
    "#from dual_loss import *\n",
    "#from dual_train import *\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:10:48,755:INFO:[TRAIN-ITER]: 0, [Time-Elapsed]: 0.156508, [Total-Loss]: 0.190648\n",
      "2024-12-16 18:10:48,758:INFO:[CONSTR-Vio]: 0.000397, [OBJECTIVE]: 0.158863, [EFFICIENCY-loss]:  0.031785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t mean: 0.03971577063202858\n",
      "t max: 0.5086209774017334\n",
      "t min: 0.0\n",
      "t mean: 0.059413403272628784\n",
      "t max: 0.5074684619903564\n",
      "t min: 0.0\n",
      "t mean: 0.07906465232372284\n",
      "t max: 0.5063648819923401\n",
      "t min: 0.0\n",
      "t mean: 0.06706331670284271\n",
      "t max: 0.5052825212478638\n",
      "t min: 0.0\n",
      "t mean: 0.0629839301109314\n",
      "t max: 0.5042276978492737\n",
      "t min: 0.0\n",
      "t mean: 0.07072551548480988\n",
      "t max: 0.5031864643096924\n",
      "t min: 0.0\n",
      "t mean: 0.08233144879341125\n",
      "t max: 0.5021237730979919\n",
      "t min: 0.0\n",
      "t mean: 0.07040353864431381\n",
      "t max: 0.5010590553283691\n",
      "t min: 0.0\n",
      "t mean: 0.054647646844387054\n",
      "t max: 0.49998587369918823\n",
      "t min: 0.0\n",
      "t mean: 0.0779205858707428\n",
      "t max: 0.49889516830444336\n",
      "t min: 0.0\n",
      "t mean: 0.05443213880062103\n",
      "t max: 0.4977753758430481\n",
      "t min: 0.0\n",
      "t mean: 0.065946564078331\n",
      "t max: 0.4966467618942261\n",
      "t min: 0.0\n",
      "t mean: 0.05806644260883331\n",
      "t max: 0.49558985233306885\n",
      "t min: 0.0\n",
      "t mean: 0.06567197293043137\n",
      "t max: 0.4945549964904785\n",
      "t min: 0.0\n",
      "t mean: 0.061685770750045776\n",
      "t max: 0.4935457706451416\n",
      "t min: 0.0\n",
      "t mean: 0.05770459771156311\n",
      "t max: 0.4925050139427185\n",
      "t min: 0.0\n",
      "t mean: 0.061418626457452774\n",
      "t max: 0.49146509170532227\n",
      "t min: 0.0\n",
      "t mean: 0.049786701798439026\n",
      "t max: 0.4904014468193054\n",
      "t min: 0.0\n",
      "t mean: 0.061109509319067\n",
      "t max: 0.48899519443511963\n",
      "t min: 0.0\n",
      "t mean: 0.04954097047448158\n",
      "t max: 0.4880746603012085\n",
      "t min: 0.0\n",
      "t mean: 0.06841011345386505\n",
      "t max: 0.48682308197021484\n",
      "t min: 0.0\n",
      "t mean: 0.05306077003479004\n",
      "t max: 0.4854615330696106\n",
      "t min: 0.0\n",
      "t mean: 0.05667664855718613\n",
      "t max: 0.48402297496795654\n",
      "t min: 0.0\n",
      "t mean: 0.06026008725166321\n",
      "t max: 0.4824873208999634\n",
      "t min: 0.0\n",
      "t mean: 0.06003285571932793\n",
      "t max: 0.4804326295852661\n",
      "t min: 0.0\n",
      "t mean: 0.05608038231730461\n",
      "t max: 0.4790971279144287\n",
      "t min: 0.0\n",
      "t mean: 0.08563689142465591\n",
      "t max: 0.47707700729370117\n",
      "t min: 0.0\n",
      "t mean: 0.0667329952120781\n",
      "t max: 0.47522908449172974\n",
      "t min: 0.0\n",
      "t mean: 0.0627441257238388\n",
      "t max: 0.4731215238571167\n",
      "t min: 0.0\n",
      "t mean: 0.06611160933971405\n",
      "t max: 0.4708912968635559\n",
      "t min: 0.0\n",
      "t mean: 0.05846955627202988\n",
      "t max: 0.4684678912162781\n",
      "t min: 0.0\n",
      "t mean: 0.05448201298713684\n",
      "t max: 0.46589207649230957\n",
      "t min: 0.0\n",
      "t mean: 0.06498052924871445\n",
      "t max: 0.4631301164627075\n",
      "t min: 0.0\n",
      "t mean: 0.07170940935611725\n",
      "t max: 0.4601135849952698\n",
      "t min: 0.0\n",
      "t mean: 0.06756691634654999\n",
      "t max: 0.4567995071411133\n",
      "t min: 0.0\n",
      "t mean: 0.0459137000143528\n",
      "t max: 0.453241229057312\n",
      "t min: 0.0\n",
      "t mean: 0.059514936059713364\n",
      "t max: 0.449415922164917\n",
      "t min: 0.0\n",
      "t mean: 0.03807876259088516\n",
      "t max: 0.4434353709220886\n",
      "t min: 0.0\n",
      "t mean: 0.05485359579324722\n",
      "t max: 0.4407247304916382\n",
      "t min: 0.0\n",
      "t mean: 0.06439949572086334\n",
      "t max: 0.4358251094818115\n",
      "t min: 0.0\n",
      "t mean: 0.04684435576200485\n",
      "t max: 0.43048256635665894\n",
      "t min: 0.0\n",
      "t mean: 0.05292024463415146\n",
      "t max: 0.4248230457305908\n",
      "t min: 0.0\n",
      "t mean: 0.06174410134553909\n",
      "t max: 0.418013334274292\n",
      "t min: 0.0\n",
      "t mean: 0.04795850068330765\n",
      "t max: 0.4118881821632385\n",
      "t min: 0.0\n",
      "t mean: 0.040821027010679245\n",
      "t max: 0.40461820363998413\n",
      "t min: 0.0\n",
      "t mean: 0.06150055676698685\n",
      "t max: 0.3968127965927124\n",
      "t min: 0.0\n",
      "t mean: 0.057125162333250046\n",
      "t max: 0.38829267024993896\n",
      "t min: 0.0\n",
      "t mean: 0.04113098606467247\n",
      "t max: 0.378983736038208\n",
      "t min: 0.0\n",
      "t mean: 0.039999306201934814\n",
      "t max: 0.36892169713974\n",
      "t min: 0.0\n",
      "t mean: 0.05541745200753212\n",
      "t max: 0.3581618070602417\n",
      "t min: 0.0\n",
      "t mean: 0.04548342153429985\n",
      "t max: 0.3464844822883606\n",
      "t min: 0.0\n",
      "t mean: 0.048925984650850296\n",
      "t max: 0.33401399850845337\n",
      "t min: 0.0\n",
      "t mean: 0.042039044201374054\n",
      "t max: 0.32055628299713135\n",
      "t min: 0.0\n",
      "t mean: 0.044623102992773056\n",
      "t max: 0.3064877986907959\n",
      "t min: 0.0\n",
      "t mean: 0.04244320094585419\n",
      "t max: 0.2914109230041504\n",
      "t min: 0.0\n",
      "t mean: 0.02967006154358387\n",
      "t max: 0.2757527232170105\n",
      "t min: 0.0\n",
      "t mean: 0.033560607582330704\n",
      "t max: 0.25938791036605835\n",
      "t min: 0.0\n",
      "t mean: 0.03173030912876129\n",
      "t max: 0.2433539628982544\n",
      "t min: 0.0\n",
      "t mean: 0.02759733237326145\n",
      "t max: 0.2266753911972046\n",
      "t min: 0.0\n",
      "t mean: 0.024034395813941956\n",
      "t max: 0.21052932739257812\n",
      "t min: 0.0\n",
      "t mean: 0.02338409796357155\n",
      "t max: 0.19388318061828613\n",
      "t min: 0.0\n",
      "t mean: 0.021524982526898384\n",
      "t max: 0.177068293094635\n",
      "t min: 0.0\n",
      "t mean: 0.021854326128959656\n",
      "t max: 0.16165459156036377\n",
      "t min: 0.0\n",
      "t mean: 0.017606865614652634\n",
      "t max: 0.14587879180908203\n",
      "t min: 0.0\n",
      "t mean: 0.017635365948081017\n",
      "t max: 0.13019627332687378\n",
      "t min: 0.0\n",
      "t mean: 0.009489688090980053\n",
      "t max: 0.115092933177948\n",
      "t min: 0.0\n",
      "t mean: 0.011380195617675781\n",
      "t max: 0.10169589519500732\n",
      "t min: 0.0\n",
      "t mean: 0.01137036457657814\n",
      "t max: 0.08939659595489502\n",
      "t min: 0.0\n",
      "t mean: 0.009311487898230553\n",
      "t max: 0.07858556509017944\n",
      "t min: 0.0\n",
      "t mean: 0.007168728858232498\n",
      "t max: 0.06824320554733276\n",
      "t min: 0.0\n",
      "t mean: 0.0043969713151454926\n",
      "t max: 0.05868363380432129\n",
      "t min: 0.0\n",
      "t mean: 0.0051376610063016415\n",
      "t max: 0.050741374492645264\n",
      "t min: 0.0\n",
      "t mean: 0.005471391137689352\n",
      "t max: 0.04406905174255371\n",
      "t min: 0.0\n",
      "t mean: 0.0028571407310664654\n",
      "t max: 0.03794628381729126\n",
      "t min: 0.0\n",
      "t mean: 0.004443526268005371\n",
      "t max: 0.03254091739654541\n",
      "t min: 0.0\n",
      "t mean: 0.0032163821160793304\n",
      "t max: 0.027879714965820312\n",
      "t min: 0.0\n",
      "t mean: 0.0029632272198796272\n",
      "t max: 0.023962676525115967\n",
      "t min: 0.0\n",
      "t mean: 0.002074744552373886\n",
      "t max: 0.020699799060821533\n",
      "t min: 0.0\n",
      "t mean: 0.0019091631984338164\n",
      "t max: 0.01804417371749878\n",
      "t min: 0.0\n",
      "t mean: 0.001828446751460433\n",
      "t max: 0.015713989734649658\n",
      "t min: 0.0\n",
      "t mean: 0.0017712560947984457\n",
      "t max: 0.013677537441253662\n",
      "t min: 0.0\n",
      "t mean: 0.0011241547763347626\n",
      "t max: 0.011903166770935059\n",
      "t min: 0.0\n",
      "t mean: 0.0011959585826843977\n",
      "t max: 0.010374844074249268\n",
      "t min: 0.0\n",
      "t mean: 0.000939683523029089\n",
      "t max: 0.009057223796844482\n",
      "t min: 0.0\n",
      "t mean: 0.0007723149610683322\n",
      "t max: 0.00797957181930542\n",
      "t min: 0.0\n",
      "t mean: 0.000599011720623821\n",
      "t max: 0.007068634033203125\n",
      "t min: 0.0\n",
      "t mean: 0.0005464979913085699\n",
      "t max: 0.006371557712554932\n",
      "t min: 0.0\n",
      "t mean: 0.0006147557287476957\n",
      "t max: 0.005763053894042969\n",
      "t min: 0.0\n",
      "t mean: 0.00037101126508787274\n",
      "t max: 0.005225479602813721\n",
      "t min: 0.0\n",
      "t mean: 0.0005592911038547754\n",
      "t max: 0.0047969818115234375\n",
      "t min: 0.0\n",
      "t mean: 0.000479654292576015\n",
      "t max: 0.004445314407348633\n",
      "t min: 0.0\n",
      "t mean: 0.00038439955096691847\n",
      "t max: 0.004126608371734619\n",
      "t min: 0.0\n",
      "t mean: 0.0003118187887594104\n",
      "t max: 0.003839254379272461\n",
      "t min: 0.0\n",
      "t mean: 0.0003826008760370314\n",
      "t max: 0.0035814642906188965\n",
      "t min: 0.0\n",
      "t mean: 0.000332473311573267\n",
      "t max: 0.0033353567123413086\n",
      "t min: 0.0\n",
      "t mean: 0.000322156265610829\n",
      "t max: 0.0031377673149108887\n",
      "t min: 0.0\n",
      "t mean: 0.00030360842356458306\n",
      "t max: 0.0029474496841430664\n",
      "t min: 0.0\n",
      "t mean: 0.00031215581111609936\n",
      "t max: 0.0027640461921691895\n",
      "t min: 0.0\n",
      "t mean: 0.0003194431192241609\n",
      "t max: 0.0026160478591918945\n",
      "t min: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:10:56,638:INFO:[TRAIN-ITER]: 100, [Time-Elapsed]: 8.042199, [Total-Loss]: 0.063234\n",
      "2024-12-16 18:10:56,639:INFO:[CONSTR-Vio]: 0.000179, [OBJECTIVE]: 0.000869, [EFFICIENCY-loss]:  0.062365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t mean: 0.0002680872566998005\n",
      "t max: 0.002471327781677246\n",
      "t min: 0.0\n",
      "t mean: 0.0002172667591366917\n",
      "t max: 0.002338230609893799\n",
      "t min: 0.0\n",
      "t mean: 0.0002950053894892335\n",
      "t max: 0.0022174715995788574\n",
      "t min: 0.0\n",
      "t mean: 0.0002439409727230668\n",
      "t max: 0.0021099448204040527\n",
      "t min: 0.0\n",
      "t mean: 0.00015733008331153542\n",
      "t max: 0.0020148754119873047\n",
      "t min: 0.0\n",
      "t mean: 0.0002153737295884639\n",
      "t max: 0.0019322037696838379\n",
      "t min: 0.0\n",
      "t mean: 0.00015901087317615747\n",
      "t max: 0.001858055591583252\n",
      "t min: 0.0\n",
      "t mean: 0.00022859693854115903\n",
      "t max: 0.0017929673194885254\n",
      "t min: 0.0\n",
      "t mean: 0.00013565225526690483\n",
      "t max: 0.0017338395118713379\n",
      "t min: 0.0\n",
      "t mean: 0.0001314126857323572\n",
      "t max: 0.0016760826110839844\n",
      "t min: 0.0\n",
      "t mean: 0.00014890951570123434\n",
      "t max: 0.0016353130340576172\n",
      "t min: 0.0\n",
      "t mean: 0.0001905540411826223\n",
      "t max: 0.001588582992553711\n",
      "t min: 0.0\n",
      "t mean: 0.0002245039213448763\n",
      "t max: 0.001553177833557129\n",
      "t min: 0.0\n",
      "t mean: 0.00016272012726403773\n",
      "t max: 0.001512765884399414\n",
      "t min: 0.0\n",
      "t mean: 0.00013765272160526365\n",
      "t max: 0.0014731287956237793\n",
      "t min: 0.0\n",
      "t mean: 0.00016201226389966905\n",
      "t max: 0.0014351606369018555\n",
      "t min: 0.0\n",
      "t mean: 0.00010759325232356787\n",
      "t max: 0.0013983845710754395\n",
      "t min: 0.0\n",
      "t mean: 0.0001303634635405615\n",
      "t max: 0.0013631582260131836\n",
      "t min: 0.0\n",
      "t mean: 0.00010731720976764336\n",
      "t max: 0.0012356042861938477\n",
      "t min: 0.0\n",
      "t mean: 0.00017289514653384686\n",
      "t max: 0.001297593116760254\n",
      "t min: 0.0\n",
      "t mean: 0.00013810786185786128\n",
      "t max: 0.0012668371200561523\n",
      "t min: 0.0\n",
      "t mean: 0.00018256899784319103\n",
      "t max: 0.0012359023094177246\n",
      "t min: 0.0\n",
      "t mean: 0.00011579803685890511\n",
      "t max: 0.0012142062187194824\n",
      "t min: 0.0\n",
      "t mean: 8.457359217572957e-05\n",
      "t max: 0.0011914372444152832\n",
      "t min: 0.0\n",
      "t mean: 0.00013001312618143857\n",
      "t max: 0.0011709332466125488\n",
      "t min: 0.0\n",
      "t mean: 0.00010941951768472791\n",
      "t max: 0.00115203857421875\n",
      "t min: 0.0\n",
      "t mean: 0.00016665553266648203\n",
      "t max: 0.0011342763900756836\n",
      "t min: 0.0\n",
      "t mean: 0.0001282038283534348\n",
      "t max: 0.0011161565780639648\n",
      "t min: 0.0\n",
      "t mean: 0.00013779733853880316\n",
      "t max: 0.0010977387428283691\n",
      "t min: 0.0\n",
      "t mean: 9.77174931904301e-05\n",
      "t max: 0.0010787248611450195\n",
      "t min: 0.0\n",
      "t mean: 0.00012349561438895762\n",
      "t max: 0.0010600686073303223\n",
      "t min: 0.0\n",
      "t mean: 0.00012508723011706024\n",
      "t max: 0.0010413527488708496\n",
      "t min: 0.0\n",
      "t mean: 8.10992787592113e-05\n",
      "t max: 0.0010238289833068848\n",
      "t min: 0.0\n",
      "t mean: 0.00013685936573892832\n",
      "t max: 0.0010080337524414062\n",
      "t min: 0.0\n",
      "t mean: 0.00012437003897503018\n",
      "t max: 0.0009927153587341309\n",
      "t min: 0.0\n",
      "t mean: 0.00011167626507813111\n",
      "t max: 0.0009784102439880371\n",
      "t min: 0.0\n",
      "t mean: 0.00010222932905890048\n",
      "t max: 0.0009644627571105957\n",
      "t min: 0.0\n",
      "t mean: 0.00010625629511196166\n",
      "t max: 0.0009502172470092773\n",
      "t min: 0.0\n",
      "t mean: 0.00014280842151492834\n",
      "t max: 0.0009354352951049805\n",
      "t min: 0.0\n",
      "t mean: 0.0001086443880922161\n",
      "t max: 0.000920712947845459\n",
      "t min: 0.0\n",
      "t mean: 0.00010125843982677907\n",
      "t max: 0.000906825065612793\n",
      "t min: 0.0\n",
      "t mean: 4.1388775571249425e-05\n",
      "t max: 0.0008347630500793457\n",
      "t min: 0.0\n",
      "t mean: 0.00010826670768437907\n",
      "t max: 0.000882267951965332\n",
      "t min: 0.0\n",
      "t mean: 9.384597069583833e-05\n",
      "t max: 0.0008710622787475586\n",
      "t min: 0.0\n",
      "t mean: 9.649340790929273e-05\n",
      "t max: 0.0008592009544372559\n",
      "t min: 0.0\n",
      "t mean: 0.00010636598744895309\n",
      "t max: 0.0008469820022583008\n",
      "t min: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m PrimalNet(cfg)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_primal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_train.py:92\u001b[0m, in \u001b[0;36mtrain_primal\u001b[0;34m(cfg, G, model, include_truncation)\u001b[0m\n\u001b[1;32m     89\u001b[0m p, q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(P)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mTensor(Q)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m r \u001b[38;5;241m=\u001b[39m model(p, q)\n\u001b[0;32m---> 92\u001b[0m loss, constr_vio, obj, efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (i\u001b[38;5;241m%\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlagr_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     94\u001b[0m     lambd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_loss.py:79\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(cfg, model, r, p, q, lambd, rho)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt min:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 79\u001b[0m efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficiency_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m lambd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(lambd)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;241m+\u001b[39m (constr_vio\u001b[38;5;241m*\u001b[39mlambd)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m efficiency_loss \u001b[38;5;66;03m# 3項目は大きいものを強く抑制するため\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:106\u001b[0m, in \u001b[0;36mcompute_efficiency_loss\u001b[0;34m(cfg, r, p, q)\u001b[0m\n\u001b[1;32m    103\u001b[0m     p_batch \u001b[38;5;241m=\u001b[39m p[batch_idx]\n\u001b[1;32m    104\u001b[0m     q_batch \u001b[38;5;241m=\u001b[39m q[batch_idx]\n\u001b[0;32m--> 106\u001b[0m     stable_matchings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_stable_matchings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     efficient_matchings \u001b[38;5;241m=\u001b[39m filter_efficient_stable_matchings(stable_matchings, p_batch, q_batch)\n\u001b[1;32m    109\u001b[0m batch_efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:23\u001b[0m, in \u001b[0;36mgenerate_stable_matchings\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# ブロッキングペアの条件をチェック\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m---> 23\u001b[0m             \u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_matching\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_matching\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (q[possible_matching[j], i] \u001b[38;5;241m>\u001b[39m q[possible_matching[j], j])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m         ):\n\u001b[1;32m     26\u001b[0m             is_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"mps\" #if torch.cuda.is_available() else \"cpu\"\n",
    "lambd = np.ones((2,2))*0.001\n",
    "# lambd = cfg.lambd\n",
    "\n",
    "cfg = HParams(num_agents = 2,\n",
    "              device = device,\n",
    "              lambd = lambd,\n",
    "              rho = 0.1,\n",
    "              lagr_iter = 10,\n",
    "              batch_size = 32,\n",
    "              epochs=10000)\n",
    "\n",
    "cfg.lr = 1e-4\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "G = Data(cfg)\n",
    "\n",
    "model = PrimalNet(cfg)\n",
    "model.to(device)\n",
    "\n",
    "train_primal(cfg,G,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[1, 0.0], [0, 1]]).to(device)\n",
    "q = torch.tensor([[1, 0.0], [0, 1]]).to(device)\n",
    "\n",
    "output = model(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5070, 0.4930],\n",
       "         [0.4929, 0.5071]]], device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3* 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" #if torch.cuda.is_available() else \"cpu\"\n",
    "lambd = np.ones((3,3))*0.001\n",
    "# lambd = cfg.lambd\n",
    "\n",
    "cfg = HParams(num_agents = 3,\n",
    "              device = device,\n",
    "              lambd = lambd,\n",
    "              rho = 0.1,\n",
    "              lagr_iter = 100,\n",
    "              batch_size = 128,\n",
    "              epochs = 10000)\n",
    "\n",
    "cfg.lr = 1e-4\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "G = Data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimalNet(\n",
       "  (input_block): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (layer_out): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PrimalNet(cfg)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 17:46:09,713:INFO:[TRAIN-ITER]: 0, [Time-Elapsed]: 1.896664, [Total-Loss]: 1.567277\n",
      "2024-12-21 17:46:09,715:INFO:[CONSTR-Vio]: 0.001671, [OBJECTIVE]: 1.561960, [EFFICIENCY-loss]:  0.005315\n",
      "2024-12-21 17:48:14,275:INFO:[TRAIN-ITER]: 100, [Time-Elapsed]: 126.458519, [Total-Loss]: 1.379439\n",
      "2024-12-21 17:48:14,278:INFO:[CONSTR-Vio]: 0.072068, [OBJECTIVE]: 1.377446, [EFFICIENCY-loss]:  0.001889\n",
      "2024-12-21 17:50:25,142:INFO:[TRAIN-ITER]: 200, [Time-Elapsed]: 257.325461, [Total-Loss]: 0.788619\n",
      "2024-12-21 17:50:25,144:INFO:[CONSTR-Vio]: 0.391810, [OBJECTIVE]: 0.775084, [EFFICIENCY-loss]:  0.011954\n",
      "2024-12-21 17:52:43,101:INFO:[TRAIN-ITER]: 300, [Time-Elapsed]: 395.284120, [Total-Loss]: 0.648144\n",
      "2024-12-21 17:52:43,102:INFO:[CONSTR-Vio]: 0.393012, [OBJECTIVE]: 0.659801, [EFFICIENCY-loss]:  -0.014967\n",
      "2024-12-21 17:54:49,689:INFO:[TRAIN-ITER]: 400, [Time-Elapsed]: 521.872136, [Total-Loss]: 0.720561\n",
      "2024-12-21 17:54:49,691:INFO:[CONSTR-Vio]: 0.443050, [OBJECTIVE]: 0.714589, [EFFICIENCY-loss]:  0.000039\n",
      "2024-12-21 17:57:01,078:INFO:[TRAIN-ITER]: 500, [Time-Elapsed]: 653.261190, [Total-Loss]: 0.596156\n",
      "2024-12-21 17:57:01,080:INFO:[CONSTR-Vio]: 0.467631, [OBJECTIVE]: 0.617917, [EFFICIENCY-loss]:  -0.030204\n",
      "2024-12-21 17:59:19,281:INFO:[TRAIN-ITER]: 600, [Time-Elapsed]: 791.463978, [Total-Loss]: 0.578421\n",
      "2024-12-21 17:59:19,282:INFO:[CONSTR-Vio]: 0.534824, [OBJECTIVE]: 0.571282, [EFFICIENCY-loss]:  -0.005617\n",
      "2024-12-21 18:01:37,812:INFO:[TRAIN-ITER]: 700, [Time-Elapsed]: 929.995319, [Total-Loss]: 0.556544\n",
      "2024-12-21 18:01:37,814:INFO:[CONSTR-Vio]: 0.510844, [OBJECTIVE]: 0.587916, [EFFICIENCY-loss]:  -0.046415\n",
      "2024-12-21 18:03:56,690:INFO:[TRAIN-ITER]: 800, [Time-Elapsed]: 1068.873324, [Total-Loss]: 0.444701\n",
      "2024-12-21 18:03:56,692:INFO:[CONSTR-Vio]: 0.418122, [OBJECTIVE]: 0.500203, [EFFICIENCY-loss]:  -0.070049\n",
      "2024-12-21 18:06:16,920:INFO:[TRAIN-ITER]: 900, [Time-Elapsed]: 1209.103750, [Total-Loss]: 0.601920\n",
      "2024-12-21 18:06:16,921:INFO:[CONSTR-Vio]: 0.535429, [OBJECTIVE]: 0.606016, [EFFICIENCY-loss]:  -0.025583\n",
      "2024-12-21 18:08:34,948:INFO:[TRAIN-ITER]: 1000, [Time-Elapsed]: 1347.131500, [Total-Loss]: 0.537920\n",
      "2024-12-21 18:08:34,949:INFO:[CONSTR-Vio]: 0.443964, [OBJECTIVE]: 0.564659, [EFFICIENCY-loss]:  -0.046941\n",
      "2024-12-21 18:10:42,502:INFO:[TRAIN-ITER]: 1100, [Time-Elapsed]: 1474.684777, [Total-Loss]: 0.544973\n",
      "2024-12-21 18:10:42,504:INFO:[CONSTR-Vio]: 0.516460, [OBJECTIVE]: 0.518334, [EFFICIENCY-loss]:  0.000523\n",
      "2024-12-21 18:12:48,579:INFO:[TRAIN-ITER]: 1200, [Time-Elapsed]: 1600.761697, [Total-Loss]: 0.480776\n",
      "2024-12-21 18:12:48,581:INFO:[CONSTR-Vio]: 0.402877, [OBJECTIVE]: 0.444695, [EFFICIENCY-loss]:  0.013452\n",
      "2024-12-21 18:14:58,315:INFO:[TRAIN-ITER]: 1300, [Time-Elapsed]: 1730.498068, [Total-Loss]: 0.598751\n",
      "2024-12-21 18:14:58,316:INFO:[CONSTR-Vio]: 0.458016, [OBJECTIVE]: 0.570854, [EFFICIENCY-loss]:  0.000050\n",
      "2024-12-21 18:17:10,635:INFO:[TRAIN-ITER]: 1400, [Time-Elapsed]: 1862.818758, [Total-Loss]: 0.443920\n",
      "2024-12-21 18:17:10,637:INFO:[CONSTR-Vio]: 0.418282, [OBJECTIVE]: 0.418038, [EFFICIENCY-loss]:  -0.001470\n",
      "2024-12-21 18:19:15,425:INFO:[TRAIN-ITER]: 1500, [Time-Elapsed]: 1987.607677, [Total-Loss]: 0.424340\n",
      "2024-12-21 18:19:15,426:INFO:[CONSTR-Vio]: 0.411398, [OBJECTIVE]: 0.391884, [EFFICIENCY-loss]:  0.003775\n",
      "2024-12-21 18:21:20,750:INFO:[TRAIN-ITER]: 1600, [Time-Elapsed]: 2112.932893, [Total-Loss]: 0.434341\n",
      "2024-12-21 18:21:20,752:INFO:[CONSTR-Vio]: 0.408796, [OBJECTIVE]: 0.450700, [EFFICIENCY-loss]:  -0.046705\n",
      "2024-12-21 18:23:25,102:INFO:[TRAIN-ITER]: 1700, [Time-Elapsed]: 2237.285094, [Total-Loss]: 0.547130\n",
      "2024-12-21 18:23:25,104:INFO:[CONSTR-Vio]: 0.478882, [OBJECTIVE]: 0.508530, [EFFICIENCY-loss]:  0.000452\n",
      "2024-12-21 18:26:50,422:INFO:[TRAIN-ITER]: 1800, [Time-Elapsed]: 2442.605040, [Total-Loss]: 0.522563\n",
      "2024-12-21 18:26:50,424:INFO:[CONSTR-Vio]: 0.480386, [OBJECTIVE]: 0.457022, [EFFICIENCY-loss]:  0.024958\n",
      "2024-12-21 18:29:08,593:INFO:[TRAIN-ITER]: 1900, [Time-Elapsed]: 2580.776567, [Total-Loss]: 0.447222\n",
      "2024-12-21 18:29:08,594:INFO:[CONSTR-Vio]: 0.369351, [OBJECTIVE]: 0.414299, [EFFICIENCY-loss]:  -0.000101\n",
      "2024-12-21 18:31:20,981:INFO:[TRAIN-ITER]: 2000, [Time-Elapsed]: 2713.163938, [Total-Loss]: 0.480052\n",
      "2024-12-21 18:31:20,982:INFO:[CONSTR-Vio]: 0.425419, [OBJECTIVE]: 0.470559, [EFFICIENCY-loss]:  -0.030798\n",
      "2024-12-21 18:33:27,881:INFO:[TRAIN-ITER]: 2100, [Time-Elapsed]: 2840.063854, [Total-Loss]: 0.458801\n",
      "2024-12-21 18:33:27,882:INFO:[CONSTR-Vio]: 0.393032, [OBJECTIVE]: 0.435326, [EFFICIENCY-loss]:  -0.015609\n",
      "2024-12-21 18:35:33,343:INFO:[TRAIN-ITER]: 2200, [Time-Elapsed]: 2965.526142, [Total-Loss]: 0.387916\n",
      "2024-12-21 18:35:33,345:INFO:[CONSTR-Vio]: 0.360277, [OBJECTIVE]: 0.350162, [EFFICIENCY-loss]:  0.000652\n",
      "2024-12-21 18:37:40,050:INFO:[TRAIN-ITER]: 2300, [Time-Elapsed]: 3092.232964, [Total-Loss]: 0.447454\n",
      "2024-12-21 18:37:40,051:INFO:[CONSTR-Vio]: 0.389235, [OBJECTIVE]: 0.421235, [EFFICIENCY-loss]:  -0.015639\n",
      "2024-12-21 18:39:46,755:INFO:[TRAIN-ITER]: 2400, [Time-Elapsed]: 3218.937915, [Total-Loss]: 0.450244\n",
      "2024-12-21 18:39:46,756:INFO:[CONSTR-Vio]: 0.374154, [OBJECTIVE]: 0.408573, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 18:41:52,980:INFO:[TRAIN-ITER]: 2500, [Time-Elapsed]: 3345.163411, [Total-Loss]: 0.320915\n",
      "2024-12-21 18:41:52,981:INFO:[CONSTR-Vio]: 0.336624, [OBJECTIVE]: 0.336360, [EFFICIENCY-loss]:  -0.054561\n",
      "2024-12-21 18:43:58,799:INFO:[TRAIN-ITER]: 2600, [Time-Elapsed]: 3470.982049, [Total-Loss]: 0.366883\n",
      "2024-12-21 18:43:58,801:INFO:[CONSTR-Vio]: 0.354646, [OBJECTIVE]: 0.402638, [EFFICIENCY-loss]:  -0.078115\n",
      "2024-12-21 18:46:29,217:INFO:[TRAIN-ITER]: 2700, [Time-Elapsed]: 3621.400441, [Total-Loss]: 0.408031\n",
      "2024-12-21 18:46:29,219:INFO:[CONSTR-Vio]: 0.379131, [OBJECTIVE]: 0.361073, [EFFICIENCY-loss]:  0.000009\n",
      "2024-12-21 19:03:59,225:INFO:[TRAIN-ITER]: 2800, [Time-Elapsed]: 4671.408030, [Total-Loss]: 0.335358\n",
      "2024-12-21 19:03:59,227:INFO:[CONSTR-Vio]: 0.338910, [OBJECTIVE]: 0.291412, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 19:36:48,543:INFO:[TRAIN-ITER]: 2900, [Time-Elapsed]: 6640.726898, [Total-Loss]: 0.368907\n",
      "2024-12-21 19:36:48,545:INFO:[CONSTR-Vio]: 0.315902, [OBJECTIVE]: 0.332446, [EFFICIENCY-loss]:  -0.004222\n",
      "2024-12-21 19:41:52,568:INFO:[TRAIN-ITER]: 3000, [Time-Elapsed]: 6944.751627, [Total-Loss]: 0.362537\n",
      "2024-12-21 19:41:52,570:INFO:[CONSTR-Vio]: 0.485767, [OBJECTIVE]: 0.346573, [EFFICIENCY-loss]:  -0.049673\n",
      "2024-12-21 19:44:11,720:INFO:[TRAIN-ITER]: 3100, [Time-Elapsed]: 7083.904641, [Total-Loss]: 0.287175\n",
      "2024-12-21 19:44:11,721:INFO:[CONSTR-Vio]: 0.316880, [OBJECTIVE]: 0.313015, [EFFICIENCY-loss]:  -0.070075\n",
      "2024-12-21 19:46:27,684:INFO:[TRAIN-ITER]: 3200, [Time-Elapsed]: 7219.866977, [Total-Loss]: 0.321342\n",
      "2024-12-21 19:46:27,685:INFO:[CONSTR-Vio]: 0.332017, [OBJECTIVE]: 0.297395, [EFFICIENCY-loss]:  -0.023710\n",
      "2024-12-21 19:48:40,464:INFO:[TRAIN-ITER]: 3300, [Time-Elapsed]: 7352.648032, [Total-Loss]: 0.386962\n",
      "2024-12-21 19:48:40,467:INFO:[CONSTR-Vio]: 0.264003, [OBJECTIVE]: 0.348396, [EFFICIENCY-loss]:  -0.000225\n",
      "2024-12-21 19:50:57,026:INFO:[TRAIN-ITER]: 3400, [Time-Elapsed]: 7489.209026, [Total-Loss]: 0.319858\n",
      "2024-12-21 19:50:57,027:INFO:[CONSTR-Vio]: 0.340054, [OBJECTIVE]: 0.268777, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 19:53:14,276:INFO:[TRAIN-ITER]: 3500, [Time-Elapsed]: 7626.459251, [Total-Loss]: 0.334170\n",
      "2024-12-21 19:53:14,278:INFO:[CONSTR-Vio]: 0.335209, [OBJECTIVE]: 0.298389, [EFFICIENCY-loss]:  -0.015624\n",
      "2024-12-21 19:55:32,725:INFO:[TRAIN-ITER]: 3600, [Time-Elapsed]: 7764.908053, [Total-Loss]: 0.382074\n",
      "2024-12-21 19:55:32,727:INFO:[CONSTR-Vio]: 0.328059, [OBJECTIVE]: 0.310741, [EFFICIENCY-loss]:  0.019348\n",
      "2024-12-21 19:57:50,308:INFO:[TRAIN-ITER]: 3700, [Time-Elapsed]: 7902.491732, [Total-Loss]: 0.344970\n",
      "2024-12-21 19:57:50,310:INFO:[CONSTR-Vio]: 0.291475, [OBJECTIVE]: 0.298452, [EFFICIENCY-loss]:  -0.000024\n",
      "2024-12-21 20:00:08,330:INFO:[TRAIN-ITER]: 3800, [Time-Elapsed]: 8040.513945, [Total-Loss]: 0.309148\n",
      "2024-12-21 20:00:08,332:INFO:[CONSTR-Vio]: 0.302298, [OBJECTIVE]: 0.274416, [EFFICIENCY-loss]:  -0.014997\n",
      "2024-12-21 20:06:48,048:INFO:[TRAIN-ITER]: 3900, [Time-Elapsed]: 8440.231562, [Total-Loss]: 0.378080\n",
      "2024-12-21 20:06:48,051:INFO:[CONSTR-Vio]: 0.335284, [OBJECTIVE]: 0.346300, [EFFICIENCY-loss]:  -0.024803\n",
      "2024-12-21 20:09:06,525:INFO:[TRAIN-ITER]: 4000, [Time-Elapsed]: 8578.709475, [Total-Loss]: 0.389435\n",
      "2024-12-21 20:09:06,527:INFO:[CONSTR-Vio]: 0.332994, [OBJECTIVE]: 0.332140, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 20:11:23,505:INFO:[TRAIN-ITER]: 4100, [Time-Elapsed]: 8715.688728, [Total-Loss]: 0.372461\n",
      "2024-12-21 20:11:23,507:INFO:[CONSTR-Vio]: 0.293512, [OBJECTIVE]: 0.352964, [EFFICIENCY-loss]:  -0.031250\n",
      "2024-12-21 20:13:41,753:INFO:[TRAIN-ITER]: 4200, [Time-Elapsed]: 8853.937113, [Total-Loss]: 0.342791\n",
      "2024-12-21 20:13:41,756:INFO:[CONSTR-Vio]: 0.319785, [OBJECTIVE]: 0.284796, [EFFICIENCY-loss]:  -0.000133\n",
      "2024-12-21 20:15:59,532:INFO:[TRAIN-ITER]: 4300, [Time-Elapsed]: 8991.715271, [Total-Loss]: 0.381146\n",
      "2024-12-21 20:15:59,534:INFO:[CONSTR-Vio]: 0.251223, [OBJECTIVE]: 0.336069, [EFFICIENCY-loss]:  0.000001\n",
      "2024-12-21 20:18:17,679:INFO:[TRAIN-ITER]: 4400, [Time-Elapsed]: 9129.861956, [Total-Loss]: 0.309233\n",
      "2024-12-21 20:18:17,681:INFO:[CONSTR-Vio]: 0.255942, [OBJECTIVE]: 0.269764, [EFFICIENCY-loss]:  -0.007798\n",
      "2024-12-21 20:20:36,964:INFO:[TRAIN-ITER]: 4500, [Time-Elapsed]: 9269.146712, [Total-Loss]: 0.378090\n",
      "2024-12-21 20:20:36,965:INFO:[CONSTR-Vio]: 0.322610, [OBJECTIVE]: 0.357446, [EFFICIENCY-loss]:  -0.040252\n",
      "2024-12-21 20:22:55,496:INFO:[TRAIN-ITER]: 4600, [Time-Elapsed]: 9407.679369, [Total-Loss]: 0.362025\n",
      "2024-12-21 20:22:55,498:INFO:[CONSTR-Vio]: 0.310783, [OBJECTIVE]: 0.295539, [EFFICIENCY-loss]:  0.007443\n",
      "2024-12-21 20:25:13,945:INFO:[TRAIN-ITER]: 4700, [Time-Elapsed]: 9546.128204, [Total-Loss]: 0.342200\n",
      "2024-12-21 20:25:13,947:INFO:[CONSTR-Vio]: 0.275551, [OBJECTIVE]: 0.280786, [EFFICIENCY-loss]:  0.007808\n",
      "2024-12-21 20:43:17,851:INFO:[TRAIN-ITER]: 4800, [Time-Elapsed]: 10630.035066, [Total-Loss]: 0.384139\n",
      "2024-12-21 20:43:17,853:INFO:[CONSTR-Vio]: 0.268601, [OBJECTIVE]: 0.339721, [EFFICIENCY-loss]:  -0.007959\n",
      "2024-12-21 20:45:21,566:INFO:[TRAIN-ITER]: 4900, [Time-Elapsed]: 10753.749201, [Total-Loss]: 0.330883\n",
      "2024-12-21 20:45:21,568:INFO:[CONSTR-Vio]: 0.295712, [OBJECTIVE]: 0.295352, [EFFICIENCY-loss]:  -0.023594\n",
      "2024-12-21 20:47:25,537:INFO:[TRAIN-ITER]: 5000, [Time-Elapsed]: 10877.720681, [Total-Loss]: 0.344213\n",
      "2024-12-21 20:47:25,538:INFO:[CONSTR-Vio]: 0.265810, [OBJECTIVE]: 0.290582, [EFFICIENCY-loss]:  -0.000007\n",
      "2024-12-21 20:49:30,240:INFO:[TRAIN-ITER]: 5100, [Time-Elapsed]: 11002.423143, [Total-Loss]: 0.339744\n",
      "2024-12-21 20:49:30,242:INFO:[CONSTR-Vio]: 0.302872, [OBJECTIVE]: 0.285281, [EFFICIENCY-loss]:  -0.008090\n",
      "2024-12-21 20:51:34,207:INFO:[TRAIN-ITER]: 5200, [Time-Elapsed]: 11126.391659, [Total-Loss]: 0.336692\n",
      "2024-12-21 20:51:34,209:INFO:[CONSTR-Vio]: 0.228863, [OBJECTIVE]: 0.320125, [EFFICIENCY-loss]:  -0.031297\n",
      "2024-12-21 20:53:38,842:INFO:[TRAIN-ITER]: 5300, [Time-Elapsed]: 11251.024619, [Total-Loss]: 0.348594\n",
      "2024-12-21 20:53:38,844:INFO:[CONSTR-Vio]: 0.275047, [OBJECTIVE]: 0.290956, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-21 20:55:42,549:INFO:[TRAIN-ITER]: 5400, [Time-Elapsed]: 11374.731527, [Total-Loss]: 0.428488\n",
      "2024-12-21 20:55:42,550:INFO:[CONSTR-Vio]: 0.349452, [OBJECTIVE]: 0.352835, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-21 20:57:46,209:INFO:[TRAIN-ITER]: 5500, [Time-Elapsed]: 11498.392252, [Total-Loss]: 0.351006\n",
      "2024-12-21 20:57:46,211:INFO:[CONSTR-Vio]: 0.295463, [OBJECTIVE]: 0.301918, [EFFICIENCY-loss]:  -0.015625\n",
      "2024-12-21 20:59:56,246:INFO:[TRAIN-ITER]: 5600, [Time-Elapsed]: 11628.429111, [Total-Loss]: 0.288299\n",
      "2024-12-21 20:59:56,248:INFO:[CONSTR-Vio]: 0.293146, [OBJECTIVE]: 0.238751, [EFFICIENCY-loss]:  -0.015645\n",
      "2024-12-21 21:02:15,440:INFO:[TRAIN-ITER]: 5700, [Time-Elapsed]: 11767.622791, [Total-Loss]: 0.342443\n",
      "2024-12-21 21:02:15,441:INFO:[CONSTR-Vio]: 0.263253, [OBJECTIVE]: 0.282761, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 21:04:34,685:INFO:[TRAIN-ITER]: 5800, [Time-Elapsed]: 11906.868469, [Total-Loss]: 0.356493\n",
      "2024-12-21 21:04:34,687:INFO:[CONSTR-Vio]: 0.305877, [OBJECTIVE]: 0.296384, [EFFICIENCY-loss]:  -0.009625\n",
      "2024-12-21 21:06:52,662:INFO:[TRAIN-ITER]: 5900, [Time-Elapsed]: 12044.845212, [Total-Loss]: 0.311274\n",
      "2024-12-21 21:06:52,664:INFO:[CONSTR-Vio]: 0.274715, [OBJECTIVE]: 0.247529, [EFFICIENCY-loss]:  -0.000001\n",
      "2024-12-21 21:09:11,293:INFO:[TRAIN-ITER]: 6000, [Time-Elapsed]: 12183.476416, [Total-Loss]: 0.305230\n",
      "2024-12-21 21:09:11,295:INFO:[CONSTR-Vio]: 0.171358, [OBJECTIVE]: 0.264693, [EFFICIENCY-loss]:  0.000099\n",
      "2024-12-21 21:11:27,047:INFO:[TRAIN-ITER]: 6100, [Time-Elapsed]: 12319.230060, [Total-Loss]: 0.308063\n",
      "2024-12-21 21:11:27,049:INFO:[CONSTR-Vio]: 0.188258, [OBJECTIVE]: 0.263803, [EFFICIENCY-loss]:  0.000004\n",
      "2024-12-21 21:13:44,816:INFO:[TRAIN-ITER]: 6200, [Time-Elapsed]: 12456.999239, [Total-Loss]: 0.277836\n",
      "2024-12-21 21:13:44,818:INFO:[CONSTR-Vio]: 0.281222, [OBJECTIVE]: 0.210221, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-21 21:16:02,767:INFO:[TRAIN-ITER]: 6300, [Time-Elapsed]: 12594.949822, [Total-Loss]: 0.258898\n",
      "2024-12-21 21:16:02,769:INFO:[CONSTR-Vio]: 0.260932, [OBJECTIVE]: 0.195364, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-21 21:18:20,882:INFO:[TRAIN-ITER]: 6400, [Time-Elapsed]: 12733.065652, [Total-Loss]: 0.390223\n",
      "2024-12-21 21:18:20,883:INFO:[CONSTR-Vio]: 0.266632, [OBJECTIVE]: 0.325386, [EFFICIENCY-loss]:  -0.000001\n",
      "2024-12-21 21:20:38,375:INFO:[TRAIN-ITER]: 6500, [Time-Elapsed]: 12870.558183, [Total-Loss]: 0.398506\n",
      "2024-12-21 21:20:38,377:INFO:[CONSTR-Vio]: 0.248674, [OBJECTIVE]: 0.336135, [EFFICIENCY-loss]:  -0.000001\n",
      "2024-12-21 21:22:57,065:INFO:[TRAIN-ITER]: 6600, [Time-Elapsed]: 13009.248200, [Total-Loss]: 0.356892\n",
      "2024-12-21 21:22:57,067:INFO:[CONSTR-Vio]: 0.271371, [OBJECTIVE]: 0.313823, [EFFICIENCY-loss]:  -0.025299\n",
      "2024-12-21 21:38:56,247:INFO:[TRAIN-ITER]: 6700, [Time-Elapsed]: 13968.428499, [Total-Loss]: 0.346215\n",
      "2024-12-21 21:38:56,249:INFO:[CONSTR-Vio]: 0.211529, [OBJECTIVE]: 0.292711, [EFFICIENCY-loss]:  0.000001\n",
      "2024-12-21 21:41:14,290:INFO:[TRAIN-ITER]: 6800, [Time-Elapsed]: 14106.473513, [Total-Loss]: 0.309373\n",
      "2024-12-21 21:41:14,292:INFO:[CONSTR-Vio]: 0.170336, [OBJECTIVE]: 0.265729, [EFFICIENCY-loss]:  0.000018\n",
      "2024-12-21 21:43:32,556:INFO:[TRAIN-ITER]: 6900, [Time-Elapsed]: 14244.739177, [Total-Loss]: 0.282028\n",
      "2024-12-21 21:43:32,558:INFO:[CONSTR-Vio]: 0.260602, [OBJECTIVE]: 0.222278, [EFFICIENCY-loss]:  -0.007824\n",
      "2024-12-21 21:45:51,216:INFO:[TRAIN-ITER]: 7000, [Time-Elapsed]: 14383.400180, [Total-Loss]: 0.310798\n",
      "2024-12-21 21:45:51,219:INFO:[CONSTR-Vio]: 0.239280, [OBJECTIVE]: 0.259587, [EFFICIENCY-loss]:  -0.011117\n",
      "2024-12-21 21:48:09,084:INFO:[TRAIN-ITER]: 7100, [Time-Elapsed]: 14521.267104, [Total-Loss]: 0.340187\n",
      "2024-12-21 21:48:09,086:INFO:[CONSTR-Vio]: 0.297774, [OBJECTIVE]: 0.308182, [EFFICIENCY-loss]:  -0.046875\n",
      "2024-12-21 21:50:27,112:INFO:[TRAIN-ITER]: 7200, [Time-Elapsed]: 14659.295072, [Total-Loss]: 0.299452\n",
      "2024-12-21 21:50:27,114:INFO:[CONSTR-Vio]: 0.208764, [OBJECTIVE]: 0.244100, [EFFICIENCY-loss]:  -0.000027\n",
      "2024-12-21 21:52:45,509:INFO:[TRAIN-ITER]: 7300, [Time-Elapsed]: 14797.692425, [Total-Loss]: 0.373545\n",
      "2024-12-21 21:52:45,512:INFO:[CONSTR-Vio]: 0.272452, [OBJECTIVE]: 0.331450, [EFFICIENCY-loss]:  -0.031249\n",
      "2024-12-21 21:55:02,917:INFO:[TRAIN-ITER]: 7400, [Time-Elapsed]: 14935.100745, [Total-Loss]: 0.293179\n",
      "2024-12-21 21:55:02,919:INFO:[CONSTR-Vio]: 0.212870, [OBJECTIVE]: 0.255290, [EFFICIENCY-loss]:  -0.020398\n",
      "2024-12-21 21:57:21,153:INFO:[TRAIN-ITER]: 7500, [Time-Elapsed]: 15073.337016, [Total-Loss]: 0.350520\n",
      "2024-12-21 21:57:21,155:INFO:[CONSTR-Vio]: 0.226841, [OBJECTIVE]: 0.288069, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-21 21:59:38,655:INFO:[TRAIN-ITER]: 7600, [Time-Elapsed]: 15210.838150, [Total-Loss]: 0.238330\n",
      "2024-12-21 21:59:38,657:INFO:[CONSTR-Vio]: 0.213535, [OBJECTIVE]: 0.202673, [EFFICIENCY-loss]:  -0.023434\n",
      "2024-12-21 22:27:15,193:INFO:[TRAIN-ITER]: 7700, [Time-Elapsed]: 16867.377439, [Total-Loss]: 0.301749\n",
      "2024-12-21 22:27:15,197:INFO:[CONSTR-Vio]: 0.204789, [OBJECTIVE]: 0.244484, [EFFICIENCY-loss]:  -0.000005\n",
      "2024-12-21 22:29:27,868:INFO:[TRAIN-ITER]: 7800, [Time-Elapsed]: 17000.051921, [Total-Loss]: 0.397929\n",
      "2024-12-21 22:29:27,869:INFO:[CONSTR-Vio]: 0.236065, [OBJECTIVE]: 0.315732, [EFFICIENCY-loss]:  0.015624\n",
      "2024-12-21 22:48:01,551:INFO:[TRAIN-ITER]: 7900, [Time-Elapsed]: 18113.735570, [Total-Loss]: 0.436412\n",
      "2024-12-21 22:48:01,554:INFO:[CONSTR-Vio]: 0.267449, [OBJECTIVE]: 0.360635, [EFFICIENCY-loss]:  0.000040\n",
      "2024-12-21 22:50:15,498:INFO:[TRAIN-ITER]: 8000, [Time-Elapsed]: 18247.682204, [Total-Loss]: 0.330614\n",
      "2024-12-21 22:50:15,499:INFO:[CONSTR-Vio]: 0.224257, [OBJECTIVE]: 0.281517, [EFFICIENCY-loss]:  -0.015464\n",
      "2024-12-21 23:07:54,311:INFO:[TRAIN-ITER]: 8100, [Time-Elapsed]: 19306.495477, [Total-Loss]: 0.322375\n",
      "2024-12-21 23:07:54,313:INFO:[CONSTR-Vio]: 0.135577, [OBJECTIVE]: 0.283165, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-21 23:25:33,561:INFO:[TRAIN-ITER]: 8200, [Time-Elapsed]: 20365.744369, [Total-Loss]: 0.295936\n",
      "2024-12-21 23:25:33,563:INFO:[CONSTR-Vio]: 0.147367, [OBJECTIVE]: 0.299781, [EFFICIENCY-loss]:  -0.046825\n",
      "2024-12-21 23:27:48,193:INFO:[TRAIN-ITER]: 8300, [Time-Elapsed]: 20500.376982, [Total-Loss]: 0.319018\n",
      "2024-12-21 23:27:48,195:INFO:[CONSTR-Vio]: 0.231737, [OBJECTIVE]: 0.267317, [EFFICIENCY-loss]:  -0.015625\n",
      "2024-12-21 23:45:14,129:INFO:[TRAIN-ITER]: 8400, [Time-Elapsed]: 21546.313588, [Total-Loss]: 0.300995\n",
      "2024-12-21 23:45:14,132:INFO:[CONSTR-Vio]: 0.221924, [OBJECTIVE]: 0.236020, [EFFICIENCY-loss]:  0.000004\n",
      "2024-12-22 00:02:51,264:INFO:[TRAIN-ITER]: 8500, [Time-Elapsed]: 22603.448023, [Total-Loss]: 0.364756\n",
      "2024-12-22 00:02:51,265:INFO:[CONSTR-Vio]: 0.212878, [OBJECTIVE]: 0.301235, [EFFICIENCY-loss]:  -0.000004\n",
      "2024-12-22 00:20:28,969:INFO:[TRAIN-ITER]: 8600, [Time-Elapsed]: 23661.152834, [Total-Loss]: 0.383000\n",
      "2024-12-22 00:20:28,970:INFO:[CONSTR-Vio]: 0.280092, [OBJECTIVE]: 0.299229, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-22 00:38:07,497:INFO:[TRAIN-ITER]: 8700, [Time-Elapsed]: 24719.681613, [Total-Loss]: 0.339641\n",
      "2024-12-22 00:38:07,500:INFO:[CONSTR-Vio]: 0.207210, [OBJECTIVE]: 0.308051, [EFFICIENCY-loss]:  -0.031230\n",
      "2024-12-22 00:55:23,905:INFO:[TRAIN-ITER]: 8800, [Time-Elapsed]: 25756.088820, [Total-Loss]: 0.278999\n",
      "2024-12-22 00:55:23,907:INFO:[CONSTR-Vio]: 0.243797, [OBJECTIVE]: 0.267270, [EFFICIENCY-loss]:  -0.062500\n",
      "2024-12-22 01:00:36,391:INFO:[TRAIN-ITER]: 8900, [Time-Elapsed]: 26068.574988, [Total-Loss]: 0.184286\n",
      "2024-12-22 01:00:36,394:INFO:[CONSTR-Vio]: 0.125111, [OBJECTIVE]: 0.223450, [EFFICIENCY-loss]:  -0.078116\n",
      "2024-12-22 01:02:54,331:INFO:[TRAIN-ITER]: 9000, [Time-Elapsed]: 26206.514888, [Total-Loss]: 0.325097\n",
      "2024-12-22 01:02:54,332:INFO:[CONSTR-Vio]: 0.218583, [OBJECTIVE]: 0.257570, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-22 01:05:12,283:INFO:[TRAIN-ITER]: 9100, [Time-Elapsed]: 26344.466215, [Total-Loss]: 0.255078\n",
      "2024-12-22 01:05:12,284:INFO:[CONSTR-Vio]: 0.174505, [OBJECTIVE]: 0.200711, [EFFICIENCY-loss]:  -0.000007\n",
      "2024-12-22 01:07:29,421:INFO:[TRAIN-ITER]: 9200, [Time-Elapsed]: 26481.604714, [Total-Loss]: 0.340672\n",
      "2024-12-22 01:07:29,423:INFO:[CONSTR-Vio]: 0.216875, [OBJECTIVE]: 0.272435, [EFFICIENCY-loss]:  -0.000000\n",
      "2024-12-22 01:09:48,463:INFO:[TRAIN-ITER]: 9300, [Time-Elapsed]: 26620.646889, [Total-Loss]: 0.349617\n",
      "2024-12-22 01:09:48,465:INFO:[CONSTR-Vio]: 0.178636, [OBJECTIVE]: 0.316339, [EFFICIENCY-loss]:  -0.023437\n",
      "2024-12-22 01:12:08,233:INFO:[TRAIN-ITER]: 9400, [Time-Elapsed]: 26760.416404, [Total-Loss]: 0.422571\n",
      "2024-12-22 01:12:08,234:INFO:[CONSTR-Vio]: 0.156734, [OBJECTIVE]: 0.372342, [EFFICIENCY-loss]:  0.000000\n",
      "2024-12-22 01:14:26,962:INFO:[TRAIN-ITER]: 9500, [Time-Elapsed]: 26899.145403, [Total-Loss]: 0.317939\n",
      "2024-12-22 01:14:26,964:INFO:[CONSTR-Vio]: 0.149321, [OBJECTIVE]: 0.309663, [EFFICIENCY-loss]:  -0.039062\n",
      "2024-12-22 01:16:44,855:INFO:[TRAIN-ITER]: 9600, [Time-Elapsed]: 27037.038316, [Total-Loss]: 0.275900\n",
      "2024-12-22 01:16:44,856:INFO:[CONSTR-Vio]: 0.240717, [OBJECTIVE]: 0.206208, [EFFICIENCY-loss]:  -0.007812\n",
      "2024-12-22 01:19:03,027:INFO:[TRAIN-ITER]: 9700, [Time-Elapsed]: 27175.210322, [Total-Loss]: 0.227034\n",
      "2024-12-22 01:19:03,029:INFO:[CONSTR-Vio]: 0.137099, [OBJECTIVE]: 0.237463, [EFFICIENCY-loss]:  -0.054687\n",
      "2024-12-22 01:21:44,852:INFO:[TRAIN-ITER]: 9800, [Time-Elapsed]: 27337.035168, [Total-Loss]: 0.317725\n",
      "2024-12-22 01:21:44,860:INFO:[CONSTR-Vio]: 0.118134, [OBJECTIVE]: 0.286479, [EFFICIENCY-loss]:  -0.007760\n",
      "2024-12-22 01:54:48,083:INFO:[TRAIN-ITER]: 9900, [Time-Elapsed]: 29320.266744, [Total-Loss]: 0.356983\n",
      "2024-12-22 01:54:48,084:INFO:[CONSTR-Vio]: 0.268781, [OBJECTIVE]: 0.292423, [EFFICIENCY-loss]:  -0.023437\n",
      "2024-12-22 01:57:00,337:INFO:[TRAIN-ITER]: 9999, [Time-Elapsed]: 29452.520974, [Total-Loss]: 0.284739\n",
      "2024-12-22 01:57:00,339:INFO:[CONSTR-Vio]: 0.148974, [OBJECTIVE]: 0.241818, [EFFICIENCY-loss]:  -0.006635\n",
      "2024-12-22 01:57:00,339:INFO:train ended!\n"
     ]
    }
   ],
   "source": [
    "train_primal(cfg,G,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/sh8zstl54dl_t2y55d7th57m0000gn/T/ipykernel_6108/452310846.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_s.load_state_dict(torch.load('model_state_dict.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrimalNet(\n",
       "  (input_block): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (layer_out): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 空のモデルインスタンスを作成\n",
    "model_s = PrimalNet(cfg)  # モデルクラスを再定義する必要があります\n",
    "model_s.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "model_s.to(device)\n",
    "model_s.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matching(p, q, match):\n",
    "    # Move tensor to CPU and convert to NumPy\n",
    "    output_matrix = match.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    annotations = np.empty_like(output_matrix, dtype=object)\n",
    "    for i in range(output_matrix.shape[0]):\n",
    "        for j in range(output_matrix.shape[1]):\n",
    "            annotations[i, j] = f'{output_matrix[i, j]:.2e}\\n[{p[i, j]}, {q[j, i]}]'\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(output_matrix, annot=annotations, fmt='', cmap='Blues', cbar=True)\n",
    "    plt.title(\"Agent Relationship Heatmap with Vector Details\")\n",
    "    plt.xlabel(\"Agent\")\n",
    "    plt.ylabel(\"Agent\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[1, 0.0, 0], [0, 1, 0], [0, 0, 1]]).to(device)\n",
    "q = torch.tensor([[1, 0.0, 0], [0, 1, 0], [0, 0, 1]]).to(device)\n",
    "\n",
    "output = model(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]], device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching(p, q, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primal_loss import compute_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficiency_loss import compute_efficiency_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import da_with_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[[0.6, 0.3, 1], [1, 0.5, 0], [0.3, 0, 1]]]).to(device)\n",
    "q = torch.tensor([[[0.5, 1.0, 0], [0, 1, 0.5], [0, 0.5, 1.0]]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([p]).to(device)\n",
    "q = torch.tensor([q]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 1., 0.]]], device='mps:0')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_with_t(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(p, q)\n",
    "model_efficiency_loss = compute_efficiency_loss(cfg, model_output, p, q)\n",
    "model_stability_loss = compute_t(model_output, p, q).mean()\n",
    "model_sp_loss = compute_spv_w(cfg, model, model_output, p, q).mean()\n",
    "\n",
    "da_output = da_with_t(p, q)\n",
    "da_efficiency_loss = compute_efficiency_loss(cfg, da_output, p, q)\n",
    "da_stability_loss = compute_t(da_output, p, q).mean()\n",
    "da_sp_loss = compute_spv_w(cfg, da_with_t, da_output, p, q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:tensor([[[0.5000, 0.1667, 0.3333],\n",
      "         [0.3333, 0.5000, 0.1667],\n",
      "         [0.3333, 0.5000, 0.1667]]], device='mps:0')\n",
      "q:tensor([[[0.4000, 0.2000, 0.4000],\n",
      "         [0.4000, 0.2000, 0.4000],\n",
      "         [0.4000, 0.4000, 0.2000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[9.3796e-01, 1.4272e-05, 6.2023e-02],\n",
      "         [6.7026e-02, 1.3577e-03, 9.3162e-01],\n",
      "         [4.1644e-04, 9.9931e-01, 2.7459e-04]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: -3.0134499073028564\n",
      "  Stability Loss: 0.19931083917617798\n",
      "  SP Loss: 0.05522657558321953\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[1., 0., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 1., 0.]]], device='mps:0')\n",
      "  Efficiency Loss: -3.0\n",
      "  Stability Loss: 0.20000000298023224\n",
      "  SP Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"p:{p}\")\n",
    "print(f\"q:{q}\")\n",
    "print(\"\\nModel Results:\")\n",
    "print(f\"  Output: {model_output}\")\n",
    "print(f\"  Efficiency Loss: {model_efficiency_loss}\")\n",
    "print(f\"  Stability Loss: {model_stability_loss}\")\n",
    "print(f\"  SP Loss: {model_sp_loss}\")\n",
    "\n",
    "print(\"\\nDA Results:\")\n",
    "print(f\"  Output: {da_output}\")\n",
    "print(f\"  Efficiency Loss: {da_efficiency_loss}\")\n",
    "print(f\"  Stability Loss: {da_stability_loss}\")\n",
    "print(f\"  SP Loss: {da_sp_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patterns_2_1\n",
    "受け手の選好は必ず二つの同等に好ましい提案者と、好ましくない一人の提案者となる\n",
    "\n",
    "提案側の選考はランダムな強選好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tie_2_1 = list(set(itertools.permutations([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(314)\n",
    "\n",
    "one_tie_2_1 = normalize_tuples(one_tie_2_1)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(one_tie_2_1, repeat=3))\n",
    "pairs = [(random.choice(preference_list_p), random.choice(preference_list_q)) for _ in range(1000)]\n",
    "df_1 = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.4, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     p  \\\n",
       "0    ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "1    ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "2    ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "3    ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "4    ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "..                                                 ...   \n",
       "995  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "996  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "997  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "998  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "999  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "\n",
       "                                                     q  \n",
       "0    ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...  \n",
       "1    ((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...  \n",
       "2    ((0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.4, 0.4, ...  \n",
       "3    ((0.4, 0.2, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...  \n",
       "4    ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...  \n",
       "..                                                 ...  \n",
       "995  ((0.2, 0.4, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...  \n",
       "996  ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.2, ...  \n",
       "997  ((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...  \n",
       "998  ((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...  \n",
       "999  ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [01:18<00:00, 12.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df_1 = apply_features(cfg, model, df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(try_convert)\n"
     ]
    }
   ],
   "source": [
    "df_1 = convert_to_float(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.120458</td>\n",
       "      <td>0.150120</td>\n",
       "      <td>2.889323e-02</td>\n",
       "      <td>-2.890750</td>\n",
       "      <td>0.170672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.827894</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>5.687820e-02</td>\n",
       "      <td>2.767406</td>\n",
       "      <td>0.101627</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.009266</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3.152915e-10</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.849229</td>\n",
       "      <td>0.089006</td>\n",
       "      <td>1.290993e-04</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.986352</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>2.615902e-03</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.345496</td>\n",
       "      <td>0.202257</td>\n",
       "      <td>3.310132e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.921623</td>\n",
       "      <td>0.377654</td>\n",
       "      <td>4.392600e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000           1000.000000   1.000000e+03   \n",
       "mean               -3.120458              0.150120   2.889323e-02   \n",
       "std                 2.827894              0.084341   5.687820e-02   \n",
       "min               -12.009266              0.000004   3.152915e-10   \n",
       "25%                -4.849229              0.089006   1.290993e-04   \n",
       "50%                -2.986352              0.140315   2.615902e-03   \n",
       "75%                -0.345496              0.202257   3.310132e-02   \n",
       "max                 2.921623              0.377654   4.392600e-01   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss  da_sp_loss  \n",
       "count         1000.000000        1000.000000      1000.0  \n",
       "mean            -2.890750           0.170672         0.0  \n",
       "std              2.767406           0.101627         0.0  \n",
       "min            -12.000000           0.000000         0.0  \n",
       "25%             -4.000000           0.088889         0.0  \n",
       "50%             -2.000000           0.177778         0.0  \n",
       "75%              0.000000           0.244444         0.0  \n",
       "max              0.000000           0.450000         0.0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df_1['p'][0]\n",
    "q = df_1['q'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patterns2-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tie_2_2 = list(set(itertools.permutations([2, 1, 1])))\n",
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "one_tie_2_2 = normalize_tuples(one_tie_2_2)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(one_tie_2_2, repeat=3))\n",
    "pairs = [(choice(preference_list_p), choice(preference_list_q)) for _ in range(1000)]\n",
    "df = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [01:08<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "df = apply_features(cfg, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_float(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.158346</td>\n",
       "      <td>1.516044e-01</td>\n",
       "      <td>3.757371e-02</td>\n",
       "      <td>-1.318750</td>\n",
       "      <td>0.166285</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.997332</td>\n",
       "      <td>8.634095e-02</td>\n",
       "      <td>7.788224e-02</td>\n",
       "      <td>1.993975</td>\n",
       "      <td>0.102561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.999909</td>\n",
       "      <td>5.121932e-07</td>\n",
       "      <td>8.617967e-10</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.998424</td>\n",
       "      <td>8.359600e-02</td>\n",
       "      <td>9.802159e-05</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.050492</td>\n",
       "      <td>1.664441e-01</td>\n",
       "      <td>1.639004e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000711</td>\n",
       "      <td>2.133432e-01</td>\n",
       "      <td>3.409407e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.836687</td>\n",
       "      <td>3.742378e-01</td>\n",
       "      <td>6.098632e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000          1.000000e+03   1.000000e+03   \n",
       "mean               -1.158346          1.516044e-01   3.757371e-02   \n",
       "std                 1.997332          8.634095e-02   7.788224e-02   \n",
       "min                -8.999909          5.121932e-07   8.617967e-10   \n",
       "25%                -1.998424          8.359600e-02   9.802159e-05   \n",
       "50%                -0.050492          1.664441e-01   1.639004e-03   \n",
       "75%                 0.000711          2.133432e-01   3.409407e-02   \n",
       "max                 4.836687          3.742378e-01   6.098632e-01   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss  da_sp_loss  \n",
       "count         1000.000000        1000.000000      1000.0  \n",
       "mean            -1.318750           0.166285         0.0  \n",
       "std              1.993975           0.102561         0.0  \n",
       "min             -9.000000           0.000000         0.0  \n",
       "25%             -2.000000           0.083333         0.0  \n",
       "50%              0.000000           0.166667         0.0  \n",
       "75%              0.000000           0.243056         0.0  \n",
       "max              1.000000           0.472222         0.0  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>-5.998079</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>-1.991758</td>\n",
       "      <td>0.083701</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>-5.755070</td>\n",
       "      <td>0.143429</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>-6.00</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>-0.003272</td>\n",
       "      <td>0.110758</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.25, 0.5, 0.25), (0.5, 0...</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.113442</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   p  \\\n",
       "0  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "1  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "2  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "3  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "4  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "\n",
       "                                                   q  model_efficiency_loss  \\\n",
       "0  ((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...              -5.998079   \n",
       "1  ((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...              -1.991758   \n",
       "2  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...              -5.755070   \n",
       "3  ((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...              -0.003272   \n",
       "4  ((0.25, 0.5, 0.25), (0.25, 0.5, 0.25), (0.5, 0...               0.008054   \n",
       "\n",
       "   model_stability_loss  model_sp_loss  da_efficiency_loss  da_stability_loss  \\\n",
       "0              0.083392       0.000075               -3.75           0.208333   \n",
       "1              0.083701       0.000215               -2.00           0.083333   \n",
       "2              0.143429       0.011078               -6.00           0.138889   \n",
       "3              0.110758       0.000056                0.00           0.111111   \n",
       "4              0.113442       0.000245                0.00           0.111111   \n",
       "\n",
       "   da_sp_loss  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indifferent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tie_pattern = [(1, 1, 1)]\n",
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))\n",
    "\n",
    "full_tie_pattern = normalize_tuples(full_tie_pattern)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(full_tie_pattern, repeat=3))\n",
    "pairs = [(random.choice(preference_list_p), random.choice(preference_list_q)) for _ in range(1000)]\n",
    "df = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [02:19<00:00,  7.16it/s]\n"
     ]
    }
   ],
   "source": [
    "df = apply_features(cfg, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_float(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.528690</td>\n",
       "      <td>0.129895</td>\n",
       "      <td>1.817858e-02</td>\n",
       "      <td>-8.700500</td>\n",
       "      <td>0.147889</td>\n",
       "      <td>0.018444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.291469</td>\n",
       "      <td>0.087515</td>\n",
       "      <td>2.884684e-02</td>\n",
       "      <td>4.221537</td>\n",
       "      <td>0.095334</td>\n",
       "      <td>0.034344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-15.997853</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>9.082012e-09</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-11.999037</td>\n",
       "      <td>0.109991</td>\n",
       "      <td>1.448882e-04</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-10.647566</td>\n",
       "      <td>0.112440</td>\n",
       "      <td>2.478159e-03</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-7.835713</td>\n",
       "      <td>0.219065</td>\n",
       "      <td>2.620166e-02</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.044759</td>\n",
       "      <td>0.333414</td>\n",
       "      <td>1.645250e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000           1000.000000   1.000000e+03   \n",
       "mean               -9.528690              0.129895   1.817858e-02   \n",
       "std                 4.291469              0.087515   2.884684e-02   \n",
       "min               -15.997853              0.000018   9.082012e-09   \n",
       "25%               -11.999037              0.109991   1.448882e-04   \n",
       "50%               -10.647566              0.112440   2.478159e-03   \n",
       "75%                -7.835713              0.219065   2.620166e-02   \n",
       "max                 0.044759              0.333414   1.645250e-01   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss   da_sp_loss  \n",
       "count         1000.000000        1000.000000  1000.000000  \n",
       "mean            -8.700500           0.147889     0.018444  \n",
       "std              4.221537           0.095334     0.034344  \n",
       "min            -16.000000           0.000000     0.000000  \n",
       "25%            -12.000000           0.111111     0.000000  \n",
       "50%             -9.000000           0.166667     0.000000  \n",
       "75%             -6.000000           0.222222     0.055556  \n",
       "max              0.000000           0.333333     0.111111  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df['p'][7]\n",
    "q = df['q'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
