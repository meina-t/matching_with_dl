{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Strategy Proofness - First Order Stochastic Dominance)\n",
    "$$\n",
    "\\forall i\\in W\\cup F \\ \\forall \\succ_i \\forall \\succ_{-i} \\forall \\succ'_i \\forall j \\\\\n",
    "\\sum_{j'\\succeq j}(g_{ij'}(\\succ'_i,\\succ_{-i})-g_{ij'}(\\succ_i,\\succ_{-i})) \\leq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Ex-ante Stability)\n",
    "$\\nexists (w,f)\\in W\\times F$ s.t. $\\exist f'\\ [g_{wf'}(\\succ)>0\\land f\\succ_w f']\\ \\exist w'\\ [g_{w'f}(\\succ)>0\\land w\\succ_f w']$\n",
    "\n",
    "## (Stability of Deterministic Matching)\n",
    "$$\n",
    "\\forall (w,f)\\in W\\times F \\ g_{wf}+\\sum_{f'\\succ_w f}g_{wf'}+\\sum_{w'\\succ_f w}g_{w'f}\\geq 1\n",
    "$$\n",
    "\n",
    "## (Ex-post Stability)\n",
    "A randomized matching is **ex-post stable** iff it can be decomposed into deterministic stable matchings.\n",
    "\n",
    "## (Fractionally Stable)\n",
    "$$\n",
    "\\forall (w,f)\\in W\\times F \\ g_{wf}+\\sum_{f'\\succ_w f}g_{wf'}+\\sum_{w'\\succ_f w}g_{w'f}\\geq 1\n",
    "$$\n",
    "\n",
    "### (Violation of Fractionally Stability)\n",
    "$$\n",
    "\\sum_\\succ\\sum_w\\sum_f\\max\\left\\{0,1-g_{wf}(\\succ)-\\sum_{w'\\succ_f w}g_{w'f}(\\succ)-\\sum_{f'\\succ_w f}g_{wf'}(\\succ)\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Primal)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min & \\sum_\\succ\\sum_w\\sum_f t_{wf}(\\succ)\\\\\n",
    "    \\text{s.t.} & \\sum_f g_{wf}(\\succ)\\leq 1 & \\forall\\succ\\forall w \\\\\n",
    "    & \\sum_w g_{wf}(\\succ)\\leq 1 & \\forall \\succ\\forall f\\\\\n",
    "    & t_{wf}(\\succ)\\geq 1-g_{wf}(\\succ)-\\sum_{w'\\succ_f w}g_{w'f}(\\succ)-\\sum_{f'\\succ_w f}g_{wf'}(\\succ) & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & \\sum_{f'\\succ_wf}(g_{wf'}(\\succ_w',\\succ_{-w})-g_{wf'}(\\succ))\\leq 0 & \\forall\\succ\\forall w\\forall\\succ_{w}'\\forall f\\\\\n",
    "    & \\sum_{w'\\succ_fw}(g_{w'f}(\\succ_f',\\succ_{-f})-g_{w'f}(\\succ))\\leq 0 & \\forall\\succ\\forall f\\forall\\succ_{f}'\\forall w\\\\\n",
    "    & g_{wf}(\\succ)\\geq 0,\\ t_{wf}(\\succ)\\geq 0 & \\forall\\succ\\forall w \\forall y\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficient_loss:\n",
    "\n",
    "$M = 効率的なマッチングの集合$ とした時、そのマッチング$\\mu$におけるefficiency_lossは\n",
    "$$\n",
    "\\min_{\\nu \\in M} \\sum_{p \\in P} \\sum_{q \\in Q}  (\\sum{}\\nu())\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Dual)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min & \\sum_\\succ\\left(\\sum_wx_w(\\succ)+\\sum_fy_f(\\succ)-\\sum_w\\sum_fz_{wf}(\\succ)\\right)\\\\\n",
    "    \\text{s.t.}  \\\\\n",
    "    & \\forall \\succ \\forall w \\forall f\\\\\n",
    "    & x_w(\\succ)+y_f(\\succ)-z_{wf}(\\succ)-\\sum_{f'\\prec_wf}z_{wf'}(\\succ)-\\sum_{w'\\prec_fw}z_{w'f}(\\succ)-\\sum_{\\succ_w'}\\left(\\sum_{f'\\prec_w f}u_{wf'}(\\succ_w',\\succ_w,\\succ_{-w})-\\sum_{f'\\prec_w'f}u_{wf'}(\\succ_w,\\succ_w',\\succ_{-w})\\right)-\\sum_{\\succ_f'}\\left(\\sum_{w'\\prec_fw}v_{w'f}(\\succ_f',\\succ_f,\\succ_{-f})-\\sum_{w'\\prec_f'w}v_{w'f}(\\succ_f,\\succ_f',\\succ_{-f})\\right)\\geq 0 & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & x_w(\\succ)\\geq 0,\\ y_f(\\succ)\\geq 0,\\ 0\\leq z_{wf}(\\succ)\\leq 1 & \\forall\\succ\\forall w\\forall f\\\\\n",
    "    & u_{wf}(\\succ'_w,\\succ_w,\\succ_{-w})\\geq 0 & \\forall\\succ\\forall w\\forall\\succ_w'\\forall f\\\\\n",
    "    & v_{wf}(\\succ'_f,\\succ_f,\\succ_{-f})\\geq 0 & \\forall\\succ\\forall f\\forall\\succ_f'\\forall w\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"primal_dual_matching.ipynb\").resolve().parent.parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import Data\n",
    "\n",
    "from primal_net import PrimalNet\n",
    "from primal_loss import *\n",
    "from primal_train import *\n",
    "\n",
    "#from dual_net import DualNet\n",
    "#from dual_loss import *\n",
    "#from dual_train import *\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:10:48,755:INFO:[TRAIN-ITER]: 0, [Time-Elapsed]: 0.156508, [Total-Loss]: 0.190648\n",
      "2024-12-16 18:10:48,758:INFO:[CONSTR-Vio]: 0.000397, [OBJECTIVE]: 0.158863, [EFFICIENCY-loss]:  0.031785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t mean: 0.03971577063202858\n",
      "t max: 0.5086209774017334\n",
      "t min: 0.0\n",
      "t mean: 0.059413403272628784\n",
      "t max: 0.5074684619903564\n",
      "t min: 0.0\n",
      "t mean: 0.07906465232372284\n",
      "t max: 0.5063648819923401\n",
      "t min: 0.0\n",
      "t mean: 0.06706331670284271\n",
      "t max: 0.5052825212478638\n",
      "t min: 0.0\n",
      "t mean: 0.0629839301109314\n",
      "t max: 0.5042276978492737\n",
      "t min: 0.0\n",
      "t mean: 0.07072551548480988\n",
      "t max: 0.5031864643096924\n",
      "t min: 0.0\n",
      "t mean: 0.08233144879341125\n",
      "t max: 0.5021237730979919\n",
      "t min: 0.0\n",
      "t mean: 0.07040353864431381\n",
      "t max: 0.5010590553283691\n",
      "t min: 0.0\n",
      "t mean: 0.054647646844387054\n",
      "t max: 0.49998587369918823\n",
      "t min: 0.0\n",
      "t mean: 0.0779205858707428\n",
      "t max: 0.49889516830444336\n",
      "t min: 0.0\n",
      "t mean: 0.05443213880062103\n",
      "t max: 0.4977753758430481\n",
      "t min: 0.0\n",
      "t mean: 0.065946564078331\n",
      "t max: 0.4966467618942261\n",
      "t min: 0.0\n",
      "t mean: 0.05806644260883331\n",
      "t max: 0.49558985233306885\n",
      "t min: 0.0\n",
      "t mean: 0.06567197293043137\n",
      "t max: 0.4945549964904785\n",
      "t min: 0.0\n",
      "t mean: 0.061685770750045776\n",
      "t max: 0.4935457706451416\n",
      "t min: 0.0\n",
      "t mean: 0.05770459771156311\n",
      "t max: 0.4925050139427185\n",
      "t min: 0.0\n",
      "t mean: 0.061418626457452774\n",
      "t max: 0.49146509170532227\n",
      "t min: 0.0\n",
      "t mean: 0.049786701798439026\n",
      "t max: 0.4904014468193054\n",
      "t min: 0.0\n",
      "t mean: 0.061109509319067\n",
      "t max: 0.48899519443511963\n",
      "t min: 0.0\n",
      "t mean: 0.04954097047448158\n",
      "t max: 0.4880746603012085\n",
      "t min: 0.0\n",
      "t mean: 0.06841011345386505\n",
      "t max: 0.48682308197021484\n",
      "t min: 0.0\n",
      "t mean: 0.05306077003479004\n",
      "t max: 0.4854615330696106\n",
      "t min: 0.0\n",
      "t mean: 0.05667664855718613\n",
      "t max: 0.48402297496795654\n",
      "t min: 0.0\n",
      "t mean: 0.06026008725166321\n",
      "t max: 0.4824873208999634\n",
      "t min: 0.0\n",
      "t mean: 0.06003285571932793\n",
      "t max: 0.4804326295852661\n",
      "t min: 0.0\n",
      "t mean: 0.05608038231730461\n",
      "t max: 0.4790971279144287\n",
      "t min: 0.0\n",
      "t mean: 0.08563689142465591\n",
      "t max: 0.47707700729370117\n",
      "t min: 0.0\n",
      "t mean: 0.0667329952120781\n",
      "t max: 0.47522908449172974\n",
      "t min: 0.0\n",
      "t mean: 0.0627441257238388\n",
      "t max: 0.4731215238571167\n",
      "t min: 0.0\n",
      "t mean: 0.06611160933971405\n",
      "t max: 0.4708912968635559\n",
      "t min: 0.0\n",
      "t mean: 0.05846955627202988\n",
      "t max: 0.4684678912162781\n",
      "t min: 0.0\n",
      "t mean: 0.05448201298713684\n",
      "t max: 0.46589207649230957\n",
      "t min: 0.0\n",
      "t mean: 0.06498052924871445\n",
      "t max: 0.4631301164627075\n",
      "t min: 0.0\n",
      "t mean: 0.07170940935611725\n",
      "t max: 0.4601135849952698\n",
      "t min: 0.0\n",
      "t mean: 0.06756691634654999\n",
      "t max: 0.4567995071411133\n",
      "t min: 0.0\n",
      "t mean: 0.0459137000143528\n",
      "t max: 0.453241229057312\n",
      "t min: 0.0\n",
      "t mean: 0.059514936059713364\n",
      "t max: 0.449415922164917\n",
      "t min: 0.0\n",
      "t mean: 0.03807876259088516\n",
      "t max: 0.4434353709220886\n",
      "t min: 0.0\n",
      "t mean: 0.05485359579324722\n",
      "t max: 0.4407247304916382\n",
      "t min: 0.0\n",
      "t mean: 0.06439949572086334\n",
      "t max: 0.4358251094818115\n",
      "t min: 0.0\n",
      "t mean: 0.04684435576200485\n",
      "t max: 0.43048256635665894\n",
      "t min: 0.0\n",
      "t mean: 0.05292024463415146\n",
      "t max: 0.4248230457305908\n",
      "t min: 0.0\n",
      "t mean: 0.06174410134553909\n",
      "t max: 0.418013334274292\n",
      "t min: 0.0\n",
      "t mean: 0.04795850068330765\n",
      "t max: 0.4118881821632385\n",
      "t min: 0.0\n",
      "t mean: 0.040821027010679245\n",
      "t max: 0.40461820363998413\n",
      "t min: 0.0\n",
      "t mean: 0.06150055676698685\n",
      "t max: 0.3968127965927124\n",
      "t min: 0.0\n",
      "t mean: 0.057125162333250046\n",
      "t max: 0.38829267024993896\n",
      "t min: 0.0\n",
      "t mean: 0.04113098606467247\n",
      "t max: 0.378983736038208\n",
      "t min: 0.0\n",
      "t mean: 0.039999306201934814\n",
      "t max: 0.36892169713974\n",
      "t min: 0.0\n",
      "t mean: 0.05541745200753212\n",
      "t max: 0.3581618070602417\n",
      "t min: 0.0\n",
      "t mean: 0.04548342153429985\n",
      "t max: 0.3464844822883606\n",
      "t min: 0.0\n",
      "t mean: 0.048925984650850296\n",
      "t max: 0.33401399850845337\n",
      "t min: 0.0\n",
      "t mean: 0.042039044201374054\n",
      "t max: 0.32055628299713135\n",
      "t min: 0.0\n",
      "t mean: 0.044623102992773056\n",
      "t max: 0.3064877986907959\n",
      "t min: 0.0\n",
      "t mean: 0.04244320094585419\n",
      "t max: 0.2914109230041504\n",
      "t min: 0.0\n",
      "t mean: 0.02967006154358387\n",
      "t max: 0.2757527232170105\n",
      "t min: 0.0\n",
      "t mean: 0.033560607582330704\n",
      "t max: 0.25938791036605835\n",
      "t min: 0.0\n",
      "t mean: 0.03173030912876129\n",
      "t max: 0.2433539628982544\n",
      "t min: 0.0\n",
      "t mean: 0.02759733237326145\n",
      "t max: 0.2266753911972046\n",
      "t min: 0.0\n",
      "t mean: 0.024034395813941956\n",
      "t max: 0.21052932739257812\n",
      "t min: 0.0\n",
      "t mean: 0.02338409796357155\n",
      "t max: 0.19388318061828613\n",
      "t min: 0.0\n",
      "t mean: 0.021524982526898384\n",
      "t max: 0.177068293094635\n",
      "t min: 0.0\n",
      "t mean: 0.021854326128959656\n",
      "t max: 0.16165459156036377\n",
      "t min: 0.0\n",
      "t mean: 0.017606865614652634\n",
      "t max: 0.14587879180908203\n",
      "t min: 0.0\n",
      "t mean: 0.017635365948081017\n",
      "t max: 0.13019627332687378\n",
      "t min: 0.0\n",
      "t mean: 0.009489688090980053\n",
      "t max: 0.115092933177948\n",
      "t min: 0.0\n",
      "t mean: 0.011380195617675781\n",
      "t max: 0.10169589519500732\n",
      "t min: 0.0\n",
      "t mean: 0.01137036457657814\n",
      "t max: 0.08939659595489502\n",
      "t min: 0.0\n",
      "t mean: 0.009311487898230553\n",
      "t max: 0.07858556509017944\n",
      "t min: 0.0\n",
      "t mean: 0.007168728858232498\n",
      "t max: 0.06824320554733276\n",
      "t min: 0.0\n",
      "t mean: 0.0043969713151454926\n",
      "t max: 0.05868363380432129\n",
      "t min: 0.0\n",
      "t mean: 0.0051376610063016415\n",
      "t max: 0.050741374492645264\n",
      "t min: 0.0\n",
      "t mean: 0.005471391137689352\n",
      "t max: 0.04406905174255371\n",
      "t min: 0.0\n",
      "t mean: 0.0028571407310664654\n",
      "t max: 0.03794628381729126\n",
      "t min: 0.0\n",
      "t mean: 0.004443526268005371\n",
      "t max: 0.03254091739654541\n",
      "t min: 0.0\n",
      "t mean: 0.0032163821160793304\n",
      "t max: 0.027879714965820312\n",
      "t min: 0.0\n",
      "t mean: 0.0029632272198796272\n",
      "t max: 0.023962676525115967\n",
      "t min: 0.0\n",
      "t mean: 0.002074744552373886\n",
      "t max: 0.020699799060821533\n",
      "t min: 0.0\n",
      "t mean: 0.0019091631984338164\n",
      "t max: 0.01804417371749878\n",
      "t min: 0.0\n",
      "t mean: 0.001828446751460433\n",
      "t max: 0.015713989734649658\n",
      "t min: 0.0\n",
      "t mean: 0.0017712560947984457\n",
      "t max: 0.013677537441253662\n",
      "t min: 0.0\n",
      "t mean: 0.0011241547763347626\n",
      "t max: 0.011903166770935059\n",
      "t min: 0.0\n",
      "t mean: 0.0011959585826843977\n",
      "t max: 0.010374844074249268\n",
      "t min: 0.0\n",
      "t mean: 0.000939683523029089\n",
      "t max: 0.009057223796844482\n",
      "t min: 0.0\n",
      "t mean: 0.0007723149610683322\n",
      "t max: 0.00797957181930542\n",
      "t min: 0.0\n",
      "t mean: 0.000599011720623821\n",
      "t max: 0.007068634033203125\n",
      "t min: 0.0\n",
      "t mean: 0.0005464979913085699\n",
      "t max: 0.006371557712554932\n",
      "t min: 0.0\n",
      "t mean: 0.0006147557287476957\n",
      "t max: 0.005763053894042969\n",
      "t min: 0.0\n",
      "t mean: 0.00037101126508787274\n",
      "t max: 0.005225479602813721\n",
      "t min: 0.0\n",
      "t mean: 0.0005592911038547754\n",
      "t max: 0.0047969818115234375\n",
      "t min: 0.0\n",
      "t mean: 0.000479654292576015\n",
      "t max: 0.004445314407348633\n",
      "t min: 0.0\n",
      "t mean: 0.00038439955096691847\n",
      "t max: 0.004126608371734619\n",
      "t min: 0.0\n",
      "t mean: 0.0003118187887594104\n",
      "t max: 0.003839254379272461\n",
      "t min: 0.0\n",
      "t mean: 0.0003826008760370314\n",
      "t max: 0.0035814642906188965\n",
      "t min: 0.0\n",
      "t mean: 0.000332473311573267\n",
      "t max: 0.0033353567123413086\n",
      "t min: 0.0\n",
      "t mean: 0.000322156265610829\n",
      "t max: 0.0031377673149108887\n",
      "t min: 0.0\n",
      "t mean: 0.00030360842356458306\n",
      "t max: 0.0029474496841430664\n",
      "t min: 0.0\n",
      "t mean: 0.00031215581111609936\n",
      "t max: 0.0027640461921691895\n",
      "t min: 0.0\n",
      "t mean: 0.0003194431192241609\n",
      "t max: 0.0026160478591918945\n",
      "t min: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:10:56,638:INFO:[TRAIN-ITER]: 100, [Time-Elapsed]: 8.042199, [Total-Loss]: 0.063234\n",
      "2024-12-16 18:10:56,639:INFO:[CONSTR-Vio]: 0.000179, [OBJECTIVE]: 0.000869, [EFFICIENCY-loss]:  0.062365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t mean: 0.0002680872566998005\n",
      "t max: 0.002471327781677246\n",
      "t min: 0.0\n",
      "t mean: 0.0002172667591366917\n",
      "t max: 0.002338230609893799\n",
      "t min: 0.0\n",
      "t mean: 0.0002950053894892335\n",
      "t max: 0.0022174715995788574\n",
      "t min: 0.0\n",
      "t mean: 0.0002439409727230668\n",
      "t max: 0.0021099448204040527\n",
      "t min: 0.0\n",
      "t mean: 0.00015733008331153542\n",
      "t max: 0.0020148754119873047\n",
      "t min: 0.0\n",
      "t mean: 0.0002153737295884639\n",
      "t max: 0.0019322037696838379\n",
      "t min: 0.0\n",
      "t mean: 0.00015901087317615747\n",
      "t max: 0.001858055591583252\n",
      "t min: 0.0\n",
      "t mean: 0.00022859693854115903\n",
      "t max: 0.0017929673194885254\n",
      "t min: 0.0\n",
      "t mean: 0.00013565225526690483\n",
      "t max: 0.0017338395118713379\n",
      "t min: 0.0\n",
      "t mean: 0.0001314126857323572\n",
      "t max: 0.0016760826110839844\n",
      "t min: 0.0\n",
      "t mean: 0.00014890951570123434\n",
      "t max: 0.0016353130340576172\n",
      "t min: 0.0\n",
      "t mean: 0.0001905540411826223\n",
      "t max: 0.001588582992553711\n",
      "t min: 0.0\n",
      "t mean: 0.0002245039213448763\n",
      "t max: 0.001553177833557129\n",
      "t min: 0.0\n",
      "t mean: 0.00016272012726403773\n",
      "t max: 0.001512765884399414\n",
      "t min: 0.0\n",
      "t mean: 0.00013765272160526365\n",
      "t max: 0.0014731287956237793\n",
      "t min: 0.0\n",
      "t mean: 0.00016201226389966905\n",
      "t max: 0.0014351606369018555\n",
      "t min: 0.0\n",
      "t mean: 0.00010759325232356787\n",
      "t max: 0.0013983845710754395\n",
      "t min: 0.0\n",
      "t mean: 0.0001303634635405615\n",
      "t max: 0.0013631582260131836\n",
      "t min: 0.0\n",
      "t mean: 0.00010731720976764336\n",
      "t max: 0.0012356042861938477\n",
      "t min: 0.0\n",
      "t mean: 0.00017289514653384686\n",
      "t max: 0.001297593116760254\n",
      "t min: 0.0\n",
      "t mean: 0.00013810786185786128\n",
      "t max: 0.0012668371200561523\n",
      "t min: 0.0\n",
      "t mean: 0.00018256899784319103\n",
      "t max: 0.0012359023094177246\n",
      "t min: 0.0\n",
      "t mean: 0.00011579803685890511\n",
      "t max: 0.0012142062187194824\n",
      "t min: 0.0\n",
      "t mean: 8.457359217572957e-05\n",
      "t max: 0.0011914372444152832\n",
      "t min: 0.0\n",
      "t mean: 0.00013001312618143857\n",
      "t max: 0.0011709332466125488\n",
      "t min: 0.0\n",
      "t mean: 0.00010941951768472791\n",
      "t max: 0.00115203857421875\n",
      "t min: 0.0\n",
      "t mean: 0.00016665553266648203\n",
      "t max: 0.0011342763900756836\n",
      "t min: 0.0\n",
      "t mean: 0.0001282038283534348\n",
      "t max: 0.0011161565780639648\n",
      "t min: 0.0\n",
      "t mean: 0.00013779733853880316\n",
      "t max: 0.0010977387428283691\n",
      "t min: 0.0\n",
      "t mean: 9.77174931904301e-05\n",
      "t max: 0.0010787248611450195\n",
      "t min: 0.0\n",
      "t mean: 0.00012349561438895762\n",
      "t max: 0.0010600686073303223\n",
      "t min: 0.0\n",
      "t mean: 0.00012508723011706024\n",
      "t max: 0.0010413527488708496\n",
      "t min: 0.0\n",
      "t mean: 8.10992787592113e-05\n",
      "t max: 0.0010238289833068848\n",
      "t min: 0.0\n",
      "t mean: 0.00013685936573892832\n",
      "t max: 0.0010080337524414062\n",
      "t min: 0.0\n",
      "t mean: 0.00012437003897503018\n",
      "t max: 0.0009927153587341309\n",
      "t min: 0.0\n",
      "t mean: 0.00011167626507813111\n",
      "t max: 0.0009784102439880371\n",
      "t min: 0.0\n",
      "t mean: 0.00010222932905890048\n",
      "t max: 0.0009644627571105957\n",
      "t min: 0.0\n",
      "t mean: 0.00010625629511196166\n",
      "t max: 0.0009502172470092773\n",
      "t min: 0.0\n",
      "t mean: 0.00014280842151492834\n",
      "t max: 0.0009354352951049805\n",
      "t min: 0.0\n",
      "t mean: 0.0001086443880922161\n",
      "t max: 0.000920712947845459\n",
      "t min: 0.0\n",
      "t mean: 0.00010125843982677907\n",
      "t max: 0.000906825065612793\n",
      "t min: 0.0\n",
      "t mean: 4.1388775571249425e-05\n",
      "t max: 0.0008347630500793457\n",
      "t min: 0.0\n",
      "t mean: 0.00010826670768437907\n",
      "t max: 0.000882267951965332\n",
      "t min: 0.0\n",
      "t mean: 9.384597069583833e-05\n",
      "t max: 0.0008710622787475586\n",
      "t min: 0.0\n",
      "t mean: 9.649340790929273e-05\n",
      "t max: 0.0008592009544372559\n",
      "t min: 0.0\n",
      "t mean: 0.00010636598744895309\n",
      "t max: 0.0008469820022583008\n",
      "t min: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m PrimalNet(cfg)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_primal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_train.py:92\u001b[0m, in \u001b[0;36mtrain_primal\u001b[0;34m(cfg, G, model, include_truncation)\u001b[0m\n\u001b[1;32m     89\u001b[0m p, q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(P)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mTensor(Q)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m r \u001b[38;5;241m=\u001b[39m model(p, q)\n\u001b[0;32m---> 92\u001b[0m loss, constr_vio, obj, efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (i\u001b[38;5;241m%\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlagr_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     94\u001b[0m     lambd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_loss.py:79\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(cfg, model, r, p, q, lambd, rho)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt max:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt min:\u001b[39m\u001b[38;5;124m\"\u001b[39m, t\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 79\u001b[0m efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficiency_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m lambd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(lambd)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;241m+\u001b[39m (constr_vio\u001b[38;5;241m*\u001b[39mlambd)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m efficiency_loss \u001b[38;5;66;03m# 3項目は大きいものを強く抑制するため\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:106\u001b[0m, in \u001b[0;36mcompute_efficiency_loss\u001b[0;34m(cfg, r, p, q)\u001b[0m\n\u001b[1;32m    103\u001b[0m     p_batch \u001b[38;5;241m=\u001b[39m p[batch_idx]\n\u001b[1;32m    104\u001b[0m     q_batch \u001b[38;5;241m=\u001b[39m q[batch_idx]\n\u001b[0;32m--> 106\u001b[0m     stable_matchings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_stable_matchings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     efficient_matchings \u001b[38;5;241m=\u001b[39m filter_efficient_stable_matchings(stable_matchings, p_batch, q_batch)\n\u001b[1;32m    109\u001b[0m batch_efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:23\u001b[0m, in \u001b[0;36mgenerate_stable_matchings\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# ブロッキングペアの条件をチェック\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m---> 23\u001b[0m             \u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_matching\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_matching\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (q[possible_matching[j], i] \u001b[38;5;241m>\u001b[39m q[possible_matching[j], j])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m         ):\n\u001b[1;32m     26\u001b[0m             is_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"mps\" #if torch.cuda.is_available() else \"cpu\"\n",
    "lambd = np.ones((2,2))*0.001\n",
    "# lambd = cfg.lambd\n",
    "\n",
    "cfg = HParams(num_agents = 2,\n",
    "              device = device,\n",
    "              lambd = lambd,\n",
    "              rho = 0.1,\n",
    "              lagr_iter = 10,\n",
    "              batch_size = 32,\n",
    "              epochs=10000)\n",
    "\n",
    "cfg.lr = 1e-4\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "G = Data(cfg)\n",
    "\n",
    "model = PrimalNet(cfg)\n",
    "model.to(device)\n",
    "\n",
    "train_primal(cfg,G,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[1, 0.0], [0, 1]]).to(device)\n",
    "q = torch.tensor([[1, 0.0], [0, 1]]).to(device)\n",
    "\n",
    "output = model(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5070, 0.4930],\n",
       "         [0.4929, 0.5071]]], device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3* 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" #if torch.cuda.is_available() else \"cpu\"\n",
    "lambd = np.ones((3,3))*0.001\n",
    "# lambd = cfg.lambd\n",
    "\n",
    "cfg = HParams(num_agents = 3,\n",
    "              device = device,\n",
    "              lambd = lambd,\n",
    "              rho = 0.1,\n",
    "              lagr_iter = 100,\n",
    "              batch_size = 128,\n",
    "              epochs = 4000)\n",
    "\n",
    "cfg.lr = 1e-4\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "G = Data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimalNet(\n",
       "  (input_block): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (layer_out): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PrimalNet(cfg)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 09:34:26,726:INFO:[TRAIN-ITER]: 0, [Time-Elapsed]: 6.266988, [Total-Loss]: 2.924650\n",
      "2024-12-23 09:34:26,737:INFO:[CONSTR-Vio]: 0.003013, [OBJECTIVE]: 1.422869, [EFFICIENCY-loss]:  1.501778\n",
      "2024-12-23 09:42:24,450:INFO:[TRAIN-ITER]: 100, [Time-Elapsed]: 483.992995, [Total-Loss]: 2.026359\n",
      "2024-12-23 09:42:24,461:INFO:[CONSTR-Vio]: 0.061650, [OBJECTIVE]: 1.139862, [EFFICIENCY-loss]:  0.886408\n",
      "2024-12-23 09:50:23,195:INFO:[TRAIN-ITER]: 200, [Time-Elapsed]: 962.740989, [Total-Loss]: -0.041631\n",
      "2024-12-23 09:50:23,198:INFO:[CONSTR-Vio]: 0.329162, [OBJECTIVE]: 0.434767, [EFFICIENCY-loss]:  -0.477589\n",
      "2024-12-23 10:51:20,090:INFO:[TRAIN-ITER]: 300, [Time-Elapsed]: 4619.634338, [Total-Loss]: -0.240349\n",
      "2024-12-23 10:51:20,099:INFO:[CONSTR-Vio]: 0.428653, [OBJECTIVE]: 0.422132, [EFFICIENCY-loss]:  -0.665835\n",
      "2024-12-23 10:59:16,331:INFO:[TRAIN-ITER]: 400, [Time-Elapsed]: 5095.875269, [Total-Loss]: -0.417135\n",
      "2024-12-23 10:59:16,342:INFO:[CONSTR-Vio]: 0.429578, [OBJECTIVE]: 0.364413, [EFFICIENCY-loss]:  -0.787033\n",
      "2024-12-23 11:07:09,526:INFO:[TRAIN-ITER]: 500, [Time-Elapsed]: 5569.071800, [Total-Loss]: -0.392611\n",
      "2024-12-23 11:07:09,529:INFO:[CONSTR-Vio]: 0.390597, [OBJECTIVE]: 0.338314, [EFFICIENCY-loss]:  -0.737821\n",
      "2024-12-23 11:44:05,849:INFO:[TRAIN-ITER]: 600, [Time-Elapsed]: 7785.392580, [Total-Loss]: -0.346331\n",
      "2024-12-23 11:44:05,859:INFO:[CONSTR-Vio]: 0.432696, [OBJECTIVE]: 0.433589, [EFFICIENCY-loss]:  -0.789796\n",
      "2024-12-23 12:37:56,088:INFO:[TRAIN-ITER]: 700, [Time-Elapsed]: 11015.632328, [Total-Loss]: -0.490582\n",
      "2024-12-23 12:37:56,100:INFO:[CONSTR-Vio]: 0.371231, [OBJECTIVE]: 0.356809, [EFFICIENCY-loss]:  -0.857339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_primal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_train.py:92\u001b[0m, in \u001b[0;36mtrain_primal\u001b[0;34m(cfg, G, model, include_truncation)\u001b[0m\n\u001b[1;32m     89\u001b[0m p, q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(P)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mTensor(Q)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m r \u001b[38;5;241m=\u001b[39m model(p, q)\n\u001b[0;32m---> 92\u001b[0m loss, constr_vio, obj, efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (i\u001b[38;5;241m%\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlagr_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     94\u001b[0m     lambd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/primal_loss.py:98\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(cfg, model, r, p, q, lambd, rho)\u001b[0m\n\u001b[1;32m     94\u001b[0m constr_vio \u001b[38;5;241m=\u001b[39m spv_w\u001b[38;5;66;03m#+spv_f\u001b[39;00m\n\u001b[1;32m     96\u001b[0m obj \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 98\u001b[0m efficiency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficiency_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m lambd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(lambd)\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;241m+\u001b[39m (constr_vio\u001b[38;5;241m*\u001b[39mlambd)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mrho\u001b[38;5;241m*\u001b[39mconstr_vio\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m efficiency_loss \u001b[38;5;66;03m# 3項目は大きいものを強く抑制するため\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:136\u001b[0m, in \u001b[0;36mcompute_efficiency_loss\u001b[0;34m(cfg, r, p, q)\u001b[0m\n\u001b[1;32m    134\u001b[0m batch_efficiency_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m matching \u001b[38;5;129;01min\u001b[39;00m efficient_matchings:\n\u001b[0;32m--> 136\u001b[0m     loss_for_matching \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_efficiency_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss_for_matching \u001b[38;5;241m<\u001b[39m batch_efficiency_loss:\n\u001b[1;32m    138\u001b[0m         batch_efficiency_loss \u001b[38;5;241m=\u001b[39m loss_for_matching\n",
      "File \u001b[0;32m~/Github/meina-t/matching_with_dl/efficiency_loss.py:73\u001b[0m, in \u001b[0;36mcalc_efficiency_loss\u001b[0;34m(cfg, r, mu, p, q)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m効率性損失を計算する。\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m- r: マッチング確率行列 (2D tensor, num_agents x num_agents)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m- q: 受け手の選好行列 (2D tensor, num_agents x num_agents)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# マッチングを2D tensorに変換\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m matching \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_agents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mu):\n\u001b[1;32m     75\u001b[0m     matching[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_primal(cfg,G,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/sh8zstl54dl_t2y55d7th57m0000gn/T/ipykernel_52695/2637426541.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_s.load_state_dict(torch.load('model_1222.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrimalNet(\n",
       "  (input_block): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (layer_out): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 空のモデルインスタンスを作成\n",
    "model_s = PrimalNet(cfg)  # モデルクラスを再定義する必要があります\n",
    "model_s.load_state_dict(torch.load('model_1222.pth'))\n",
    "model_s.to(device)\n",
    "model_s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matching(p, q, match):\n",
    "    # Move tensor to CPU and convert to NumPy\n",
    "    output_matrix = match.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    annotations = np.empty_like(output_matrix, dtype=object)\n",
    "    for i in range(output_matrix.shape[0]):\n",
    "        for j in range(output_matrix.shape[1]):\n",
    "            annotations[i, j] = f'{output_matrix[i, j]:.2e}\\n[{p[i, j]}, {q[j, i]}]'\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(output_matrix, annot=annotations, fmt='', cmap='Blues', cbar=True)\n",
    "    plt.title(\"Agent Relationship Heatmap with Vector Details\")\n",
    "    plt.xlabel(\"Agent\")\n",
    "    plt.ylabel(\"Agent\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[1, 0.0, 0], [0, 1, 0], [0, 0, 1]]).to(device)\n",
    "q = torch.tensor([[1, 0.0, 0], [0, 1, 0], [0, 0, 1]]).to(device)\n",
    "\n",
    "output = model(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]], device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoUAAAIjCAYAAAB1bGEnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB+PUlEQVR4nO3deVxN6R8H8M9tu6VNum2yRLYsLYoUWRvMkGUY+zoME4WJQbZkkJ0h2xj7vgzGbxiMbSwla2IsWbKlUjfapO2e3x9xuCoqXRWf9+91X7/pOc85z/fcjttzv89zniMRBEEAEREREX3R1Io7ACIiIiIqfuwUEhERERE7hURERETETiERERERgZ1CIiIiIgI7hUREREQEdgqJiIiICOwUEhERERHYKSQiIiIisFNInwkrKysMGDCgSI85YMAAWFlZFekxC6N58+aoW7fuB+vdv38fEokE69atU31QpHIFuf4GDBgAPT091QZEKte8eXM0b95c/Jn/pulTY6ewFFi2bBkkEgmcnZ2LO5RcLVu2rEAfWhKJROllYGCAZs2aYf/+/aoLMg9PnjzB1KlTERoa+snbLg3e1yF9/Qdr3rx5Ko1h5syZ2Lt3r0rbKA1evHiBqVOn4sSJE0V2zKdPn0JDQwN9+vTJs05SUhJ0dHTw7bffFlm7AHD9+nVMnToV9+/fL9Ljvs/UqVOVPnvKlCmDSpUqwcPDA2vXrkVaWlqhjx0UFISpU6fi+fPnRRcw0SfGTmEpsHnzZlhZWeHcuXO4c+dOcYeTQ0E7hQDw1VdfYePGjdiwYQPGjh2LO3fuwMPDA4cOHVJNkHl48uQJ/P39c+0Urlq1Crdu3fqk8XyMypUrIzU1FX379i3uUIrUl9opfPf6e/HiBfz9/Yu0U2hqaoqvvvoKf/75J168eJFrnd27d+Ply5fv7TgWxvXr1+Hv7/9JO4WvLV++HBs3bsSSJUswePBgxMfH4/vvv0fDhg3x6NGjQh0zKCgI/v7+H9UpPHz4MA4fPlzo/Yk+FjuFJVxERASCgoKwYMECmJiYYPPmzcUdUpGoUaMG+vTpg759+2LSpEk4cuQIBEHAr7/+WtyhiTQ1NSGVSos7jHyTSCTQ1taGurp6cYdCReBTXX+9e/dGcnIy9u3bl+v2LVu2wNDQEO3atVN5LEUhr87t27p27Yo+ffpg0KBBmDJlCs6cOYNNmzbh2rVr+O677z5BlLnT0tKClpZWsbVPxE5hCbd582YYGRmhXbt26Nq1a56dQrlcjr59+8LAwABly5ZF//79ceXKlVzno9y8eRNdu3ZFuXLloK2tDScnpxx/ENatWweJRIIzZ87Ax8cHJiYm0NXVRefOnREbGyvWs7Kywn///Yd///1XHJJ5e05MftnY2EAmk+Hu3btK5WlpafDz80O1atUglUpRsWJFjB079oPDPPHx8RgzZgzq1asHPT09GBgY4Ouvv8aVK1fEOidOnECDBg0AAAMHDhTjf/1+5TanKyUlBaNHj0bFihUhlUpRs2ZNzJs3D4IgKNWTSCTw8vLC3r17UbduXUilUtSpUwcHDx5UqpeUlIRRo0bBysoKUqlUzNxcunQpxzldv34dLVq0QJkyZWBpaYk5c+Yobc9t/tHruWb37t1DmzZtoKuri/Lly2PatGk5Yi4qz58/x6hRo8T3qFq1apg9ezYUCoVSvXnz5sHV1RXGxsbQ0dGBo6Mjdu3apVRHIpEgJSUF69evF38/r+eOvh4KDA8PR58+fWBoaAgTExNMnjwZgiDg0aNH6NixIwwMDGBubo758+crHTs9PR1TpkyBo6MjDA0NoaurCzc3Nxw/flyp3tvD5AsXLkTlypWho6ODZs2a4dq1ax98L9TV1bF48WKxLC4uDmpqajA2Nlb6HXh6esLc3Fz8+e3r7/79+zAxMQEA+Pv7i+/F1KlTldqLjIxEp06doKenBxMTE4wZMwZZWVnvjbFz587Q1dXFli1bcmx7+vQpjh49iq5du4od1JCQELRt2xaGhoYoU6YMmjVrhjNnzuTYNzIyEoMGDUL58uUhlUpRpUoVeHp6Ij09HevWrRM7Xy1atBDP5+0s6LJly1CnTh1IpVKUL18ew4cPz5GFez294eLFi2jatCnKlCmDCRMmvPd889K7d28MHjwYISEh+Oeff5S2feicp06dip9//hkAUKVKFfF8XmdB165di5YtW8LU1BRSqRS1a9fG8uXLc8Tw7pzC3ERHR2PgwIGoUKECpFIpLCws0LFjx2LJuNLnR6O4A6D327x5M7799ltoaWmhZ8+eWL58Oc6fPy92ZgBAoVDAw8MD586dg6enJ2rVqoU///wT/fv3z3G8//77D40bN4alpSXGjx8PXV1d7NixA506dcIff/yBzp07K9X39vaGkZER/Pz8cP/+fSxatAheXl7Yvn07AGDRokXw9vaGnp4eJk6cCAAwMzMr8HkmJCTg2bNnsLa2VjqvDh064PTp0xgyZAhsbGxw9epVLFy4EOHh4e8dUrx37x727t2L7777DlWqVEFMTAxWrlyJZs2a4fr16yhfvjxsbGwwbdo0TJkyBUOGDIGbmxsAwNXVNddjCoKADh064Pjx4xg0aBDs7e1x6NAh/Pzzz4iMjMTChQuV6p8+fRq7d+/GsGHDoK+vj8WLF6NLly54+PAhjI2NAQA//vgjdu3aBS8vL9SuXRtyuRynT5/GjRs3UL9+ffFYz549Q9u2bfHtt9+iW7du2LVrF8aNG4d69erh66+/fu97m5WVhbZt26JRo0aYM2cODh48CD8/P2RmZmLatGnv3ff1/nFxcTnKnz17lqPsxYsXaNasGSIjIzF06FBUqlQJQUFB8PX1RVRUFBYtWiTW/fXXX9GhQwf07t0b6enp2LZtG7777jv89ddfYlZq48aNGDx4MBo2bIghQ4YAgNI1AgDdu3eHjY0NZs2ahf3792P69OkoV64cVq5ciZYtW2L27NnYvHkzxowZgwYNGqBp06YAgMTERPz+++/o2bMnfvjhByQlJWH16tVo06YNzp07B3t7e6V2NmzYgKSkJAwfPhwvX77Er7/+ipYtW+Lq1at5XvNly5ZF3bp1cfLkSYwYMQJA9nUhkUgQHx+P69evo06dOgCAU6dOidfgu0xMTLB8+XJ4enqic+fO4vw+W1tbpd9TmzZt4OzsjHnz5uHIkSOYP38+rK2t4enpmetxAUBXVxcdO3bErl27EB8fj3Llyonbtm/fjqysLPTu3RsAcOzYMXz99ddwdHSEn58f1NTUxA7PqVOn0LBhQwDZ0zIaNmyI58+fY8iQIahVqxYiIyOxa9cuvHjxAk2bNsWIESOwePFiTJgwATY2NgAg/v/UqVPh7+8Pd3d3eHp64tatW+Jn35kzZ6CpqSnGKJfL8fXXX6NHjx7o06dPoT5/Xuvbty9+++03HD58GF999VW+z/nbb79FeHg4tm7dioULF0Imk4m/NyB7uLpOnTro0KEDNDQ08L///Q/Dhg2DQqHA8OHDCxRjly5d8N9//8Hb2xtWVlZ4+vQp/vnnHzx8+LBE3BhHpZxAJdaFCxcEAMI///wjCIIgKBQKoUKFCsLIkSOV6v3xxx8CAGHRokViWVZWltCyZUsBgLB27VqxvFWrVkK9evWEly9fimUKhUJwdXUVqlevLpatXbtWACC4u7sLCoVCLP/pp58EdXV14fnz52JZnTp1hGbNmuX7vAAIgwYNEmJjY4WnT58KFy5cENq2bSsAEObOnSvW27hxo6CmpiacOnVKaf8VK1YIAIQzZ86IZZUrVxb69+8v/vzy5UshKytLab+IiAhBKpUK06ZNE8vOnz+f4z16rX///kLlypXFn/fu3SsAEKZPn65Ur2vXroJEIhHu3LmjdI5aWlpKZVeuXBEACEuWLBHLDA0NheHDh+fxTmVr1qyZAEDYsGGDWJaWliaYm5sLXbp0UTq/d8+lf//+AgDB29tbLFMoFEK7du0ELS0tITY2Nl9tv+/19u/sl19+EXR1dYXw8HCl44wfP15QV1cXHj58KJa9ePFCqU56erpQt25doWXLlkrlurq6Sr/b1/z8/AQAwpAhQ8SyzMxMoUKFCoJEIhFmzZollj979kzQ0dFROk5mZqaQlpamdMxnz54JZmZmwvfffy+WvX5fdXR0hMePH4vlISEhAgDhp59+yu2tEw0fPlwwMzMTf/bx8RGaNm0qmJqaCsuXLxcEQRDkcrkgkUiEX3/9Vaz37vUXGxsrABD8/PxytPH69/z2tS0IguDg4CA4Ojq+Nz5BEIT9+/cLAISVK1cqlTdq1EiwtLQUsrKyBIVCIVSvXl1o06aN0mfCixcvhCpVqghfffWVWNavXz9BTU1NOH/+fI62Xu+7c+dOAYBw/Phxpe1Pnz4VtLS0hNatWyv9Gw4MDBQACGvWrBHLXl+fK1as+OA5CsKbayav6/7Zs2cCAKFz585irPk957lz5woAhIiIiBzHffdaFwRBaNOmjVC1alWlsmbNmil9lr77b/p1fG//myMqShw+LsE2b94MMzMztGjRAkD2UFr37t2xbds2pSGhgwcPQlNTEz/88INYpqamluMbaHx8PI4dO4Zu3bohKSkJcXFxiIuLg1wuR5s2bXD79m1ERkYq7TNkyBBIJBLxZzc3N2RlZeHBgwcfdW6rV6+GiYkJTE1N4eTkhKNHj2Ls2LHw8fER6+zcuRM2NjaoVauWGGtcXBxatmwJADmG+d4mlUqhppZ9eWdlZUEul0NPTw81a9bMdWg2Pw4cOAB1dXUx4/Pa6NGjIQgC/v77b6Vyd3d3payWra0tDAwMcO/ePbGsbNmyCAkJwZMnT97btp6entJEfy0tLTRs2FDpWO/j5eUl/vfroe309HQcOXLkg/taWVnhn3/+yfHatGlTjro7d+6Em5sbjIyMlH5n7u7uyMrKwsmTJ8W6Ojo64n8/e/YMCQkJcHNzK/DvZ/DgweJ/q6urw8nJCYIgYNCgQWJ52bJlUbNmTaX3S11dXZy/pVAoEB8fj8zMTDg5OeUaQ6dOnWBpaSn+3LBhQzg7O+PAgQPvjc/NzQ0xMTHiTSOnTp1C06ZN4ebmhlOnTgHIzh4KgpBnpjC/fvzxxxxt5+caad26NUxMTJSGkCMiInD27Fn07NkTampqCA0Nxe3bt9GrVy/I5XLxd5uSkoJWrVrh5MmTUCgUUCgU2Lt3Lzw8PODk5JSjrbc/T3Jz5MgRpKenY9SoUeK/YQD44YcfYGBgkGOVAqlUioEDB37wHPPj9bI+SUlJAJDvc/6Qt6/1hIQExMXFoVmzZrh37x4SEhLyHZ+Ojg60tLRw4sSJXDP1RB+Lw8clVFZWFrZt24YWLVogIiJCLHd2dsb8+fNx9OhRtG7dGgDw4MEDWFhYoEyZMkrHqFatmtLPd+7cgSAImDx5MiZPnpxru0+fPlX6w1epUiWl7UZGRgByHzosiI4dO4odk/Pnz2PmzJl48eKF0h+B27dv48aNG+IQTG6x5kWhUODXX3/FsmXLEBERodSJfj10W1APHjxA+fLloa+vr1T+esjr3Y7yu+8dkP3+vf3ezZkzB/3790fFihXh6OiIb775Bv369UPVqlWV9qtQoUKOP6ZGRkYICwv7YNxqamo5jlejRg0AyNc8JF1dXbi7u+coz23f27dvIywsLF+/s7/++gvTp09HaGio0hzRD3Ua3vXu+2xoaAhtbW1xCO/tcrlcrlS2fv16zJ8/Hzdv3kRGRoZYXqVKlRztVK9ePUdZjRo1sGPHjvfG97qjd+rUKVSoUAGXL1/G9OnTYWJiIi7nc+rUKRgYGMDOzu69x3ofbW3tHO/7u9dbXjQ0NNC9e3csW7YMkZGRsLS0FDuIr4eOb9++DQC5Tkt5LSEhAenp6UhMTMzX2pq5ef3vqGbNmkrlWlpaqFq1ao5/Z5aWlkV2c0ZycjIAiP/G83vOrz8X83LmzBn4+fkhODg4x40wCQkJMDQ0zFd8UqkUs2fPxujRo2FmZoZGjRqhffv26Nevn9J8VKLCYqewhDp27BiioqKwbds2bNu2Lcf2zZs3i53C/Hr9jXbMmDFo06ZNrnXe7UjmdSer8JE3KVSoUEHsaHzzzTeQyWTw8vJCixYtxPlSCoUC9erVw4IFC3I9RsWKFfM8/syZMzF58mR8//33+OWXX1CuXDmoqalh1KhR+fpmXxTy895169YNbm5u2LNnDw4fPoy5c+di9uzZ2L17t9JcQVX9HoqaQqHAV199hbFjx+a6/XVn9NSpU+jQoQOaNm2KZcuWwcLCApqamli7dm2uNzy8T27vTX7er02bNmHAgAHo1KkTfv75Z5iamkJdXR0BAQE5bnj6GOXLl0eVKlVw8uRJWFlZQRAEuLi4wMTEBCNHjsSDBw9w6tQpuLq6Kn0pKqiPveu8T58+CAwMxNatWzFmzBhs3boVtWvXFudWvv53M3fu3BzzLV/T09NDfHz8R8VRUG9n4T7W6xuHXn8O5vec3+fu3bto1aoVatWqhQULFqBixYrQ0tLCgQMHsHDhwgJ/Ho0aNQoeHh7Yu3cvDh06hMmTJyMgIADHjh2Dg4NDgY5F9C52CkuozZs3w9TUFEuXLs2xbffu3dizZw9WrFgBHR0dVK5cGcePH8eLFy+UsoXvrmn4OlukqamZa+ansAqa2cnN0KFDsXDhQkyaNAmdO3eGRCKBtbU1rly5glatWhW4jV27dqFFixZYvXq1Uvnz58+VMkgFOW7lypVx5MgRJCUlKWULb968KW4vDAsLCwwbNgzDhg3D06dPUb9+fcyYMeODN5Dkl0KhwL1798QOGQCEh4cDQJFPTLe2tkZycvIHr68//vgD2traOHTokNKyK2vXrs1Rtyiur9zs2rULVatWxe7du5Xa8PPzy7X+66zR28LDw/P1Hrq5ueHkyZOoUqUK7O3toa+vDzs7OxgaGuLgwYO4dOkS/P3933sMVb0Przk7O8Pa2hpbtmzBV199hf/++w8zZswQt7+eCmFgYPDe36+JiQkMDAw+eGd2Xufz+t/RrVu3lDLc6enpiIiIKNLPrndt3LgRAMQvzfk9ZyDv8/nf//6HtLQ07Nu3Tymr/b7pLx9ibW2N0aNHY/To0bh9+zbs7e0xf/78XKd0EBUE5xSWQKmpqdi9ezfat2+Prl275nh5eXkhKSlJXEamTZs2yMjIwKpVq8RjKBSKHB1KU1NTNG/eHCtXrkRUVFSOdt9eaqYgdHV1P3oVfw0NDYwePRo3btzAn3/+CSA7ixYZGal0Xq+lpqYiJSUlz+Opq6vnyKLt3Lkzx5xJXV1dAMhX/N988w2ysrIQGBioVL5w4UJIJJICd+KysrJyzCcyNTVF+fLlP+rJCrl5O2ZBEBAYGAhNTU20atWqSNvp1q0bgoODc12E/Pnz58jMzASQ/fuRSCRKw/r379/P9Y7yori+cvM6s/b2dRISEoLg4OBc6+/du1fp+jl37hxCQkLy9Xt3c3PD/fv3sX37dnE4WU1NDa6urliwYAEyMjI+OJ/w9Rc+VT4xo3fv3rh8+TL8/PwgkUjQq1cvcZujoyOsra0xb948cZj1ba8/P9TU1NCpUyf873//w4ULF3LUe/1+5/Vvz93dHVpaWli8eLHS72b16tVISEhQ2XqJW7Zswe+//w4XFxfx30V+zxnI+3xyu84SEhJy/QL0IS9evMDLly+VyqytraGvr1/knxn0ZWKmsATat28fkpKS0KFDh1y3N2rUSFzIunv37ujUqRMaNmyI0aNH486dO6hVqxb27dsnDuO8/Q126dKlaNKkCerVq4cffvgBVatWRUxMDIKDg/H48WOldfzyy9HREcuXL8f06dNRrVo1mJqaijeDFMSAAQMwZcoUzJ49G506dULfvn2xY8cO/Pjjjzh+/DgaN26MrKws3Lx5Ezt27MChQ4dyncgOAO3bt8e0adMwcOBAuLq64urVq9i8eXOOuXXW1tYoW7YsVqxYAX19fejq6sLZ2TnXOWUeHh5o0aIFJk6ciPv378POzg6HDx/Gn3/+iVGjRuVYKuVDkpKSUKFCBXTt2hV2dnbQ09PDkSNHcP78+Rxr6n0MbW1tHDx4EP3794ezszP+/vtv7N+/HxMmTMhz7l9h/fzzz9i3bx/at2+PAQMGwNHRESkpKbh69Sp27dqF+/fvQyaToV27dliwYAHatm2LXr164enTp1i6dCmqVauWY56ko6Mjjhw5ggULFohDsUXxyMf27dtj9+7d6Ny5M9q1a4eIiAisWLECtWvXzrUDUK1aNTRp0gSenp5IS0vDokWLYGxsnOdQ+dted/hu3bqFmTNniuVNmzbF33//DalUqrTMVG50dHRQu3ZtbN++HTVq1EC5cuVQt27dQs/dy02fPn0wbdo0/Pnnn2jcuLFSFlRNTQ2///47vv76a9SpUwcDBw6EpaUlIiMjcfz4cRgYGOB///sfgOzpG4cPH0azZs3E5aSioqKwc+dOnD59GmXLloW9vT3U1dUxe/ZsJCQkQCqVimv5+fr6wt/fH23btkWHDh1w69YtLFu2DA0aNCiSJ6vs2rULenp6SE9PR2RkJA4dOoQzZ87Azs4OO3fuLNQ5Ozo6AgAmTpyIHj16QFNTEx4eHmjdujW0tLTg4eGBoUOHIjk5GatWrYKpqWmuX87fJzw8HK1atUK3bt1Qu3ZtaGhoYM+ePYiJiUGPHj0++n0h4pI0JZCHh4egra0tpKSk5FlnwIABgqamphAXFycIQvZyFb169RL09fUFQ0NDYcCAAcKZM2cEAMK2bduU9r17967Qr18/wdzcXNDU1BQsLS2F9u3bC7t27RLrvF6S5t0lJY4fP55jGYno6GihXbt2gr6+vgDgg8vTAMhzGZapU6cqHT89PV2YPXu2UKdOHUEqlQpGRkaCo6Oj4O/vLyQkJIj75bYkzejRowULCwtBR0dHaNy4sRAcHJxjyQdBEIQ///xTqF27tqChoaG0/MO7S4IIgiAkJSUJP/30k1C+fHlBU1NTqF69ujB37lyl5Sred45vx5mWlib8/PPPgp2dnaCvry/o6uoKdnZ2wrJly5T2adasmVCnTp0cx3o3vryWpNHV1RXu3r0rtG7dWihTpoxgZmYm+Pn55ViyJzd5tf12e+8uj5GUlCT4+voK1apVE7S0tASZTCa4uroK8+bNE9LT08V6q1evFqpXry5IpVKhVq1awtq1a8UlQ9528+ZNoWnTpoKOjo4AQHz/8lpe5PU5f+hcFAqFMHPmTKFy5cqCVCoVHBwchL/++ivP93Xu3LnC/PnzhYoVKwpSqVRwc3MTrly58sH38DVTU1MBgBATEyOWnT59WgAguLm55aif2/UXFBQkODo6ClpaWkrL0+R1zrm9nx/SoEEDAUCO6/C1y5cvC99++61gbGwsSKVSoXLlykK3bt2Eo0ePKtV78OCB0K9fP8HExESQSqVC1apVheHDhystA7Rq1SqhatWqgrq6eo7PlcDAQKFWrVqCpqamYGZmJnh6egrPnj1TauN912duXr8fr1/a2tpChQoVhPbt2wtr1qxRWqqrMOf8yy+/CJaWloKamprS8jT79u0TbG1tBW1tbcHKykqYPXu2sGbNmhxL2HxoSZq4uDhh+PDhQq1atQRdXV3B0NBQcHZ2Fnbs2JHv94DofSSCUMJmqlOR2bt3Lzp37ozTp0+jcePGxR0OFYMBAwZg165duWa+KH/u37+PKlWqYO7cuRgzZkxxh0NEpDKcU/iZSE1NVfo5KysLS5YsgYGBgdKTMYiIiIhywzmFnwlvb2+kpqbCxcUFaWlp2L17N4KCgjBz5swiXbKBiIiIPk/sFH4mWrZsifnz5+Ovv/7Cy5cvUa1aNSxZskTpSRZEREREeeGcQiIiIqIS5OTJk5g7dy4uXryIqKgo7NmzB506dXrvPidOnICPjw/+++8/VKxYEZMmTcKAAQMK1C7nFBIRERGVICkpKbCzs8v1ARa5iYiIQLt27dCiRQuEhoZi1KhRGDx4cK5rxr4PM4VEREREJZREIvlgpnDcuHHYv3+/0pOEevTogefPn+PgwYP5bouZQiIiIiIVSktLQ2JiotKrKJ9CExwcnONRjG3atMnzCU15+SxvNNFx4M0VVPI8Ox/44UpERF8w7WLslaiy7zCuoyzH8839/PwwderUIjl+dHQ0zMzMlMrMzMyQmJiI1NTUfK9C8ll2ComIiIhKCl9fX/j4+CiVSaXSYoomb+wUEhEREUlUN6NOKpWqtBNobm6OmJgYpbKYmBgYGBgUaK1idgqJiIiIJJLijqDQXFxccODAAaWyf/75By4uLgU6Dm80ISIiIipBkpOTERoaitDQUADZS86Ehobi4cOHALKHo/v16yfW//HHH3Hv3j2MHTsWN2/exLJly7Bjxw789NNPBWqXmUIiIiIiFQ4fF9SFCxfQokUL8efX8xH79++PdevWISoqSuwgAkCVKlWwf/9+/PTTT/j1119RoUIF/P7772jTpk2B2v0s1ynk3cdUEvHuYyKi9yvWu4+dCpZVK4jUCwtVduyixEwhERERUSmeU1hUSk6ulIiIiIiKDTOFRERERCVoTmFx4TtARERERMwUEhEREXFOITuFRERERBw+BoePiYiIiAjMFBIRERFx+BjMFBIRERERmCkkIiIi4pxCMFNIRERERGCmkIiIiIhzCsFMIRERERGBmUIiIiIizikEO4VEREREHD4Gh4+JiIiICMwUEhEREXH4GMwUEhERERGYKSQiIiJiphDMFBIRERERmCkkIiIiAtR49zEzhURERETETCERERER5xSyU0hERETExavB4WMiIiIiAjOFRERERBw+BjOFRERERARmComIiIg4pxDMFBIRERERmCkkIiIi4pxCMFNIRERERGCmkIiIiIhzCsFOIRERERGHj8HhYyIiIiICM4VEREREHD4GM4VEREREBGYKiYiIiDinEMwUEhERERGYKSQiIiLinEIwU0hEREREYKaQiIiIiHMKwU4hERERETuF4PAxEREREYGZQiIiIiLeaAJmComIiIgIzBQSERERcU4hmCkkIiIiIjBTSERERMQ5hWCmkIiIiIjATCERERER5xSCnUIiIiIiDh+Dw8dEREREBGYKiYiIiCBhppCZQiIiIiJippCIiIiImUIwU0hEREREYKewRGtc3xq7Fg3FvcMzkHo5EB7NbT+4j5tjdQRtGYfnIQtx7U8/9PFwzlFnaLemuLnfH8/OLsTJDWPgVKeyKsJXMtmzHe4dnoH44AXYv8IL1pVMlLYbGZTB2hn9EXNqLqJOzsFyv17Q1dFSeVxUONu2bMbXX7VEA4d66N3jO1wNC3tv/cOH/kbH9m3RwKEeunTywKmT/yptFwQBS5f8ilbNmqBhfVsMGTQADx7cV+EZAGlpaZj5iz+aujqjkZMDfEZ6Qx4Xp1Qn6skTeHkOgbOjHZq7uWDBvNnIzMxUaVxUOLwm6aNJVPgqJdgpLMF0daS4Gh6JUQHb81W/cnlj7FnyI05eCIdzj1kI3HIcy6f0gruLjVina+v6mD26M2as/BsuvWYjLDwS+5YNh4mRXqHjnDj0G/zm3yfP7aMHuGNYz2YYMXMbmvabh5TUdPxv6XBItd7MXlg7sz9srC3Q3jMQXUasQJP61bB0cq9Cx0Sqc/DvA5g3JwBDhw3Htp17ULNmLXgOHQS5XJ5r/dDLlzD+59Ho/G1XbN+1Fy1atsIo7+G4fTtcrLN29Sps3bwRk/ymYtPWHdDR0YHnkEFIS0srdJyTJ4zH8qVL8tw+d/ZM/HviOOYuWIQ16zciNvYpfEZ6iduzsrLgNWwoMjIysH7TNkyfOQv79u7BssDFhY6JVIPXJK9JKhrsFJZgh89ch/+yv7Dv+Pu/8b72Q9cmuB8px/gFe3ArIgYrtp/EnqOh8O7dQqwzok9LrN0dhI37zuLmvWh4z9iG1Jfp6N/JRaxjqKeDZVN64eGxAMScmou/V3qjXg3LQp/H8F4tMHvVIfx14iqu3X6CwZM3wMLEEB1a2AEAalYxQ5vGdTBs2hacv/YAQaH34DN7J75rUx8WJoaFbpdUY+P6tfi2azd06twF1tWqYZKfP7S1tbF39x+51t+8aQNcm7hhwPeDUdXaGl4jRsGmdm1s27IJQHZGZvPGDfhhqCdatHRHjZq1MD1gDmKfPsWxo0fE40RHReFnn5Fo0sgJbi4NMdLLE5GRjwt1DklJSdjzxx8YM3Y8nBu5oHadupg2fSZCQy8j7EooACA46DTu3b2DmbPmopaNDZq4NcMw75HYvnUzMtLTC9UuqQavSV6TRUEikajsVVqwU/gZcbarguMht5TK/gm6AWfbKgAATQ11ONhUxLG36giCgGMht9DwVR0A2Dx3EEzK6aOT13K49p6D0JuPcWCFN4wMyhQ4JitLY1iYGOJYyE2xLDH5Jc5fuw9nW6vsuG2r4FniC1y6/lCscyzkFhQKAQ3qqn5om/IvIz0dN67/h0YurmKZmpoaGjVyRdiVy7nuExYaikaNXJTKXBs3QVhoKAAg8vFjxMXFwrnRm2Pq6+ujnq2deMyMjAx4DhmEMrq6WLthM9Zv2ooyZcpg2NDBhfpjeP2/a8jMzIDzW+dRpao1LCzK48qruK6EhqJ69RowlsmU4k5OTsadu3cK3CapBq9JXpNFhZ1Cdgo/K2bGBoiJT1IqexqfCEN9HWhLNSEz0oOGhjqevltHnghzYwMAgKt9VTjVqYzeP6/GpesPcfdhLHwX7kFCUio6uzsUOCZzmcGrON5tMwlmr9o0MzZA7Dvbs7IUiE98AbNX+1PJ8Oz5M2RlZcHY2Fip3NjYGHHvzH16LS4uDsbGspz15XGvtsdml8nyPuahgwegEBSYOm0GqteoiarW1pg2PQDRUVE4f/5cgc9DHhcHTU1NGBgoX1/ljI3FeORxcSiXI27Zq22xBW6TVIPXJK9JKjpckoaU1KtRAXplpIg8MVupXEeqiaoVsj98GjtYY2/gMHGblqY6JJAodRq9p2/Ftr8vfJqg6bMXfusmHj18CJcG9ZXK09LS8PhRdoZ5/1/78MtUP3FbRkY6AAnWr10jli1buQr1HZ0+Scz0eeM1+fkpTRk9VWGn8DMSI0+EWTl9pTLTcgZISErFy7QMxD1LRmZmFkzfrWNsgGh5IgBAr4wWouMS0PqHX3McPyEpFQBw8fpDOPcIEMuH92yO8qZlMfHXvWLZU3l25i86LvFVHPrif2e3qY+wW4/FuE3eiUldXQ3lDMog5q19qPgZlTWCurp6jgn8crkcMpks131kMhnk8ric9V9lOGSy7DvR5XFymJiYKtWpWasWAODFixewqV0HAbPn5YypXDkAQPMWLVGvnp1YvmjBPJiamaFX775imamZGQDAWCZDRkYGEhMTlTIz8XK5GI+xTIZrV5Xn874+D2OZ8t3zVHx4TfKapKLD4ePPSMiVCDRvWFOprFWjWggJiwAAZGRm4fKNR2jh/KaORCJBi4Y1cO5Vncs3HsPM2ACZmQrcexSn9JI/TwEAvEzLUCqPT3iBpJSXSmXJL7Lv0LsfKUdUbIJSm/q62mhQ1wohYfez4w6LgJFBGTjYVBTrNG9QA2pqEpy/9qDo3ygqNE0tLdjUroOQs8FimUKhQEhIMGztcp9eYGtvj5CzZ5XKzgYHwdbeHgBgWaECZDIThIS8OWZycjKuhl0Rj2ljUwcPHzxAOWNjVKpcWemlr5/9hUJXV0+pXFdXF4aGhkpl2traAIDadepCQ0MT5946j/sR9xAV9QR2r+Kys7fH7dvhSp2Ns0FB0NPTg7V1tUK+g1TUeE3ymiwqnFPITmGJpqujBdsalrB9deevlaUxbGtYoqK5EQBgmncH/P7Lm2+cq3adRpUKxpgxsiNqWJlhyHdu6PKVA5ZsPi7WWbzpGAZ2dkVvD2fUrGKGxRO6o4yOFBv+zP6APBZyEyFhEdixcAhaNaqFShbl0MiuCqYO90D92pUKdR5LtxzHuMFt0a5ZPdSpVh6rf+mLqNgE7Dt+BQBwKyIGh878h6WTe8GpTmW42FXFwvHdsPPQJUTFJhSqTVKdvv0HYveuHdi3dw/u3b2L6dOmIjU1FZ06fwsAmOg7Fr8unC/W792nH4LOnML6dWsQce8uli9dgv+uXUOPXtnLGEkkEvTu2w+rVi7HiWNHcTv8Fib5joWJqSlatnIHAHzT3gNljYww0ssTly5ewOPHj3D+XAhmzZyOmOjoAp+Dvr4+OnfpgnlzZuFcyFlc/+8apkyaADt7B9ja2QMAXFyboKp1NUwcPxa3bt7EmdOnELhkEbr37A0tLa6hWZLwmuQ1SUWDw8clWP3alXH495Hiz3PGdAEAbNx3FkP8NsFcZoCK5uXE7Q+eyNHZewXmjPkWw3s1R2TMc3hO24IjwTfEOrsOX4LMSA9TPNvBzFgfYbci0XH4UqUbQTp5L4e/lwd+8+8DmZEeYuIScfrSHTyVF24od/66IyijI0XgpJ4oq6+DoNC76DB8GdLS3yy4OnDCeiwc3w0HVnpDoRCw92goRs/ZWaj2SLXafv0NnsXHY1ngYsTFxaJmLRssW/m7eEdkdFQU1CRvvm/aO9RHwJx5CFy8CEsWLUClylZYtGQpqlevIdYZOOgHpKamYtrUKUhKSoRDfUcsW/k7pFIpAEBHRwdr12/CogXz4DPSCykpKTA1M4Ozswt09Qq3xubP4yZATaKG0aNGID0jHa6Nm2DipDfzv9TV1bFk2QrMmDYV/Xp3h46ODjw6dsYwrxGFao9Uh9ckr8kiUXoSeiojEQRBKO4gipqOg9eHKxF9Ys/OBxZ3CEREJZp2MaaqDHttVNmxE7b0/XClEoCZQiIiIvrilaa5f6pSrJ3CuLg4rFmzBsHBwYh+NQfD3Nwcrq6uGDBgAExMeDcVERER0adQbJ3C8+fPo02bNihTpgzc3d1Ro0b2XI6YmBgsXrwYs2bNwqFDh+Dk9P71m9LS0nI8i1JQZEGipq6y2ImIiOjzwkxhMXYKvb298d1332HFihU5fhGCIODHH3+Et7c3goOD8zhCtoCAAPj7+yuVqZs1gKZFwyKPmYiIiD5P7BQW440mOjo6uHz5Mmq9Wgj0XTdv3oSDgwNSU1Pfe5zcMoWmbuOYKaQShzeaEBG9X3HeaFKu7xaVHTt+Yy+VHbsoFds6hebm5jh3Lu/nQ547dw5mr1Z6fx+pVAoDAwOl1+fWITy0aiRSLwci9XKguGbh56iPh7N4nnNfLb9DJdegAX1hV6cm7OrUxM0bNz68Qyk1ecJ48TyPHT1S3OHQe/CapI/BxauLsVM4ZswYDBkyBCNHjsS+ffsQEhKCkJAQ7Nu3DyNHjsSPP/6IsWPHFld4Jc7qP87Ayt0X/92NEsvmj+2KM5vH4nnIQpzdNj5fx5FqaWDh+G54fHw2Ys/Mx9Z5g3M89u5DzGUGWDdzAML2TkHKxcX57sBVNDfC7sU/Qh60AA+OBmDmqE5QV39zCe46fAlW7r44e+VegeKh4tOlazccPXEa1apXF8uinjyBl+cQODvaobmbCxbMm43MzMz3HAVIeP4cvmNHw7VhfTRp5AS/yRPwIiWlwPEcPvQ3OrZviwYO9dClkwdOnfz3g/ucPxeC7l07w8m+Ltq3/Qp/7tmttH2s70QcPXG6wLFQ8eA1SVR4xdYpHD58ONavX4+QkBB06dIFLi4ucHFxQZcuXRASEoJ169Zh2LBhxRVeiZP6Mh0x8iRkZSmUyjf8eRa7Dl/K93HmjOmCdk3rovfY1Wg9eBEsTAyxbf7gAsWipamBuGdJmPX7QYSFR+ZrHzU1CXYv9oSWpgZaDJiPH6ZsRJ8Ozpji2U6s8zItAzHyJKRnZBUoHio+2trakJmYQEMje8wnKysLXsOGIiMjA+s3bcP0mbOwb+8eLAtc/N7j+I4bg7t37mDF72uxeOkKXLpwAdOmTilQLKGXL2H8z6PR+duu2L5rL1q0bIVR3sNx+3Z4nvs8fvwIXsOGokFDZ+z440/07tsf/n6TcOb0KbGOvr4+ZFwJodTgNUmFJlHhq5Qo1sfcde/eHWfPnsWLFy8QGRmJyMhIvHjxAmfPnkW3bt2KM7RSYfScXVi54yQiHss/XBmAgZ42BnRywbgFu/Hv+XBcvvEIQ/w2wcXeGg3rWeW73YdR8Rgz9w9s+escEpNf5msfdxcb2FQ1x/cT1yMsPBKHz1zHtGX7MbRbU2hqfF7D/V+y4KDTuHf3DmbOmotaNjZo4tYMw7xHYvvWzchIT891n3t37+LM6VPwmzYdtrZ2qO/ohPETJuHg3/vx9GlMvtvevGkDXJu4YcD3g1HV2hpeI0bBpnZtbNuyKc99dm7fBkvLChgzdjyqWlujZ+8+cG/dBps2rCvoqVMJxWuSKP9KxLOPNTU1YWFhAQsLC2hqahZ3OJ8tB5tK0NLUwLGzt8Sy8PsxeBgVD2fbKipt29m2Cq7deaL0OL1/gm7AUF8Hta0tVNo2fTpXQkNRvXoN8fFiAODauAmSk5Nx5+6d3Pe5chn6BgaoU7eeWObs4go1NTVcDQvLd9thoaFo1MhFqcy1cROEhYbmvc+VPPa5kvc+VLrwmqT84pzCEtIppE/D3NgAaekZSEhWvqP7qTwRZsYGKm3bzNgAT+VJSmVP47OfpWwmU23b9OnI4+JQzlimVGb86md5XGze+5Qrp1SmoaEBA0PDPPfJTVxcnNjWm7aNESePe/8+spzxJicn4+XL/GXBqWTjNUmUf3zMHREREX3xSlNGT1WYKfyCRMsTIdXShKGejlK5qbEBYuSJKm07Rp4IU2Plu5xNy2VnCGPiVNs2fTrGMhni38mCyF/9bCzLfWK8sUyG+Ph4pbLMzEwkJiTkuU9uZDKZ2NabtuWQvZOpybFPXM549fT0oK2tne+2qeTiNUn5xeFjdgq/KJdvPER6RiZaONcUy6pXNkUli3IICYtQadshYRGoW608TIz0xLJWjWohISkVN+5Fq7Rt+nTs7O1x+3Y45PI3Nz+dDQqCnp4erK2r5b6PnQOSEhNx/b9rYtm5kLNQKBSoZ2ub77Zt7e0RcvasUtnZ4CDY2tvnvY+dPUJC3tknKAi2dnnvQ6ULr0mi/GOnsBSrWlEG2xqWMJMZQEeqCdsalrCtYSnezVvexBChuyfBqU5lAEBi8kus2xuM2aO/RVOn6nCwqYjf/Pvg7JV7OHf1foHaft2WbhkpZEZ6sK1hiVpVzcXtHVrYInT3JPHnI8E3cONeNFZP7496NSzh7mIDv+HtsXLHSaRnvH+9MCo9XFyboKp1NUwcPxa3bt7EmdOnELhkEbr37A0tLS0AwNWwMHRs3xYxMdl3cVa1tkbjJm7w95uMq2FhuHzpIgJm/IK2X7eDqemHF7B/rXeffgg6cwrr161BxL27WL50Cf67dg09evUR6/y6cD4m+r5Z//S77j3w+PEjLJw3BxH37mL71s04fOhv9Ok3oGjeECp2vCYp30rYkjRLly6FlZUVtLW14ezs/N4HfgDAokWLULNmTejo6KBixYr46aefCjwPlXMKS7HlU3qjqdObBVpDtvsCAGp+MwUPo+KhoaGOmlXMoaOtJdYZO+8PKBQCts4bDKmWBo4E3cDIgO1Kx7253x8b94VgxsoDebb9ui0AcKxdCT2+aYAHT+So1c4PAGCgp4OaVd50EhUKAV1GLsevE3rgxLrRSHmZhs3/O4dpy/d/3JtAJYq6ujqWLFuBGdOmol/v7tDR0YFHx84Y5jVCrPPyZSruR0QgMzNDLAuYPQ8BM37BkEH9oaamhlZftcZ430lKx7arUxPTpgegY+dvc23b3qE+AubMQ+DiRViyaAEqVbbCoiVLUb16DbFOXGwsoqPeLABfoUJFBC5bibmzA7B50waYmZvDz386GjdxK6J3hIobr0kqjbZv3w4fHx+sWLECzs7OWLRoEdq0aYNbt27B1NQ0R/0tW7Zg/PjxWLNmDVxdXREeHo4BAwZAIpFgwYIF+W632J59rEo6Dl7FHUKROrRqJMJuPcbP8/5QeVs62pqIPD4bHb2W49TF2ypvLzef8nw/pc/t2ceDBvRFzZq1MNZ3osrbevz4ETq2a4vd+/ajcmUrlbeXG7s6NbFw8VK0bOVeLO3Th/GaLP2K89nHZoN3quzYMb9/V6D6zs7OaNCgAQIDs/9uKBQKVKxYEd7e3hg/PucTzLy8vHDjxg0cPXpULBs9ejRCQkJw+nT+n37D4eNSYkg3N8SemY861cqrtJ1mTjVw4nx4sXQIe3zthNgz89HYwfqTt02Fs33bVjRycsDt8FsfrvwRTp88iS5duxXLH99f/KegkZPDJ2+XCofXJJVEaWlpSExMVHqlpaXlWjc9PR0XL16Eu/ubzr6amhrc3d0RHByc6z6urq64ePGiOMR87949HDhwAN98802B4mSmsBQob2IIbe3sRb0fRT1DRubn+Rg4vTJS8Q7lhKRUyJ8X/DmjJdnnlimMiYlB2qv5KhYWFtDU0vrAHqWTXC5HSnIyAEBmYoIyZcoUc0SUF16TpV9xZgrNf9ilsmP/aHkN/v7+SmV+fn6YOnVqjrpPnjyBpaUlgoKC4OLyZiHzsWPH4t9//0VISEiubSxevBhjxoyBIAjIzMzEjz/+iOXLlxcoTs4pLAWexCYUdwifRPKLNCS/yP2bE5U8Zmb5n3BfmhkbG8PY2Li4w6B84DVJJZWvry98fHyUyqRSaZEd/8SJE5g5cyaWLVsGZ2dn3LlzByNHjsQvv/yCyZMn5/s47BQSERHRF0+V6wlKpdJ8dwJlMhnU1dXFu+Ffi4mJgbm5ea77TJ48GX379sXgwYMBAPXq1UNKSgqGDBmCiRMnQk0tf7MFOaeQiIiIvnglZfFqLS0tODo6Kt00olAocPToUaXh5Le9ePEiR8dPXT17ebqCzBJkppCIiIioBPHx8UH//v3h5OSEhg0bYtGiRUhJScHAgQMBAP369YOlpSUCAgIAAB4eHliwYAEcHBzE4ePJkyfDw8ND7BzmBzuFRERERCXoaXTdu3dHbGwspkyZgujoaNjb2+PgwYPivNmHDx8qZQYnTZoEiUSCSZMmITIyEiYmJvDw8MCMGTMK1C7vPib6RD63u4+JiIpacd59XP7H3So79pMVuS9wXtIwU0hERERfPFXeaFJa8EYTIiIiImKmkIiIiIiZQmYKiYiIiAjMFBIRERExUwh2ComIiIhK1JI0xYXDx0RERETETCERERERh4+ZKSQiIiIiMFNIRERExEwhmCkkIiIiIjBTSERERMRMIZgpJCIiIiIwU0hERETETCHYKSQiIiLi4tXg8DERERERgZlCIiIiIg4fg5lCIiIiIgIzhURERETMFIKZQiIiIiICM4VEREREYKKQmUIiIiIiAjOFRERERJxTCHYKiYiIiDh8DA4fExERERGYKSQiIiLi8DGYKSQiIiIiMFNIRERExDmFYKaQiIiIiMBMIRERERHU1JgqZKaQiIiIiJgpJCIiIuKcQnYKiYiIiLgkDTh8TERERERgppCIiIiIw8dgppCIiIiIwEwhEREREecUgplCIiIiIgIzhURERETMFIKZQiIiIiICM4VEREREvPsY7BQSERERcfgYHD4mIiIiIjBTSERERMThYzBTSERERERgppCIiIiIcwrBTCERERERgZlCIiIiIs4pBDOFRERERARmComIiIg4pxDMFBIRERERmCkkIiIi4pxCsFNIRERExOFjcPiYiIiIiMBMIRERERGHj/GZdgqfnQ8s7hCIcjBq4FXcIRAp4WclEb3ts+wUEhERERUE5xRyTiERERERgZlCIiIiIs4pBDOFRERERARmComIiIg4pxDsFBIRERFx+BgcPiYiIiIiMFNIRERExOFjMFNIRERERGCmkIiIiIiZQjBTSERERERgppCIiIiIdx+DmUIiIiIiAjOFRERERJxTCHYKiYiIiDh8DA4fExERERGYKSQiIiLi8DGYKSQiIiIiMFNIRERExDmFYKaQiIiIiMBMIRERERHUmCpkppCIiIiImCkkIiIi4pxCsFNIRERExCVpwOFjIiIiIgIzhURERERQY6KQmUIiIiKikmbp0qWwsrKCtrY2nJ2dce7cuffWf/78OYYPHw4LCwtIpVLUqFEDBw4cKFCbzBQSERHRF68kzSncvn07fHx8sGLFCjg7O2PRokVo06YNbt26BVNT0xz109PT8dVXX8HU1BS7du2CpaUlHjx4gLJlyxaoXXYKiYiIiEqQBQsW4IcffsDAgQMBACtWrMD+/fuxZs0ajB8/Pkf9NWvWID4+HkFBQdDU1AQAWFlZFbhdDh8TERHRF08iUd0rLS0NiYmJSq+0tLRc40hPT8fFixfh7u4ulqmpqcHd3R3BwcG57rNv3z64uLhg+PDhMDMzQ926dTFz5kxkZWUV6D1gp5CIiIhIhQICAmBoaKj0CggIyLVuXFwcsrKyYGZmplRuZmaG6OjoXPe5d+8edu3ahaysLBw4cACTJ0/G/PnzMX369ALFyeFjIiIi+uJJoLo5hb6+vvDx8VEqk0qlRXZ8hUIBU1NT/Pbbb1BXV4ejoyMiIyMxd+5c+Pn55fs47BQSERHRF0+VS9JIpdJ8dwJlMhnU1dURExOjVB4TEwNzc/Nc97GwsICmpibU1dXFMhsbG0RHRyM9PR1aWlr5apvDx0REREQlhJaWFhwdHXH06FGxTKFQ4OjRo3Bxccl1n8aNG+POnTtQKBRiWXh4OCwsLPLdIQTYKSQiIiKCRCJR2augfHx8sGrVKqxfvx43btyAp6cnUlJSxLuR+/XrB19fX7G+p6cn4uPjMXLkSISHh2P//v2YOXMmhg8fXqB2OXxMREREVIJ0794dsbGxmDJlCqKjo2Fvb4+DBw+KN588fPgQampv8noVK1bEoUOH8NNPP8HW1haWlpYYOXIkxo0bV6B2JYIgCEV6JiXAy8zijoAoJ6MGXsUdApGSZ+cDizsEIiXaxZiq6vT7BZUde+9gJ5Uduyhx+JiIiIiIOHxMREREpFaCHnNXXJgpJCIiIiJmComIiIiYKGSnkIiIiKhQS8d8bjh8TERERETMFBIRERExUchMIRERERGBmUIiIiIiLkkDZgqJiIiICMwUEhEREYF5QmYKiYiIiAjMFBIRERFxnUKwU0hEREQENfYJOXxMRERERMwUEhEREXH4GMwUEhERERGYKSQiIiLiY+7ATCERERERgZlCIiIiIs4pBDOFRERERIRCdApbtmyJ58+f5yhPTExEy5YtiyImIiIiok9KTaK6V2lR4OHjEydOID09PUf5y5cvcerUqSIJioiIiOhT4vBxATqFYWFh4n9fv34d0dHR4s9ZWVk4ePAgLC0tizY6IiIiIvok8t0ptLe3h0QigUQiyXWYWEdHB0uWLCnS4IiIiIg+BeYJCzCnMCIiAnfv3oUgCDh37hwiIiLEV2RkJBITE/H999+rMtYv1rYtm/H1Vy3RwKEeevf4Dlffytrm5vChv9GxfVs0cKiHLp08cOrkv0rbBUHA0iW/olWzJmhY3xZDBg3Agwf3VXgGQFpaGmb+4o+mrs5o5OQAn5HekMfFKdWJevIEXp5D4Oxoh+ZuLlgwbzYyMzNVGhcVXOP61ti1aCjuHZ6B1MuB8Ghu+8F93ByrI2jLODwPWYhrf/qhj4dzjjpDuzXFzf3+eHZ2IU5uGAOnOpVVEb6SyZ7tcO/wDMQHL8D+FV6wrmSitN3IoAzWzuiPmFNzEXVyDpb79YKujpbK46KC4+ck0cfLd6ewcuXKsLKygkKhgJOTEypXriy+LCwsoK6urso4v1gH/z6AeXMCMHTYcGzbuQc1a9aC59BBkMvludYPvXwJ438ejc7fdsX2XXvRomUrjPIejtu3w8U6a1evwtbNGzHJbyo2bd0BHR0deA4ZhLS0tELHOXnCeCxfmnemeO7smfj3xHHMXbAIa9ZvRGzsU/iM9BK3Z2VlwWvYUGRkZGD9pm2YPnMW9u3dg2WBiwsdE6mGro4UV8MjMSpge77qVy5vjD1LfsTJC+Fw7jELgVuOY/mUXnB3sRHrdG1dH7NHd8aMlX/DpddshIVHYt+y4TAx0it0nBOHfoPf/PvkuX30AHcM69kMI2ZuQ9N+85CSmo7/LR0OqdabAZS1M/vDxtoC7T0D0WXECjSpXw1LJ/cqdEykGvyc5OdkUVCTSFT2Ki0KtSTN7du38dtvv2H69OmYNm2a0ouK1sb1a/Ft127o1LkLrKtVwyQ/f2hra2Pv7j9yrb950wa4NnHDgO8Ho6q1NbxGjIJN7drYtmUTgOxvv5s3bsAPQz3RoqU7atSshekBcxD79CmOHT0iHic6Kgo/+4xEk0ZOcHNpiJFenoiMfFyoc0hKSsKeP/7AmLHj4dzIBbXr1MW06TMRGnoZYVdCAQDBQadx7+4dzJw1F7VsbNDErRmGeY/E9q2bkZHLjU1UfA6fuQ7/ZX9h3/H3Z2Je+6FrE9yPlGP8gj24FRGDFdtPYs/RUHj3biHWGdGnJdbuDsLGfWdx8140vGdsQ+rLdPTv5CLWMdTTwbIpvfDwWABiTs3F3yu9Ua9G4ecxD+/VArNXHcJfJ67i2u0nGDx5AyxMDNGhhR0AoGYVM7RpXAfDpm3B+WsPEBR6Dz6zd+K7NvVhYWJY6Hap6PFzkp+TVDQK3ClctWoVbGxsMGXKFOzatQt79uwRX3v37lVBiF+ujPR03Lj+Hxq5uIplampqaNTIFWFXLue6T1hoKBo1clEqc23cBGGhoQCAyMePERcXC+dGb46pr6+PerZ24jEzMjLgOWQQyujqYu2GzVi/aSvKlCmDYUMHF+qD5/p/15CZmQHnt86jSlVrWFiUx5VXcV0JDUX16jVgLJMpxZ2cnIw7d+8UuE0qOZztquB4yC2lsn+CbsDZtgoAQFNDHQ42FXHsrTqCIOBYyC00fFUHADbPHQSTcvro5LUcrr3nIPTmYxxY4Q0jgzIFjsnK0hgWJoY4FnJTLEtMfonz1+7D2dYqO27bKniW+AKXrj8U6xwLuQWFQkCDuqof2qb84eckPyeLikSiuldpUeAlaaZPn44ZM2Zg3LhxqoiH3vLs+TNkZWXB2NhYqdzY2BgREfdy3ScuLg7GxrIc9ePkca+2x2aXyXIeM+7V3JVDBw9AISgwddoM8Rb9adMD0MSlAc6fPwfXxk0KdB7yuDhoamrCwMBAqbycsbEYjzwuDuVyxC17tS22QO1RyWJmbICY+CSlsqfxiTDU14G2VBNGBmWgoaGOp+/WkSeippUZAMDVviqc6lRGpVa+SM/Inj/lu3APPJrborO7A9bsPlOgmMxlBq/ieLfNJJgZG4hxx76zPStLgfjEFzCTKV/LVHz4OcnPSSo6Be4UPnv2DN99950qYqESIvzWTTx6+BAuDeorlaelpeHxo+ysyf6/9uGXqX7itoyMdAASrF+7RixbtnIV6js6fZKY6fNWr0YF6JWRIvLEbKVyHakmqlbI/qPY2MEaewOHidu0NNUhgQSd3R3EMu/pW7Ht7wufJmj6rPFz8vPDdQoL0Sn87rvvcPjwYfz444+qiIfeYlTWCOrq6jkmS8vlcshkslz3kclkkMvjctZ/9W1SJsu+u1IeJ4eJialSnZq1agEAXrx4AZvadRAwe17OmMqVAwA0b9ES9erZieWLFsyDqZkZevXuK5aZmmVneYxlMmRkZCAxMVHpW3C8XC7GYyyT4dpV5Tlqr8/DWKZ8RyiVLjHyRJiV01cqMy1ngISkVLxMy0Dcs2RkZmbB9N06xgaIlicCAPTKaCE6LgGtf/g1x/ETklIBABevP4RzjwCxfHjP5ihvWhYTf90rlj2VZ2f+ouMSX8WhL/53dpv6CLv1WIzb5J2Y1NXVUM6gDGLe2oeKFz8n+TlJRafAncJq1aph8uTJOHv2LOrVqwdNTU2l7SNGjCiy4L50mlpasKldByFng9GylTsAQKFQICQkGD165n5Xpa29PULOnkWffgPEsrPBQbC1twcAWFaoAJnMBCEhwahlk333Z3JyMq6GXcF33XsCAGxs6uDQ33+jnLEx9PRyv/tTV1cPurp6b/2sC0NDQ1SqnHOuVe06daGhoYlzZ4Ph3roNAOB+xD1ERT2B3au47Ozt8ftvKyCXy8VhoLNBQdDT04O1dbV8vmNUEoVciUCbJnWUylo1qoWQsAgAQEZmFi7feIQWzjXxvxPZf/AkEglaNKyBFdtPAgAu33gMM2MDZGYq8DAqPtd2XqZl4N6jN3/o4xNeQF9XW6nstfuRckTFJqCFc02EhUcCAPR1tdGgrhVW7TydHXdYBIwMysDBpiIu33gEAGjeoAbU1CQ4f+3Bx7wlVIT4OcnPyaLCRGEhOoW//fYb9PT08O+//+Lff5XXdZJIJOwUFrG+/Qdi8oRxqFOnLurWs8WmjeuRmpqKTp2/BQBM9B0LU1MzjPxpNACgd59+GDSgL9avW4OmTZvh4N8H8N+1a5g8NfvOcIlEgt59+2HVyuWoXKkyLCtUwNIlv8LE1FT8QP2mvQfWrV2NkV6eGO49EqZmZoh68gRHj/yDgd8Phpm5eYHOQV9fH527dMG8ObNgYGgIPT09zJo5HXb2DrC1swcAuLg2QVXrapg4fix+Gv0z4uJiEbhkEbr37A0tLa4LV5Lo6mjBuuKbrISVpTFsa1jiWeILPIp+hmneHVDe1BCDJ28EAKzadRo/9miKGSM7Yv2fZ9G8QQ10+coBnUesEI+xeNMxrJrWFxevP8SFa/fh1asFyuhIseHPswCAYyE3ERIWgR0Lh2Dior24/eApypsaom2Tuth3/IrSzSD5tXTLcYwb3BZ3HsbifqQcfsPaISo2AfuOXwEA3IqIwaEz/2Hp5F4YMWMbNDXUsXB8N+w8dAlRsQkf8xZSEePnJD8ni0JpWjpGVQrcKYyIiFBFHJSHtl9/g2fx8VgWuBhxcbGoWcsGy1b+Lt59Fh0VBTXJm5vI7R3qI2DOPAQuXoQlixagUmUrLFqyFNWr1xDrDBz0A1JTUzFt6hQkJSXCob4jlq38HVKpFED202nWrt+ERQvmwWekF1JSUmBqZgZnZxfo5vGN+EN+HjcBahI1jB41AukZ6XBt3AQTJ72Za6Ouro4ly1ZgxrSp6Ne7O3R0dODRsTOGefFLRklTv3ZlHP59pPjznDFdAAAb953FEL9NMJcZoKJ5OXH7gydydPZegTljvsXwXs0RGfMcntO24EjwDbHOrsOXIDPSwxTPdjAz1kfYrUh0HL5U6UaQTt7L4e/lgd/8+0BmpIeYuEScvnQHT+WFG8qdv+4IyuhIETipJ8rq6yAo9C46DF+GtPQ3CwEPnLAeC8d3w4GV3lAoBOw9GorRc3YWqj1SHX5O8nOSioZEEAShMDump6cjIiIC1tbW0NAocN9SpV5ycXcqgYwaeH24EtEn9Ox8YHGHQKREuxi7E8N2X1fZsZd9W1tlxy5KBV6n8MWLFxg0aBDKlCmDOnXq4OHD7GEbb29vzJo1q8gDJCIiIiLVK3Cn0NfXF1euXMGJEyegra0tlru7u2P79vw99oqIiIioJJFIJCp7lRYF7hTu3bsXgYGBaNKkidKJ1qlTB3fv3i3S4B49eoTvv//+vXXS0tKQmJio9PqYZ1MSERERfYkK3CmMjY2FqalpjvKUlJQi7w3Hx8dj/fr1760TEBAAQ0NDpdfc2QHv3YeIiIjobWoqfJUWBZ7S6eTkhP3798Pb2xvAmxXAf//9d7i4uLxv1xz27dv33u337uX+iKK3+fr6wsfHR6lMUJcWKA4iIiKiL12BO4UzZ87E119/jevXryMzMxO//vorrl+/jqCgoBzrFn5Ip06dIJFI8L4boD+UfZRKpeISAa99bncfDxrQFxfOnwMAbN+1V1xM9XMzecJ47PtzDwBg4eKl4npgVPIcWjUSTZ2qAwCcuweIC0B/bvp4OGPVtOynTwRuPo6f5/1RzBHR+/Czkj5GaZr7pyoFzmo2adIEoaGhyMzMRL169XD48GGYmpoiODgYjo6OBTqWhYUFdu/eDYVCkevr0qVLBQ3vs9WlazccPXEa1apXF8uinjyBl+cQODvaobmbCxbMm43MzPf3iBOeP4fv2NFwbVgfTRo5wW/yBLxISSlwPIcP/Y2O7duigUM9dOnkgVMnP/yF4Py5EHTv2hlO9nXRvu1X+HPPbqXtY30n4uiJ0wWOhYrH6j/OwMrdF//djRLL5o/tijObx+J5yEKc3TY+X8eRamlg4fhueHx8NmLPzMfWeYNzPPLuQ8xlBlg3cwDC9k5BysXFmPtq7cQPqWhuhN2Lf4Q8aAEeHA3AzFGdoK7+5mNx1+FLsHL3xdkrHx61oJKBn5VUWGoS1b1Ki0INdVtbW2PVqlU4d+4crl+/jk2bNqFevXoFPo6joyMuXryY5/YPZRG/JNra2pCZmIhrQmZlZcFr2FBkZGRg/aZtmD5zFvbt3YNlgYvfexzfcWNw984drPh9LRYvXYFLFy5g2tQpBYol9PIljP95NDp/2xXbd+1Fi5atMMp7OG7fDs9zn8ePH8Fr2FA0aOiMHX/8id59+8PfbxLOnD4l1tHX14fMhM/vLC1SX6YjRp6ErCyFUvmGP89i1+H8f6GbM6YL2jWti95jV6P14EWwMDHEtvmDCxSLlqYG4p4lYdbvB/OdtVRTk2D3Yk9oaWqgxYD5+GHKRvTp4Iwpnu3EOi/TMhAjT0J6RlaB4qHiw89KosIrcKfw3Tt9X7+SkpKQnp5eoGP9/PPPcHV1zXN7tWrVcPz48YKG+EUIDjqNe3fvYOasuahlY4Mmbs0wzHsktm/djIw8fg/37t7FmdOn4DdtOmxt7VDf0QnjJ0zCwb/34+nTmHy3vXnTBrg2ccOA7wejqrU1vEaMgk3t2ti2ZVOe++zcvg2WlhUwZux4VLW2Rs/efeDeug02bVhX0FOnEmz0nF1YueMkIh7L81XfQE8bAzq5YNyC3fj3fDgu33iEIX6b4GJvjYb1rPLd7sOoeIyZ+we2/HUOickv87WPu4sNbKqa4/uJ6xEWHonDZ65j2rL9GNqtKTQ11PPdNpVs/Kyk/GKmsBCdwrJly8LIyCjHq2zZstDR0UHlypXh5+cHhULxwWO5ubmhbdu2eW7X1dVFs2bNChriF+FKaCiqV68hPsYJAFwbN0FycjLu3L2T+z5XLkPfwAB16r7J6jq7uEJNTQ1Xw8Ly3XZYaCgaNVK+qci1cROEhYbmvc+VPPa5kvc+9PlzsKkELU0NHDt7SywLvx+Dh1HxcLatotK2nW2r4NqdJ0qP0vsn6AYM9XVQ29pCpW3Tp8PPSqL8K/CNJuvWrcPEiRMxYMAANGzYEABw7tw5rF+/HpMmTUJsbCzmzZsHqVSKCRMmFHnAlE0eF4dyxjKlMuNXP8vjYvPep1w5pTINDQ0YGBrmuU9u4uLixLbetG2MOHnc+/eR5Yw3OTkZL1++VFoInb4c5sYGSEvPQEJyqlL5U3kizIwNVNq2mbEBnsqTlMqexmc/R9lMZgDcym0vKm34WUn5xRtNCtEpXL9+PebPn49u3bqJZR4eHqhXrx5WrlyJo0ePolKlSpgxYwY7hURERESlRIGHj4OCguDg4JCj3MHBAcHBwQCy71B+/UxkUg1jmQzx73zblL/62ViW+wRkY5kM8fHxSmWZmZlITEjIc5/cyGQysa03bcshe+cbcY594nLGq6enx2++X7BoeSKkWpow1NNRKjc1NkCMPFGlbcfIE2FqrHyXs2m57OxkTJxq26ZPh5+VlF+cU1iITmHFihWxevXqHOWrV69GxYoVAWRf9EZGRh8fHeXJzt4et2+HQy5/M6H/bFAQ9PT0YG1dLfd97ByQlJiI6/9dE8vOhZyFQqFAPVvbfLdta2+PkLNnlcrOBgfB1t4+733s7BES8s4+QUGwtct7H/r8Xb7xEOkZmWjhXFMsq17ZFJUsyiEkLEKlbYeERaButfIwMdITy1o1qoWEpFTcuBet0rbp0+FnJVH+FbhTOG/ePCxcuBB2dnYYPHgwBg8eDHt7eyxcuBDz588HAJw/fx7du3cv8mDpDRfXJqhqXQ0Tx4/FrZs3ceb0KQQuWYTuPXtDS0sLAHA1LAwd27dFTEz23XJVra3RuIkb/P0m42pYGC5fuoiAGb+g7dftYGpqlu+2e/fph6Azp7B+3RpE3LuL5UuX4L9r19CjVx+xzq8L52Oi71jx5++698Djx4+wcN4cRNy7i+1bN+Pwob/Rp9+AonlDqESoWlEG2xqWMJMZQEeqCdsalrCtYSnezVvexBChuyfBqU5lAEBi8kus2xuM2aO/RVOn6nCwqYjf/Pvg7JV7OHf1foHaft2WbhkpZEZ6sK1hiVpVzcXtHVrYInT3JPHnI8E3cONeNFZP7496NSzh7mIDv+HtsXLHSaRnfGYr4H/B+FlJ+SWRqO5VWhR4TmGHDh1w69YtrFixAuHh2Wstff3119i7dy+Sk5MBAJ6enkUbJeWgrq6OJctWYMa0qejXuzt0dHTg0bEzhnmNEOu8fJmK+xERyMzMEMsCZs9DwIxfMGRQf6ipqaHVV60x3neS0rHt6tTEtOkB6Nj521zbtneoj4A58xC4eBGWLFqASpWtsGjJUlSvXkOsExcbi+ioN4saV6hQEYHLVmLu7ABs3rQBZubm8POfjsZN3IroHaGSYPmU3uKTTgAgZLsvAKDmN1PwMCoeGhrqqFnFHDraWmKdsfP+gEIhYOu8wZBqaeBI0A2MDNiudNyb+/2xcV8IZqw8kGfbr9sCAMfaldDjmwZ48ESOWu38AAAGejqoWeVNJ1GhENBl5HL8OqEHTqwbjZSXadj8v3OYtnz/x70JVKLws5LyS6009d5URCJ85OrQiYmJ2Lp1K9asWYMLFy4gK6v4F3n9HB9zV7NmLYz1najyth4/foSO7dpi9779qFzZSuXt5cauTs3P8tFNRg28ijuEInNo1UiE3Xr8SR77pqOticjjs9HRazlOXbyt8vZy8ynP91N6dj6wuEMoUvysLP20C5yqKjrjD+S9qPjHmvVNjQ9XKgEK9UQTADh58iT69++P8uXLY/78+WjRogXOvjN3gorO9m1b0cjJAbfDVbtOxumTJ9Gla7di+ZD7xX8KGjnlvImJSqYh3dwQe2Y+6lQrr9J2mjnVwInz4cXSIezxtRNiz8xHYwfrT942FQ4/K6mw1FT4Ki0KlCmMjo7GunXrsHr1aiQmJqJbt25YsWIFrly5gtq1a6syzgL53DKFMTExSHuZ/ZQGCwsLaGppfWCP0kkulyPl1RQEmYkJypQpU8wRFa3PKVNY3sQQ2tqaAIBHUc+QkVn8IwSqoFdGKt6hnJCUCvnzgj/7tiT73DKF/Kws/YozUzhBhZnCmaUkU5jvt9/DwwMnT55Eu3btsGjRIrRt2xbq6upYsWKFKuMjAGZm+Z/YXJoZGxvD2Ni4uMOgfHgSm1DcIXwSyS/SkPwirbjDoHziZyV9DE4pLECn8O+//8aIESPg6emJ6tWrf3gHIiIiIio18j3Uffr0aSQlJcHR0RHOzs4IDAxEXFzej+ohIiIiKi3UJBKVvUqLfHcKGzVqhFWrViEqKgpDhw7Ftm3bUL58eSgUCvzzzz9ISkr68EGIiIiIqEQq8E0xurq6+P7773H69GlcvXoVo0ePxqxZs2BqaooOHTqoIkYiIiIileLi1R95p3TNmjUxZ84cPH78GFu3bi2qmIiIiIg+KT77uIiWz1FXV0enTp2wb9++ojgcEREREX1ixbgiEBEREVHJUJpuCFGV0rTQNhERERGpCDOFRERE9MVjopCZQiIiIiICM4VEREREpeouYVVhppCIiIiImCkkIiIikoCpQnYKiYiI6IvH4WMOHxMRERERmCkkIiIiYqYQzBQSEREREZgpJCIiIoKEq1czU0hEREREzBQSERERcU4hmCkkIiIiIjBTSERERAROKWSnkIiIiAhq7BVy+JiIiIiImCkkIiIi4o0mYKaQiIiIqMRZunQprKysoK2tDWdnZ5w7dy5f+23btg0SiQSdOnUqcJvsFBIREdEXTyJR3augtm/fDh8fH/j5+eHSpUuws7NDmzZt8PTp0/fud//+fYwZMwZubm6Feg/YKSQiIiIqQRYsWIAffvgBAwcORO3atbFixQqUKVMGa9asyXOfrKws9O7dG/7+/qhatWqh2mWnkIiIiL54apCo7JWWlobExESlV1paWq5xpKen4+LFi3B3d38Tm5oa3N3dERwcnGf806ZNg6mpKQYNGvQR7wERERERqUxAQAAMDQ2VXgEBAbnWjYuLQ1ZWFszMzJTKzczMEB0dnes+p0+fxurVq7Fq1aqPipN3HxMREdEXT5XLFPr6+sLHx0epTCqVFsmxk5KS0LdvX6xatQoymeyjjsVOIREREX3xVLkkjVQqzXcnUCaTQV1dHTExMUrlMTExMDc3z1H/7t27uH//Pjw8PMQyhUIBANDQ0MCtW7dgbW2dr7Y5fExERERUQmhpacHR0RFHjx4VyxQKBY4ePQoXF5cc9WvVqoWrV68iNDRUfHXo0AEtWrRAaGgoKlasmO+2mSkkIiKiL15Jesydj48P+vfvDycnJzRs2BCLFi1CSkoKBg4cCADo168fLC0tERAQAG1tbdStW1dp/7JlywJAjvIPYaeQiIiIqATp3r07YmNjMWXKFERHR8Pe3h4HDx4Ubz55+PAh1NSKfrBXIgiCUORHLWYvM4s7AqKcjBp4FXcIREqenQ8s7hCIlGgXY6pqVcgDlR37B+fKKjt2UeKcQiIiIiLi8DERERFRSZpTWFyYKSQiIiIiZgqJiIiImChkp5CIiIiIQ6fge0BEREREYKaQiIiICBKOHzNTSERERETMFBIRERGBeUJmComIiIgIzBQSERERcfFqMFNIRERERGCmkIiIiIhzCsFOIRERERGfaAIOHxMRERERmCkkIiIi4uLVYKaQiIiIiMBMIRERERGzZOB7QERERERgppCIiIiIcwrBTCERERERgZlCIiIiIi5eDWYKiYiIiAjMFBIRERFxTiHYKST6ZJ6dDyzuEIiUGDXwKu4QiJSkXi6+z0kOnfI9ICIiIiIwU0hERETE4WMwU0hEREREYKaQiIiIiEvSgJlCIiIiIgIzhURERETglEJmComIiIgIzBQSERERQY2zCtkpJCIiIuLwMYePiYiIiAjMFBIRERFBwuFjZgqJiIiIiJlCIiIiIs4pBDOFRERERARmComIiIi4JA2YKSQiIiIiMFNIRERExDmFYKeQiIiIiJ1CcPiYiIiIiMBMIREREREXrwYzhUREREQEZgqJiIiIoMZEITOFRERERMRMIRERERHnFIKZQiIiIiICM4VEREREXKcQ7BQSERERcfgYHD4mIiIiIjBTSERERMQlacBMIRERERGBmUIiIiIizikEM4VEREREBGYKiYiIiLgkDZgpJCIiIiIwU0hERETEGYVgp5CIiIgIahw/5vAxERERETFTSERERMThYzBTSERERERgppCIiIiIqUIwU0hEREREYKaQiIiIiI+5AzOFRERERARmComIiIj4mDuwU0hERETEwWNw+JiIiIiIwEwhEREREVOFYKaQiIiIiMBMIRERERGXpAEzhUREREQEZgqJiIiIuCQNmCkkIiIiIjBTSERERMQZhWCnkIiIiIi9QnD4mIiIiIjATCERERERl6QBM4VEREREJc7SpUthZWUFbW1tODs749y5c3nWXbVqFdzc3GBkZAQjIyO4u7u/t35e2CkkIiKiL55EorpXQW3fvh0+Pj7w8/PDpUuXYGdnhzZt2uDp06e51j9x4gR69uyJ48ePIzg4GBUrVkTr1q0RGRlZsPdAEASh4OGWbC8zizsCIqKSz6iBV3GHQKQk9XJgsbUd+jBJZce2r6RfoPrOzs5o0KABAgOz3w+FQoGKFSvC29sb48eP/+D+WVlZMDIyQmBgIPr165fvdpkpJCIioi+eRIWvtLQ0JCYmKr3S0tJyjSM9PR0XL16Eu7u7WKampgZ3d3cEBwfn61xevHiBjIwMlCtXrkDvATuFRERERCoUEBAAQ0NDpVdAQECudePi4pCVlQUzMzOlcjMzM0RHR+ervXHjxqF8+fJKHcv84N3HRERERCq8+djX1xc+Pj5KZVKpVCVtzZo1C9u2bcOJEyegra1doH2ZKSwFtm3ZjK+/aokGDvXQu8d3uBoW9t76hw/9jY7t26KBQz106eSBUyf/VdouCAKWLvkVrZo1QcP6thgyaAAePLivwjPITp3P/MUfTV2d0cjJAT4jvSGPi1OqE/XkCbw8h8DZ0Q7N3VywYN5sZGZygmhJxGuSSpLG9a2xa9FQ3Ds8A6mXA+HR3PaD+7g5VkfQlnF4HrIQ1/70Qx8P5xx1hnZripv7/fHs7EKc3DAGTnUqqyJ8JZM92+He4RmID16A/Su8YF3JRGm7kUEZrJ3RHzGn5iLq5Bws9+sFXR0tlcf1JZCo8H9SqRQGBgZKr7w6hTKZDOrq6oiJiVEqj4mJgbm5+XvPYd68eZg1axYOHz4MW9sP/zt4FzuFJdzBvw9g3pwADB02HNt27kHNmrXgOXQQ5HJ5rvVDL1/C+J9Ho/O3XbF91160aNkKo7yH4/btcLHO2tWrsHXzRkzym4pNW3dAR0cHnkMG5Tm/IT8mTxiP5UuX5Ll97uyZ+PfEccxdsAhr1m9EbOxT+Ix8M8k9KysLXsOGIiMjA+s3bcP0mbOwb+8eLAtcXOiYSDV4TfKaLGl0daS4Gh6JUQHb81W/cnlj7FnyI05eCIdzj1kI3HIcy6f0gruLjVina+v6mD26M2as/BsuvWYjLDwS+5YNh4mRXqHjnDj0G/zm3yfP7aMHuGNYz2YYMXMbmvabh5TUdPxv6XBItd4M6q2d2R821hZo7xmILiNWoEn9alg6uVehY6KSR0tLC46Ojjh69KhYplAocPToUbi4uOS535w5c/DLL7/g4MGDcHJyKlTb7BSWcBvXr8W3XbuhU+cusK5WDZP8/KGtrY29u//Itf7mTRvg2sQNA74fjKrW1vAaMQo2tWtj25ZNALIzMps3bsAPQz3RoqU7atSshekBcxD79CmOHT0iHic6Kgo/+4xEk0ZOcHNpiJFenoiMfFyoc0hKSsKeP/7AmLHj4dzIBbXr1MW06TMRGnoZYVdCAQDBQadx7+4dzJw1F7VsbNDErRmGeY/E9q2bkZGeXqh2STV4TfKaLGkOn7kO/2V/Yd/x92esX/uhaxPcj5Rj/II9uBURgxXbT2LP0VB4924h1hnRpyXW7g7Cxn1ncfNeNLxnbEPqy3T07/Tmj7Khng6WTemFh8cCEHNqLv5e6Y16NSwLfR7De7XA7FWH8NeJq7h2+wkGT94ACxNDdGhhBwCoWcUMbRrXwbBpW3D+2gMEhd6Dz+yd+K5NfViYGBa6XcpWkpak8fHxwapVq7B+/XrcuHEDnp6eSElJwcCBAwEA/fr1g6+vr1h/9uzZmDx5MtasWQMrKytER0cjOjoaycnJBWqXncISLCM9HTeu/4dGLq5imZqaGho1ckXYlcu57hMWGopGjZS/Sbg2boKw0FAAQOTjx4iLi4VzozfH1NfXRz1bO/GYGRkZ8BwyCGV0dbF2w2as37QVZcqUwbChgwv1x/D6f9eQmZkB57fOo0pVa1hYlMeVV3FdCQ1F9eo1YCyTKcWdnJyMO3fvFLhNUg1ek7wmPwfOdlVwPOSWUtk/QTfgbFsFAKCpoQ4Hm4o49lYdQRBwLOQWGr6qAwCb5w6CSTl9dPJaDtfecxB68zEOrPCGkUGZAsdkZWkMCxNDHAu5KZYlJr/E+Wv34WxrlR23bRU8S3yBS9cfinWOhdyCQiGgQV3VD23Tp9O9e3fMmzcPU6ZMgb29PUJDQ3Hw4EHx5pOHDx8iKipKrL98+XKkp6eja9eusLCwEF/z5s0rULu80aQEe/b8GbKysmBsbKxUbmxsjIiIe7nuExcXB2NjWY76cfK4V9tjs8tkOY8Z92o+1aGDB6AQFJg6bQYkr77iTJsegCYuDXD+/Dm4Nm5SoPOQx8VBU1MTBgYGSuXljI3FeORxcSiXI27Zq22xBWqPVIfXJK/Jz4GZsQFi4pXXpHsanwhDfR1oSzVhZFAGGhrqePpuHXkialpl/1F2ta8KpzqVUamVL9IzsueZ+i7cA4/mtujs7oA1u88UKCZzmcGrON5tMwlmxgZi3LHvbM/KUiA+8QXMZMrXMhVcSXvInZeXF7y8cl9L9MSJE0o/379/v0jaZKeQcgi/dROPHj6ES4P6SuVpaWl4/Cj7G+r+v/bhl6l+4raMjHQAEqxfu0YsW7ZyFeo7Fm5eA9HbeE1SSVOvRgXolZEi8sRspXIdqSaqVsj+8tDYwRp7A4eJ27Q01SGBBJ3dHcQy7+lbse3vC58maKIPYKewBDMqawR1dfUcE/jlcjlkMlmu+8hkMsjlcTnrv8pwyGTZd7LJ4+QwMTFVqlOzVi0A2Yte2tSug4DZOdPORq8WwmzeoiXq1bMTyxctmAdTMzP06t1XLDN9leY2lsmQkZGBxMREpcxMvFwuxmMsk+HaVeX5QK/Pw1imfPcdFR9ek7wmPwcx8kSYlVN+woRpOQMkJKXiZVoG4p4lIzMzC6bv1jE2QLQ8EQCgV0YL0XEJaP3DrzmOn5CUCgC4eP0hnHu8WYtueM/mKG9aFhN/3SuWPZVnZ/6i4xJfxaEv/nd2m/oIu/VYjNvknZjU1dVQzqAMYt7ahwqppKUKiwHnFJZgmlpasKldByFn36xgrlAoEBISDFs7h1z3sbW3R8jZs0plZ4ODYGtvDwCwrFABMpkJQkLeHDM5ORlXw66Ix7SxqYOHDx6gnLExKlWurPTS18/+QNLV1VMq19XVhaGhoVLZ6/WRatepCw0NTZx76zzuR9xDVNQT2L2Ky87eHrdvhyt1Ns4GBUFPTw/W1tUK+Q5SUeM1yWvycxByJQLNG9ZUKmvVqBZCwiIAABmZWbh84xFaOL+pI5FI0KJhDZx7VefyjccwMzZAZqYC9x7FKb3kz1MAAC/TMpTK4xNeICnlpVJZ8ovsO+zvR8oRFZug1Ka+rjYa1LVCSNj97LjDImBkUAYONhXFOs0b1ICamgTnrz0o+jeKvjjsFJZwffsPxO5dO7Bv7x7cu3sX06dNRWpqKjp1/hYAMNF3LH5dOF+s37tPPwSdOYX169Yg4t5dLF+6BP9du4YevbKXQZBIJOjdtx9WrVyOE8eO4nb4LUzyHQsTU1O0bJW98vk37T1Q1sgII708ceniBTx+/Ajnz4Vg1szpiMnnaupv09fXR+cuXTBvziycCzmL6/9dw5RJE2Bn7wBbO3sAgItrE1S1roaJ48fi1s2bOHP6FAKXLEL3nr2hpcU1uEoSXpO8JksaXR0t2NawhO2rO3+tLI1hW8MSFc2NAADTvDvg91/eZIxX7TqNKhWMMWNkR9SwMsOQ79zQ5SsHLNl8XKyzeNMxDOzsit4ezqhZxQyLJ3RHGR0pNvyZ/QXnWMhNhIRFYMfCIWjVqBYqWZRDI7sqmDrcA/VrVyrUeSzdchzjBrdFu2b1UKdaeaz+pS+iYhOw7/gVAMCtiBgcOvMflk7uBac6leFiVxULx3fDzkOXEBWbUKg26Q1VrlNYWnD4uIRr+/U3eBYfj2WBixEXF4uatWywbOXv4h2R0VFRUJO86dvbO9RHwJx5CFy8CEsWLUClylZYtGQpqlevIdYZOOgHpKamYtrUKUhKSoRDfUcsW/m7uJCmjo4O1q7fhEUL5sFnpBdSUlJgamYGZ2cX6OoVbo2un8dNgJpEDaNHjUB6RjpcGzfBxElv5n+pq6tjybIVmDFtKvr17g4dHR14dOyMYV4jCtUeqQ6vSV6TJU392pVx+PeR4s9zxnQBAGzcdxZD/DbBXGaAiuZvngH74Ikcnb1XYM6YbzG8V3NExjyH57QtOBJ8Q6yz6/AlyIz0MMWzHcyM9RF2KxIdhy9VuhGkk/dy+Ht54Df/PpAZ6SEmLhGnL93BU3nhhnLnrzuCMjpSBE7qibL6OggKvYsOw5chLf3NgukDJ6zHwvHdcGClNxQKAXuPhmL0nJ2Fao/oXRJBEITiDqKoveQDB4iIPsioQe53NhIVl9TLgcXW9vUnKSo7du3yuio7dlFippCIiIi+eKVnkFd1OKeQiIiIiIq/U5iamorTp0/j+vXrOba9fPkSGzZseO/+aWlpSExMVHp9zPNSiYiI6AskUeGrlCjWTmF4eDhsbGzQtGlT1KtXD82aNVN6bEtCQoL4nL+8BAQEwNDQUOk1d3bAe/chIiIiImXF2ikcN24c6tati6dPn+LWrVvQ19dH48aN8fDhww/v/Iqvry8SEhKUXj+P8/3wjkRERESvcEmaYu4UBgUFISAgADKZDNWqVcP//vc/tGnTBm5ubrh3L/fnqL5LKpXCwMBA6fV6GYvPxaABfWFXpybs6tTEzRs3PrxDKTV5wnjxPI8dPVLc4dAH8LqkkubQqpFIvRyI1MuB4pqFn6M+Hs7iec59tfwOUVEo1k5hamoqNDTe3AAtkUiwfPlyeHh4oFmzZggPDy/G6EqWLl274eiJ06hWvbpYFvXkCbw8h8DZ0Q7N3VywYN5sZGa+fz2ehOfP4Tt2NFwb1keTRk7wmzwBL1IKfhv+4UN/o2P7tmjgUA9dOnng1Ml/P7jP+XMh6N61M5zs66J926/w557dStvH+k7E0ROnCxwLFR9el1TSrP7jDKzcffHf3TdTkeaP7Yozm8fiechCnN02Pl/HkWppYOH4bnh8fDZiz8zH1nmDczz27kPMZQZYN3MAwvZOQcrFxfnuwFU0N8LuxT9CHrQAD44GYOaoTlBXf/PnetfhS7By98XZK/lLnlD+SCSqe5UWxdoprFWrFi5cyPkg8MDAQHTs2BEdOnQohqhKJm1tbchMTMROdFZWFryGDUVGRgbWb9qG6TNnYd/ePVgWuPi9x/EdNwZ379zBit/XYvHSFbh04QKmTZ1SoFhCL1/C+J9Ho/O3XbF91160aNkKo7yH4/btvDvxjx8/gtewoWjQ0Bk7/vgTvfv2h7/fJJw5fUqso6+vD5kJnylbmvC6pJIm9WU6YuRJyMpSKJVv+PMsdh2+lO/jzBnTBe2a1kXvsavRevAiWJgYYtv8wQWKRUtTA3HPkjDr94MIC4/M1z5qahLsXuwJLU0NtBgwHz9M2Yg+HZwxxbOdWOdlWgZi5ElIz8gqUDxEH1KsncLOnTtj69atuW4LDAxEz5498RmurV0kgoNO497dO5g5ay5q2digiVszDPMeie1bNyMjPT3Xfe7dvYszp0/Bb9p02Nraob6jE8ZPmISDf+/H06cx+W5786YNcG3ihgHfD0ZVa2t4jRgFm9q1sW3Lpjz32bl9GywtK2DM2PGoam2Nnr37wL11G2zasK6gp04lGK9LKolGz9mFlTtOIuKx/MOVARjoaWNAJxeMW7Ab/54Px+UbjzDEbxNc7K3RsJ5Vvtt9GBWPMXP/wJa/ziEx+WW+9nF3sYFNVXN8P3E9wsIjcfjMdUxbth9DuzWFpoZ6vtumguPNx8XcKfT19cWBAwfy3L5s2TIoFIo8t3/JroSGonr1GuKjxQDAtXETJCcn487dO7nvc+Uy9A0MUKduPbHM2cUVampquBoWlu+2w0JD0aiRi1KZa+MmCAsNzXufK3nscyXvfaj04XVJnwMHm0rQ0tTAsbO3xLLw+zF4GBUPZ9sqKm3b2bYKrt15ovQ4vX+CbsBQXwe1rS1U2vYXj73C4l+nkApHHheHcsYypTLjVz/L42Lz3qdcOaUyDQ0NGBga5rlPbuLi4sS23rRtjDh53Pv3keWMNzk5GS9f5u8bNJV8vC7pc2BubIC09AwkJKcqlT+VJ8LM2EClbZsZG+CpPEmp7Gl89rOUzWSqbZuIj7kjIiKiL15pWjpGVZgpLKWMZTLEv5MBkb/62ViW+6R4Y5kM8fHxSmWZmZlITEjIc5/cyGQysa03bcsheydLk2OfuJzx6unpQVtbO99tU8nG65I+B9HyREi1NGGop6NUbmpsgBh5okrbjpEnwtRY+S5n03LZGcKYONW2TcROYSllZ2+P27fDIZe/mTh9NigIenp6sLaulvs+dg5ISkzE9f+uiWXnQs5CoVCgnq1tvtu2tbdHyNmzSmVng4Nga2+f9z529ggJeWefoCDY2uW9D5U+vC7pc3D5xkOkZ2SihXNNsax6ZVNUsiiHkLAIlbYdEhaButXKw8RITyxr1agWEpJSceNetErb/tJxSRp2CkstF9cmqGpdDRPHj8Wtmzdx5vQpBC5ZhO49e0NLSwsAcDUsDB3bt0VMTPYdnFWtrdG4iRv8/SbjalgYLl+6iIAZv6Dt1+1gamqW77Z79+mHoDOnsH7dGkTcu4vlS5fgv2vX0KNXH7HOrwvnY6LvWPHn77r3wOPHj7Bw3hxE3LuL7Vs34/Chv9Gn34CieUOoROB1SSVR1Yoy2NawhJnMADpSTdjWsIRtDUvxbt7yJoYI3T0JTnUqAwASk19i3d5gzB79LZo6VYeDTUX85t8HZ6/cw7mr9wvU9uu2dMtIITPSg20NS9Sqai5u79DCFqG7J4k/Hwm+gRv3orF6en/Uq2EJdxcb+A1vj5U7TiI94/3rfRJ9LM4pLKXU1dWxZNkKzJg2Ff16d4eOjg48OnbGMK8RYp2XL1NxPyICmZkZYlnA7HkImPELhgzqDzU1NbT6qjXG+05SOrZdnZqYNj0AHTt/m2vb9g71ETBnHgIXL8KSRQtQqbIVFi1ZiurVa4h14mJjEf3Wc6wrVKiIwGUrMXd2ADZv2gAzc3P4+U9H4yZuRfSOUEnA65JKouVTeqOp05sF1kO2Zz8KteY3U/AwKh4aGuqoWcUcOtpaYp2x8/6AQiFg67zBkGpp4EjQDYwM2K503Jv7/bFxXwhmrMx7FY3XbQGAY+1K6PFNAzx4Iketdn4AAAM9HdSs8qaTqFAI6DJyOX6d0AMn1o1Gyss0bP7fOUxbvv/j3gT6oFKU0FMZifAZLgT48jP7MjVoQF/UrFkLY30nqrytx48foWO7tti9bz8qV7ZSeXu5satTEwsXL0XLVu7F0j7lD6/L0s+ogVdxh1CkDq0aibBbj/HzvD9U3paOtiYij89GR6/lOHXxtsrby82nPN9PJfVyYLG1ffdp6ocrFZK1qc6HK5UAHD4uJbZv24pGTg64HX7rw5U/wumTJ9Gla7di+cP7i/8UNHJy+OTtUuHxuqSSZkg3N8SemY861cqrtJ1mTjVw4nx4sXQIe3zthNgz89HYwfqTt/1Z4zqFzBSWBjExMUh7tWaahYUFNLW0PrBH6SSXy5GSnAwAkJmYoEyZMsUcEb0Pr8vS73PLFJY3MYS2tiYA4FHUM2Rkfp6PgdMrIxXvUE5ISoX8ecGfE15SFWem8F6s6tYmrWpSOlYzYKeQiOgL9bl1Cqn0Y6ewePFGEyIiIvrilaalY1SFcwqJiIiIiJlCIiIiIiYKmSkkIiIiIjBTSERERMRUIZgpJCIiIiIwU0hEREQECVOF7BQSERERcUkaDh8TEREREZgpJCIiIuLgMZgpJCIiIiIwU0hERETEOYVgppCIiIiIwEwhERERETirkJlCIiIiIgIzhUREREScUwh2ComIiIg4eAwOHxMRERERmCkkIiIi4vAxmCkkIiIiIjBTSERERAQJZxUyU0hEREREzBQSERER8fZjMFNIRERERGCmkIiIiIiJQrBTSERERMQlacDhYyIiIiICM4VEREREXJIGzBQSEREREZgpJCIiIuKdJmCmkIiIiIjATCERERERE4VgppCIiIiIwEwhEREREdcpBDuFRERERFySBhw+JiIiIiIwU0hERETE4WMwU0hEREREYKeQiIiIiMBOIRERERGBcwqJiIiIOKcQzBQSEREREZgpJCIiIuI6hWCnkIiIiIjDx+DwMRERERGBmUIiIiIiDh6DmUIiIiIiAjOFREREREwVgplCIiIiIgIzhURERERckgbMFBIRERERmCkkIiIi4jqFYKaQiIiIiMBMIRERERFnFIKdQiIiIiL2CsHhYyIiIiICO4VEREREkKjwf4WxdOlSWFlZQVtbG87Ozjh37tx76+/cuRO1atWCtrY26tWrhwMHDhS4TXYKiYiIiEqQ7du3w8fHB35+frh06RLs7OzQpk0bPH36NNf6QUFB6NmzJwYNGoTLly+jU6dO6NSpE65du1agdiWCIAhFcQIlycvM4o6AiKjkM2rgVdwhEClJvRxYbG2rsu+gXcA7OJydndGgQQMEBma/HwqFAhUrVoS3tzfGjx+fo3737t2RkpKCv/76Syxr1KgR7O3tsWLFiny3y0whERERkQqlpaUhMTFR6ZWWlpZr3fT0dFy8eBHu7u5imZqaGtzd3REcHJzrPsHBwUr1AaBNmzZ51s/LZ3n3cUF75JS7tLQ0BAQEwNfXF1KptLjDIeI1WcSKMyvzOeF1+XlQZd9h6vQA+Pv7K5X5+flh6tSpOerGxcUhKysLZmZmSuVmZma4efNmrsePjo7OtX50dHSB4mSmkPKUlpYGf3//PL/NEH1qvCapJOJ1SR/i6+uLhIQEpZevr29xh5UDc2pEREREKiSVSvOdRZbJZFBXV0dMTIxSeUxMDMzNzXPdx9zcvED188JMIREREVEJoaWlBUdHRxw9elQsUygUOHr0KFxcXHLdx8XFRak+APzzzz951s8LM4VEREREJYiPjw/69+8PJycnNGzYEIsWLUJKSgoGDhwIAOjXrx8sLS0REBAAABg5ciSaNWuG+fPno127dti2bRsuXLiA3377rUDtslNIeZJKpfDz8+PEaSoxeE1SScTrkopa9+7dERsbiylTpiA6Ohr29vY4ePCgeDPJw4cPoab2ZrDX1dUVW7ZswaRJkzBhwgRUr14de/fuRd26dQvU7me5TiERERERFQznFBIRERERO4VERERExE4hEREREYGdQiIiIiICO4WUh6VLl8LKygra2tpwdnbGuXPnijsk+oKdPHkSHh4eKF++PCQSCfbu3VvcIdEXLiAgAA0aNIC+vj5MTU3RqVMn3Lp1q7jDIvoo7BRSDtu3b4ePjw/8/Pxw6dIl2NnZoU2bNnj69Glxh0ZfqJSUFNjZ2WHp0qXFHQoRAODff//F8OHDcfbsWfzzzz/IyMhA69atkZKSUtyhERUal6ShHJydndGgQQMEBgYCyF5JvWLFivD29sb48eOLOTr60kkkEuzZswedOnUq7lCIRLGxsTA1NcW///6Lpk2bFnc4RIXCTCEpSU9Px8WLF+Hu7i6Wqampwd3dHcHBwcUYGRFRyZWQkAAAKFeuXDFHQlR47BSSkri4OGRlZYmrpr9mZmaG6OjoYoqKiKjkUigUGDVqFBo3blzgJ0gQlSR8zB0REdFHGD58OK5du4bTp08XdyhEH4WdQlIik8mgrq6OmJgYpfKYmBiYm5sXU1RERCWTl5cX/vrrL5w8eRIVKlQo7nCIPgqHj0mJlpYWHB0dcfToUbFMoVDg6NGjcHFxKcbIiIhKDkEQ4OXlhT179uDYsWOoUqVKcYdE9NGYKaQcfHx80L9/fzg5OaFhw4ZYtGgRUlJSMHDgwOIOjb5QycnJuHPnjvhzREQEQkNDUa5cOVSqVKkYI6Mv1fDhw7Flyxb8+eef0NfXF+dcGxoaQkdHp5ijIyocLklDuQoMDMTcuXMRHR0Ne3t7LF68GM7OzsUdFn2hTpw4gRYtWuQo79+/P9atW/fpA6IvnkQiybV87dq1GDBgwKcNhqiIsFNIRERERJxTSERERETsFBIRERER2CkkIiIiIrBTSERERERgp5CIiIiIwE4hEREREYGdQiIiIiICO4VEREREBHYKiYiIiAjsFBLRJxQcHAx1dXW0a9euWNq/f/8+JBIJQkNDi6V9IqKSjJ1CIvpkVq9eDW9vb5w8eRJPnjwp7nCIiOgt7BQS0SeRnJyM7du3w9PTE+3atcO6deuUtu/btw/Vq1eHtrY2WrRogfXr10MikeD58+dindOnT8PNzQ06OjqoWLEiRowYgZSUFHG7lZUVZs6cie+//x76+vqoVKkSfvvtN3F7lSpVAAAODg6QSCRo3ry5Kk+ZiKhUYaeQiD6JHTt2oFatWqhZsyb69OmDNWvWQBAEAEBERAS6du2KTp064cqVKxg6dCgmTpyotP/du3fRtm1bdOnSBWFhYdi+fTtOnz4NLy8vpXrz58+Hk5MTLl++jGHDhsHT0xO3bt0CAJw7dw4AcOTIEURFRWH37t2f4MyJiEoHifD6U5mISIUaN26Mbt26YeTIkcjMzISFhQV27tyJ5s2bY/z48di/fz+uXr0q1p80aRJmzJiBZ8+eoWzZshg8eDDU1dWxcuVKsc7p06fRrFkzpKSkQFtbG1ZWVnBzc8PGjRsBAIIgwNzcHP7+/vjxxx9x//59VKlSBZcvX4a9vf2nfguIiEo0ZgqJSOVu3bqFc+fOoWfPngAADQ0NdO/eHatXrxa3N2jQQGmfhg0bKv185coVrFu3Dnp6euKrTZs2UCgUiIiIEOvZ2tqK/y2RSGBubo6nT5+q6tSIiD4bGsUdABF9/lavXo3MzEyUL19eLBMEAVKpFIGBgfk6RnJyMoYOHYoRI0bk2FapUiXxvzU1NZW2SSQSKBSKQkZORPTlYKeQiFQqMzMTGzZswPz589G6dWulbZ06dcLWrVtRs2ZNHDhwQGnb+fPnlX6uX78+rl+/jmrVqhU6Fi0tLQBAVlZWoY9BRPS5YqeQiFTqr7/+wrNnzzBo0CAYGhoqbevSpQtWr16NHTt2YMGCBRg3bhwGDRqE0NBQ8e5kiUQCABg3bhwaNWoELy8vDB48GLq6urh+/Tr++eeffGcbTU1NoaOjg4MHD6JChQrQ1tbOERMR0ZeKcwqJSKVWr14Nd3f3XDtfXbp0wYULF5CUlIRdu3Zh9+7dsLW1xfLly8W7j6VSKYDsuYL//vsvwsPD4ebmBgcHB0yZMkVpSPpDNDQ0sHjxYqxcuRLly5dHx44di+YkiYg+A7z7mIhKpBkzZmDFihV49OhRcYdCRPRF4PAxEZUIy5YtQ4MGDWBsbIwzZ85g7ty5OdYgJCIi1WGnkIhKhNu3b2P69OmIj49HpUqVMHr0aPj6+hZ3WEREXwwOHxMRERERbzQhIiIiInYKiYiIiAjsFBIRERER2CkkIiIiIrBTSERERERgp5CIiIiIwE4hEREREYGdQiIiIiIC8H8QsY1FfJYDGAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching(p, q, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primal_loss import compute_t\n",
    "from efficiency_loss import compute_efficiency_loss\n",
    "from utils import da_with_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[[0.6, 0.3, 1], [1, 0.5, 0], [0.3, 0, 1]]]).to(device)\n",
    "q = torch.tensor([[[0.5, 1.0, 0], [0, 1, 0.5], [0, 0.5, 1.0]]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([p]).to(device)\n",
    "q = torch.tensor([q]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(p, q)\n",
    "model_efficiency_loss = compute_efficiency_loss(cfg, model_output, p, q)\n",
    "model_stability_loss = compute_t(model_output, p, q).mean()\n",
    "model_sp_loss = compute_spv_w(cfg, model, model_output, p, q).mean()\n",
    "\n",
    "da_output = da_with_t(p, q)\n",
    "da_efficiency_loss = compute_efficiency_loss(cfg, da_output, p, q)\n",
    "da_stability_loss = compute_t(da_output, p, q).mean()\n",
    "da_sp_loss = compute_spv_w(cfg, da_with_t, da_output, p, q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:tensor([[[0.5000, 0.1667, 0.3333],\n",
      "         [0.3333, 0.1667, 0.5000],\n",
      "         [0.3333, 0.1667, 0.5000]]], device='mps:0')\n",
      "q:tensor([[[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.5000, 0.1667],\n",
      "         [0.5000, 0.2500, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[0.9453, 0.0242, 0.0305],\n",
      "         [0.0015, 0.1340, 0.8645],\n",
      "         [0.0361, 0.9422, 0.0216]]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 0.2690471410751343\n",
      "  Stability Loss: 0.3243393003940582\n",
      "  SP Loss: 0.6883907318115234\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.7500, 0.0000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.0000, 0.7500, 0.2500]]], device='mps:0')\n",
      "  Efficiency Loss: 0.5\n",
      "  Stability Loss: 0.3263888955116272\n",
      "  SP Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"p:{p}\")\n",
    "print(f\"q:{q}\")\n",
    "print(\"\\nModel Results:\")\n",
    "print(f\"  Output: {model_output}\")\n",
    "print(f\"  Efficiency Loss: {model_efficiency_loss}\")\n",
    "print(f\"  Stability Loss: {model_stability_loss}\")\n",
    "print(f\"  SP Loss: {model_sp_loss}\")\n",
    "\n",
    "print(\"\\nDA Results:\")\n",
    "print(f\"  Output: {da_output}\")\n",
    "print(f\"  Efficiency Loss: {da_efficiency_loss}\")\n",
    "print(f\"  Stability Loss: {da_stability_loss}\")\n",
    "print(f\"  SP Loss: {da_sp_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patterns_2_1\n",
    "受け手の選好は必ず二つの同等に好ましい提案者と、好ましくない一人の提案者となる\n",
    "\n",
    "提案側の選考はランダムな強選好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_tuples, apply_features, convert_to_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tie_2_1 = list(set(itertools.permutations([2, 2, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(314)\n",
    "\n",
    "one_tie_2_1 = normalize_tuples(one_tie_2_1)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(one_tie_2_1, repeat=3))\n",
    "pairs = [(random.choice(preference_list_p), random.choice(preference_list_q)) for _ in range(1000)]\n",
    "df_1 = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [01:31<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_1 = apply_features(cfg, model, df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(try_convert)\n"
     ]
    }
   ],
   "source": [
    "df_1 = convert_to_float(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.258292</td>\n",
       "      <td>1.520026e-01</td>\n",
       "      <td>0.248798</td>\n",
       "      <td>-0.176250</td>\n",
       "      <td>0.170672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.738410</td>\n",
       "      <td>8.622905e-02</td>\n",
       "      <td>0.208262</td>\n",
       "      <td>0.321484</td>\n",
       "      <td>0.101627</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.250152</td>\n",
       "      <td>7.094528e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.912164</td>\n",
       "      <td>8.914764e-02</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.001587</td>\n",
       "      <td>1.498319e-01</td>\n",
       "      <td>0.221915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.005111</td>\n",
       "      <td>2.075212e-01</td>\n",
       "      <td>0.397366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.924163</td>\n",
       "      <td>4.031537e-01</td>\n",
       "      <td>0.882955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000          1.000000e+03    1000.000000   \n",
       "mean               -0.258292          1.520026e-01       0.248798   \n",
       "std                 0.738410          8.622905e-02       0.208262   \n",
       "min                -2.250152          7.094528e-07       0.000001   \n",
       "25%                -0.912164          8.914764e-02       0.057300   \n",
       "50%                -0.001587          1.498319e-01       0.221915   \n",
       "75%                 0.005111          2.075212e-01       0.397366   \n",
       "max                 2.924163          4.031537e-01       0.882955   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss  da_sp_loss  \n",
       "count         1000.000000        1000.000000      1000.0  \n",
       "mean            -0.176250           0.170672         0.0  \n",
       "std              0.321484           0.101627         0.0  \n",
       "min             -1.250000           0.000000         0.0  \n",
       "25%             -0.500000           0.088889         0.0  \n",
       "50%              0.000000           0.177778         0.0  \n",
       "75%              0.000000           0.244444         0.0  \n",
       "max              0.000000           0.450000         0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [p, q, model_efficiency_loss, model_stability_loss, model_sp_loss, da_efficiency_loss, da_stability_loss, da_sp_loss]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[df_1[\"da_efficiency_loss\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "      <td>0.086036</td>\n",
       "      <td>0.209564</td>\n",
       "      <td>0.355142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.133355</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.4, 0.4, ...</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>0.310892</td>\n",
       "      <td>0.486120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...</td>\n",
       "      <td>-0.997764</td>\n",
       "      <td>0.222454</td>\n",
       "      <td>0.150245</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...</td>\n",
       "      <td>-0.999738</td>\n",
       "      <td>0.111141</td>\n",
       "      <td>0.156208</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   p  \\\n",
       "0  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "1  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "2  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "3  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "4  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "\n",
       "                                                   q  model_efficiency_loss  \\\n",
       "0  ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...               0.086036   \n",
       "1  ((0.2, 0.4, 0.4), (0.4, 0.2, 0.4), (0.4, 0.4, ...               0.000292   \n",
       "2  ((0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.4, 0.4, ...              -0.002414   \n",
       "3  ((0.4, 0.2, 0.4), (0.2, 0.4, 0.4), (0.2, 0.4, ...              -0.997764   \n",
       "4  ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.2, 0.4, ...              -0.999738   \n",
       "\n",
       "   model_stability_loss  model_sp_loss  da_efficiency_loss  da_stability_loss  \\\n",
       "0              0.209564       0.355142                 0.0           0.200000   \n",
       "1              0.133355       0.002530                 0.0           0.133333   \n",
       "2              0.310892       0.486120                 0.0           0.361111   \n",
       "3              0.222454       0.150245                -0.5           0.266667   \n",
       "4              0.111141       0.156208                -0.5           0.177778   \n",
       "\n",
       "   da_sp_loss  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df_1['p'][0]\n",
    "q = df_1['q'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patterns2-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tie_2_2 = list(set(itertools.permutations([2, 1, 1])))\n",
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "one_tie_2_2 = normalize_tuples(one_tie_2_2)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(one_tie_2_2, repeat=3))\n",
    "pairs = [(choice(preference_list_p), choice(preference_list_q)) for _ in range(1000)]\n",
    "df_2 = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [01:24<00:00, 11.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df_2 = apply_features(cfg, model, df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(try_convert)\n"
     ]
    }
   ],
   "source": [
    "df_2 = convert_to_float(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.020783</td>\n",
       "      <td>1.550077e-01</td>\n",
       "      <td>2.350064e-01</td>\n",
       "      <td>-0.026750</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.710292</td>\n",
       "      <td>8.536403e-02</td>\n",
       "      <td>2.014992e-01</td>\n",
       "      <td>0.275768</td>\n",
       "      <td>0.100310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.004567</td>\n",
       "      <td>2.329875e-07</td>\n",
       "      <td>5.652393e-07</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.032678</td>\n",
       "      <td>8.429097e-02</td>\n",
       "      <td>3.654488e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000106</td>\n",
       "      <td>1.659087e-01</td>\n",
       "      <td>2.188067e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.023245</td>\n",
       "      <td>2.151292e-01</td>\n",
       "      <td>3.700833e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.540068</td>\n",
       "      <td>4.078784e-01</td>\n",
       "      <td>1.031788e+00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000          1.000000e+03   1.000000e+03   \n",
       "mean                0.020783          1.550077e-01   2.350064e-01   \n",
       "std                 0.710292          8.536403e-02   2.014992e-01   \n",
       "min                -2.004567          2.329875e-07   5.652393e-07   \n",
       "25%                -0.032678          8.429097e-02   3.654488e-02   \n",
       "50%                 0.000106          1.659087e-01   2.188067e-01   \n",
       "75%                 0.023245          2.151292e-01   3.700833e-01   \n",
       "max                 3.540068          4.078784e-01   1.031788e+00   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss  da_sp_loss  \n",
       "count         1000.000000        1000.000000      1000.0  \n",
       "mean            -0.026750           0.169000         0.0  \n",
       "std              0.275768           0.100310         0.0  \n",
       "min             -0.500000           0.000000         0.0  \n",
       "25%              0.000000           0.083333         0.0  \n",
       "50%              0.000000           0.166667         0.0  \n",
       "75%              0.000000           0.243056         0.0  \n",
       "max              1.500000           0.458333         0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.138916</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.5, 0...</td>\n",
       "      <td>-0.033278</td>\n",
       "      <td>0.218525</td>\n",
       "      <td>0.433840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>0.023896</td>\n",
       "      <td>0.056468</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   p  \\\n",
       "0  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "1  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "2  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "3  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "4  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "\n",
       "                                                   q  model_efficiency_loss  \\\n",
       "0  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               0.000417   \n",
       "1  ((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.5, 0...              -0.033278   \n",
       "2  ((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...               0.002076   \n",
       "3  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               0.023896   \n",
       "4  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.000452   \n",
       "\n",
       "   model_stability_loss  model_sp_loss  da_efficiency_loss  da_stability_loss  \\\n",
       "0              0.138916       0.008987                 0.0           0.138889   \n",
       "1              0.218525       0.433840                 0.0           0.222222   \n",
       "2              0.000171       0.000574                 0.0           0.000000   \n",
       "3              0.056468       0.003617                 0.0           0.055556   \n",
       "4              0.027815       0.000061                 0.0           0.027778   \n",
       "\n",
       "   da_sp_loss  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.5, 0...</td>\n",
       "      <td>-0.050925</td>\n",
       "      <td>0.161119</td>\n",
       "      <td>0.209633</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.808807</td>\n",
       "      <td>0.242641</td>\n",
       "      <td>0.310507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.25, 0.5, 0.25), (0.25, ...</td>\n",
       "      <td>0.593361</td>\n",
       "      <td>0.234134</td>\n",
       "      <td>0.333771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.994446</td>\n",
       "      <td>0.333503</td>\n",
       "      <td>0.674636</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.997842</td>\n",
       "      <td>0.333311</td>\n",
       "      <td>0.478036</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>1.992907</td>\n",
       "      <td>0.333625</td>\n",
       "      <td>0.379503</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.5, 0...</td>\n",
       "      <td>0.974320</td>\n",
       "      <td>0.248394</td>\n",
       "      <td>0.655089</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>2.267154</td>\n",
       "      <td>0.278830</td>\n",
       "      <td>0.707462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.5, 0.25), (0.25, ...</td>\n",
       "      <td>1.908226</td>\n",
       "      <td>0.339930</td>\n",
       "      <td>0.465255</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>1.974267</td>\n",
       "      <td>0.331905</td>\n",
       "      <td>0.302838</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.986963</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>0.534925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>1.988299</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.462060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.990233</td>\n",
       "      <td>0.193097</td>\n",
       "      <td>0.206517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>1.983302</td>\n",
       "      <td>0.249537</td>\n",
       "      <td>0.442687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.990192</td>\n",
       "      <td>0.332631</td>\n",
       "      <td>0.456754</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.25, ...</td>\n",
       "      <td>1.981450</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.710837</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.979422</td>\n",
       "      <td>0.303430</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.979422</td>\n",
       "      <td>0.303430</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.5, 0...</td>\n",
       "      <td>1.715757</td>\n",
       "      <td>0.348977</td>\n",
       "      <td>0.518826</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>2.020269</td>\n",
       "      <td>0.366622</td>\n",
       "      <td>0.605002</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.5, 0...</td>\n",
       "      <td>0.997255</td>\n",
       "      <td>0.308214</td>\n",
       "      <td>0.625794</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.955211</td>\n",
       "      <td>0.274711</td>\n",
       "      <td>0.645359</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.083508</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>((0.5, 0.3333333333333333, 0.16666666666666666...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.138933</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.25, 0.5, 0.25), (0.5, 0...</td>\n",
       "      <td>1.989897</td>\n",
       "      <td>0.249941</td>\n",
       "      <td>0.404813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.25, 0.25, 0.5), (0.25, 0.5, 0.25), (0.25, ...</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.113831</td>\n",
       "      <td>0.263105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.5, 0...</td>\n",
       "      <td>0.990418</td>\n",
       "      <td>0.193060</td>\n",
       "      <td>0.600171</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.5, 0.25, 0.25), (0.25, 0.5, 0.25), (0.25, ...</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.173267</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     p  \\\n",
       "62   ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "140  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "175  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "183  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "204  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "208  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "211  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "226  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "261  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "266  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "339  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "354  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "368  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "431  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "440  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "498  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "534  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "570  ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "606  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "650  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "674  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "699  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "820  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "876  ((0.5, 0.3333333333333333, 0.16666666666666666...   \n",
       "908  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "936  ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "952  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "953  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "\n",
       "                                                     q  model_efficiency_loss  \\\n",
       "62   ((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.5, 0...              -0.050925   \n",
       "140  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.808807   \n",
       "175  ((0.25, 0.5, 0.25), (0.25, 0.5, 0.25), (0.25, ...               0.593361   \n",
       "183  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.994446   \n",
       "204  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.997842   \n",
       "208  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               1.992907   \n",
       "211  ((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.5, 0...               0.974320   \n",
       "226  ((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...               2.267154   \n",
       "261  ((0.5, 0.25, 0.25), (0.25, 0.5, 0.25), (0.25, ...               1.908226   \n",
       "266  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               1.974267   \n",
       "339  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.986963   \n",
       "354  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               1.988299   \n",
       "368  ((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.990233   \n",
       "431  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               1.983302   \n",
       "440  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.990192   \n",
       "498  ((0.25, 0.5, 0.25), (0.25, 0.25, 0.5), (0.25, ...               1.981450   \n",
       "534  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.979422   \n",
       "570  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.979422   \n",
       "606  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.5, 0...               1.715757   \n",
       "650  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               2.020269   \n",
       "674  ((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.5, 0...               0.997255   \n",
       "699  ((0.25, 0.5, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.955211   \n",
       "820  ((0.5, 0.25, 0.25), (0.5, 0.25, 0.25), (0.25, ...               0.001480   \n",
       "876  ((0.25, 0.25, 0.5), (0.5, 0.25, 0.25), (0.25, ...               0.001231   \n",
       "908  ((0.25, 0.25, 0.5), (0.25, 0.5, 0.25), (0.5, 0...               1.989897   \n",
       "936  ((0.25, 0.25, 0.5), (0.25, 0.5, 0.25), (0.25, ...               0.020629   \n",
       "952  ((0.5, 0.25, 0.25), (0.25, 0.25, 0.5), (0.5, 0...               0.990418   \n",
       "953  ((0.5, 0.25, 0.25), (0.25, 0.5, 0.25), (0.25, ...               0.135682   \n",
       "\n",
       "     model_stability_loss  model_sp_loss  da_efficiency_loss  \\\n",
       "62               0.161119       0.209633                 1.5   \n",
       "140              0.242641       0.310507                 1.0   \n",
       "175              0.234134       0.333771                 1.0   \n",
       "183              0.333503       0.674636                 1.5   \n",
       "204              0.333311       0.478036                 1.5   \n",
       "208              0.333625       0.379503                 1.5   \n",
       "211              0.248394       0.655089                 1.5   \n",
       "226              0.278830       0.707462                 1.0   \n",
       "261              0.339930       0.465255                 1.5   \n",
       "266              0.331905       0.302838                 1.5   \n",
       "339              0.249472       0.534925                 1.0   \n",
       "354              0.250317       0.462060                 1.0   \n",
       "368              0.193097       0.206517                 1.0   \n",
       "431              0.249537       0.442687                 1.0   \n",
       "440              0.332631       0.456754                 1.5   \n",
       "498              0.359147       0.710837                 1.5   \n",
       "534              0.303430       0.523438                 1.5   \n",
       "570              0.303430       0.523438                 1.5   \n",
       "606              0.348977       0.518826                 1.5   \n",
       "650              0.366622       0.605002                 1.5   \n",
       "674              0.308214       0.625794                 1.5   \n",
       "699              0.274711       0.645359                 1.5   \n",
       "820              0.083508       0.000494                 1.0   \n",
       "876              0.138933       0.000511                 1.0   \n",
       "908              0.249941       0.404813                 1.0   \n",
       "936              0.113831       0.263105                 1.0   \n",
       "952              0.193060       0.600171                 1.0   \n",
       "953              0.173267       0.133789                 1.0   \n",
       "\n",
       "     da_stability_loss  da_sp_loss  \n",
       "62            0.458333         0.0  \n",
       "140           0.208333         0.0  \n",
       "175           0.222222         0.0  \n",
       "183           0.319444         0.0  \n",
       "204           0.319444         0.0  \n",
       "208           0.319444         0.0  \n",
       "211           0.458333         0.0  \n",
       "226           0.277778         0.0  \n",
       "261           0.319444         0.0  \n",
       "266           0.319444         0.0  \n",
       "339           0.208333         0.0  \n",
       "354           0.208333         0.0  \n",
       "368           0.305556         0.0  \n",
       "431           0.208333         0.0  \n",
       "440           0.319444         0.0  \n",
       "498           0.347222         0.0  \n",
       "534           0.409722         0.0  \n",
       "570           0.409722         0.0  \n",
       "606           0.347222         0.0  \n",
       "650           0.347222         0.0  \n",
       "674           0.409722         0.0  \n",
       "699           0.347222         0.0  \n",
       "820           0.194444         0.0  \n",
       "876           0.277778         0.0  \n",
       "908           0.208333         0.0  \n",
       "936           0.222222         0.0  \n",
       "952           0.305556         0.0  \n",
       "953           0.208333         0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[df_2[\"da_efficiency_loss\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df_2['p'][62]\n",
    "q = df_2['q'][62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indifferent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tie_pattern = [(1, 1, 1)]\n",
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))\n",
    "\n",
    "full_tie_pattern = normalize_tuples(full_tie_pattern)\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(full_tie_pattern, repeat=3))\n",
    "pairs = [(random.choice(preference_list_p), random.choice(preference_list_q)) for _ in range(1000)]\n",
    "df = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [02:16<00:00,  7.33it/s]\n"
     ]
    }
   ],
   "source": [
    "df = apply_features(cfg, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:162: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(try_convert)\n"
     ]
    }
   ],
   "source": [
    "df = convert_to_float(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.522233</td>\n",
       "      <td>0.131974</td>\n",
       "      <td>0.266827</td>\n",
       "      <td>-0.407750</td>\n",
       "      <td>0.144694</td>\n",
       "      <td>0.019556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.669161</td>\n",
       "      <td>0.091692</td>\n",
       "      <td>0.215364</td>\n",
       "      <td>0.556091</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>0.034982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.003780</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.999246</td>\n",
       "      <td>0.107023</td>\n",
       "      <td>0.111496</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.082704</td>\n",
       "      <td>0.111715</td>\n",
       "      <td>0.222880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.218959</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.139783</td>\n",
       "      <td>0.348865</td>\n",
       "      <td>0.875721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_efficiency_loss  model_stability_loss  model_sp_loss  \\\n",
       "count            1000.000000           1000.000000    1000.000000   \n",
       "mean               -0.522233              0.131974       0.266827   \n",
       "std                 0.669161              0.091692       0.215364   \n",
       "min                -2.003780              0.000008       0.000013   \n",
       "25%                -0.999246              0.107023       0.111496   \n",
       "50%                -0.082704              0.111715       0.222880   \n",
       "75%                 0.000373              0.218959       0.414400   \n",
       "max                 0.139783              0.348865       0.875721   \n",
       "\n",
       "       da_efficiency_loss  da_stability_loss   da_sp_loss  \n",
       "count         1000.000000        1000.000000  1000.000000  \n",
       "mean            -0.407750           0.144694     0.019556  \n",
       "std              0.556091           0.097664     0.034982  \n",
       "min             -2.000000           0.000000     0.000000  \n",
       "25%             -1.000000           0.111111     0.000000  \n",
       "50%              0.000000           0.111111     0.000000  \n",
       "75%              0.000000           0.222222     0.055556  \n",
       "max              0.000000           0.333333     0.111111  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df['p'][7]\n",
    "q = df['q'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>model_efficiency_loss</th>\n",
       "      <th>model_stability_loss</th>\n",
       "      <th>model_sp_loss</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_loss</th>\n",
       "      <th>da_sp_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [p, q, model_efficiency_loss, model_stability_loss, model_sp_loss, da_efficiency_loss, da_stability_loss, da_sp_loss]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"da_efficiency_loss\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_all_patterns\n",
    "no_tie_patterns = list(itertools.permutations([3, 2, 1]))\n",
    "\n",
    "full_pattern = normalize_tuples(generate_all_patterns())\n",
    "no_tie_patterns = normalize_tuples(no_tie_patterns)\n",
    "preference_list_p = list(itertools.product(no_tie_patterns, repeat=3))\n",
    "preference_list_q = list(itertools.product(full_pattern, repeat=3))\n",
    "pairs = [(random.choice(preference_list_p), random.choice(preference_list_q)) for _ in range(300)]\n",
    "df = pd.DataFrame(pairs, columns=['p', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_(cfg, model,df):\n",
    "    da_efficiency_losses = []\n",
    "    da_stability_losses = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "        p = torch.tensor([row['p']], dtype=torch.float32).to(device)\n",
    "        q = torch.tensor([row['q']], dtype=torch.float32).to(device)\n",
    "\n",
    "        da_output = da_with_t(p, q)\n",
    "        da_efficiency_loss = compute_efficiency_loss(cfg, da_output, p, q).cpu().detach().numpy()\n",
    "        da_stability_loss = compute_t(da_output, p, q).mean().cpu().detach().numpy()\n",
    "\n",
    "        da_efficiency_losses.append(da_efficiency_loss)\n",
    "        da_stability_losses.append(da_stability_loss)\n",
    "        \n",
    "    df['da_efficiency_loss'] = da_efficiency_losses\n",
    "    df['da_stability_losses'] = da_stability_losses\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 300/300 [00:09<00:00, 31.58it/s]\n",
      "/Users/meina/Github/meina-t/matching_with_dl/utils.py:161: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(try_convert)\n"
     ]
    }
   ],
   "source": [
    "df = apply_(cfg, model, df)\n",
    "df = convert_to_float(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.080833</td>\n",
       "      <td>0.168860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.271884</td>\n",
       "      <td>0.102854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       da_efficiency_loss  da_stability_losses\n",
       "count          300.000000           300.000000\n",
       "mean            -0.080833             0.168860\n",
       "std              0.271884             0.102854\n",
       "min             -1.250000             0.000000\n",
       "25%              0.000000             0.096296\n",
       "50%              0.000000             0.166667\n",
       "75%              0.000000             0.232870\n",
       "max              1.500000             0.472222"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>da_efficiency_loss</th>\n",
       "      <th>da_stability_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.16666666666666666, 0.3333333333333333, 0.5...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.25, 0.5,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.3333333333333333, 0.3333333333333333, 0.33...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.326389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((0.5, 0.16666666666666666, 0.3333333333333333...</td>\n",
       "      <td>((0.4, 0.4, 0.2), (0.3333333333333333, 0.5, 0....</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.238889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>((0.16666666666666666, 0.5, 0.3333333333333333...</td>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.25, 0.5, 0.25), (0.3333333333333333, 0.5, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>((0.3333333333333333, 0.16666666666666666, 0.5...</td>\n",
       "      <td>((0.2, 0.4, 0.4), (0.25, 0.25, 0.5), (0.25, 0....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>((0.3333333333333333, 0.5, 0.16666666666666666...</td>\n",
       "      <td>((0.4, 0.2, 0.4), (0.16666666666666666, 0.5, 0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     p  \\\n",
       "0    ((0.16666666666666666, 0.3333333333333333, 0.5...   \n",
       "1    ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "2    ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "4    ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "5    ((0.5, 0.16666666666666666, 0.3333333333333333...   \n",
       "..                                                 ...   \n",
       "295  ((0.16666666666666666, 0.5, 0.3333333333333333...   \n",
       "296  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "297  ((0.3333333333333333, 0.16666666666666666, 0.5...   \n",
       "298  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "299  ((0.3333333333333333, 0.5, 0.16666666666666666...   \n",
       "\n",
       "                                                     q  da_efficiency_loss  \\\n",
       "0    ((0.4, 0.2, 0.4), (0.4, 0.2, 0.4), (0.25, 0.5,...                 0.0   \n",
       "1    ((0.3333333333333333, 0.3333333333333333, 0.33...                 0.5   \n",
       "2    ((0.3333333333333333, 0.16666666666666666, 0.5...                 0.0   \n",
       "4    ((0.3333333333333333, 0.5, 0.16666666666666666...                 0.0   \n",
       "5    ((0.4, 0.4, 0.2), (0.3333333333333333, 0.5, 0....                -1.0   \n",
       "..                                                 ...                 ...   \n",
       "295  ((0.3333333333333333, 0.5, 0.16666666666666666...                 0.0   \n",
       "296  ((0.25, 0.5, 0.25), (0.3333333333333333, 0.5, ...                 0.0   \n",
       "297  ((0.2, 0.4, 0.4), (0.25, 0.25, 0.5), (0.25, 0....                 0.0   \n",
       "298  ((0.3333333333333333, 0.5, 0.16666666666666666...                 0.0   \n",
       "299  ((0.4, 0.2, 0.4), (0.16666666666666666, 0.5, 0...                 0.0   \n",
       "\n",
       "     da_stability_losses  \n",
       "0               0.177778  \n",
       "1               0.326389  \n",
       "2               0.009259  \n",
       "4               0.092593  \n",
       "5               0.238889  \n",
       "..                   ...  \n",
       "295             0.325926  \n",
       "296             0.101852  \n",
       "297             0.216667  \n",
       "298             0.118519  \n",
       "299             0.077778  \n",
       "\n",
       "[285 rows x 4 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"da_stability_losses\"]> 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df['p'][1]\n",
    "q = df['q'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:tensor([[[0.3333, 0.1667, 0.5000],\n",
      "         [0.5000, 0.1667, 0.3333],\n",
      "         [0.5000, 0.1667, 0.3333]]], device='mps:0')\n",
      "q:tensor([[[0.5000, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[9.9922e-01, 1.9617e-04, 5.8315e-04],\n",
      "         [5.0195e-04, 1.1350e-03, 9.9836e-01],\n",
      "         [1.9216e-04, 9.9945e-01, 3.5870e-04]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.9995033740997314\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.7500, 0.0000, 0.2500],\n",
      "         [0.0000, 1.0000, 0.0000],\n",
      "         [0.2500, 0.0000, 0.7500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.3333, 0.1667, 0.5000],\n",
      "         [0.5000, 0.1667, 0.3333],\n",
      "         [0.3333, 0.1667, 0.5000]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.5000, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[1.7922e-05, 1.1473e-05, 9.9997e-01],\n",
      "         [9.9964e-01, 2.3045e-04, 1.2908e-04],\n",
      "         [2.7906e-04, 9.9971e-01, 6.0439e-06]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 0.0003394692321307957\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 1.0000, 0.0000],\n",
      "         [0.2500, 0.0000, 0.7500],\n",
      "         [0.7500, 0.0000, 0.2500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.3333, 0.1667, 0.5000],\n",
      "         [0.3333, 0.1667, 0.5000],\n",
      "         [0.5000, 0.1667, 0.3333]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[9.5683e-01, 4.3047e-02, 1.2639e-04],\n",
      "         [1.7681e-05, 1.1493e-04, 9.9987e-01],\n",
      "         [4.1532e-02, 9.5844e-01, 2.5115e-05]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 0.9600794315338135\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 1.0000, 0.0000],\n",
      "         [0.7500, 0.0000, 0.2500],\n",
      "         [0.2500, 0.0000, 0.7500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.3333, 0.1667, 0.5000],\n",
      "         [0.3333, 0.1667, 0.5000],\n",
      "         [0.5000, 0.1667, 0.3333]]], device='mps:0')\n",
      "q:tensor([[[0.5000, 0.2500, 0.2500],\n",
      "         [0.5000, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[9.7211e-01, 1.0113e-02, 1.7779e-02],\n",
      "         [3.6509e-02, 1.3459e-02, 9.5003e-01],\n",
      "         [3.2721e-04, 9.7102e-01, 2.8657e-02]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.0264475345611572\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.7500, 0.0000, 0.2500],\n",
      "         [0.0000, 1.0000, 0.0000],\n",
      "         [0.2500, 0.0000, 0.7500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.1667, 0.3333, 0.5000],\n",
      "         [0.1667, 0.5000, 0.3333],\n",
      "         [0.3333, 0.5000, 0.1667]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.5000, 0.2500],\n",
      "         [0.5000, 0.2500, 0.2500],\n",
      "         [0.2500, 0.5000, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[2.5817e-03, 3.4859e-04, 9.9707e-01],\n",
      "         [9.8244e-01, 1.9028e-05, 1.7539e-02],\n",
      "         [4.3490e-04, 9.9956e-01, 3.9312e-07]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 0.9883706569671631\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 0.5000, 0.5000],\n",
      "         [0.0000, 0.5000, 0.5000],\n",
      "         [1.0000, 0.0000, 0.0000]]], device='mps:0')\n",
      "  Efficiency Loss: 1.0\n",
      "p:tensor([[[0.1667, 0.5000, 0.3333],\n",
      "         [0.1667, 0.3333, 0.5000],\n",
      "         [0.1667, 0.5000, 0.3333]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[0.0770, 0.7104, 0.2126],\n",
      "         [0.6861, 0.2252, 0.0887],\n",
      "         [0.2325, 0.0550, 0.7125]]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.1414077281951904\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.7500, 0.2500],\n",
      "         [0.0000, 0.2500, 0.7500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.1667, 0.5000, 0.3333],\n",
      "         [0.1667, 0.3333, 0.5000],\n",
      "         [0.1667, 0.3333, 0.5000]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.5000, 0.2500, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[6.3519e-06, 9.9999e-01, 3.0333e-07],\n",
      "         [1.4528e-05, 6.8826e-07, 9.9998e-01],\n",
      "         [9.9986e-01, 4.2616e-05, 9.7387e-05]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: -0.00019484758377075195\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 0.2500, 0.7500],\n",
      "         [1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.7500, 0.2500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.1667, 0.5000, 0.3333],\n",
      "         [0.1667, 0.3333, 0.5000],\n",
      "         [0.1667, 0.5000, 0.3333]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[0.0770, 0.7104, 0.2126],\n",
      "         [0.6861, 0.2252, 0.0887],\n",
      "         [0.2325, 0.0550, 0.7125]]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.1414077281951904\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.7500, 0.2500],\n",
      "         [0.0000, 0.2500, 0.7500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.5000, 0.3333, 0.1667],\n",
      "         [0.3333, 0.5000, 0.1667],\n",
      "         [0.5000, 0.3333, 0.1667]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.2500, 0.5000]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[0.4447, 0.5485, 0.0068],\n",
      "         [0.0019, 0.2851, 0.7129],\n",
      "         [0.7377, 0.0885, 0.1738]]], device='mps:0', grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 0.42594218254089355\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 0.0000, 1.0000],\n",
      "         [0.7500, 0.2500, 0.0000],\n",
      "         [0.2500, 0.7500, 0.0000]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.5000, 0.1667, 0.3333],\n",
      "         [0.5000, 0.1667, 0.3333],\n",
      "         [0.3333, 0.1667, 0.5000]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.5000, 0.2500, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[4.1942e-01, 8.5401e-02, 4.9518e-01],\n",
      "         [1.4230e-01, 1.0537e-01, 7.5233e-01],\n",
      "         [4.0512e-01, 5.9486e-01, 1.9936e-05]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.2238898277282715\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.2500, 0.0000, 0.7500],\n",
      "         [0.0000, 1.0000, 0.0000],\n",
      "         [0.7500, 0.0000, 0.2500]]], device='mps:0')\n",
      "  Efficiency Loss: 1.5\n",
      "p:tensor([[[0.5000, 0.1667, 0.3333],\n",
      "         [0.3333, 0.5000, 0.1667],\n",
      "         [0.5000, 0.3333, 0.1667]]], device='mps:0')\n",
      "q:tensor([[[0.2500, 0.5000, 0.2500],\n",
      "         [0.2500, 0.2500, 0.5000],\n",
      "         [0.2500, 0.5000, 0.2500]]], device='mps:0')\n",
      "\n",
      "Model Results:\n",
      "  Output: tensor([[[8.4426e-01, 2.0286e-03, 1.5371e-01],\n",
      "         [4.6736e-03, 1.2596e-01, 8.6937e-01],\n",
      "         [1.1794e-01, 8.8157e-01, 4.8240e-04]]], device='mps:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "  Efficiency Loss: 1.783710241317749\n",
      "\n",
      "DA Results:\n",
      "  Output: tensor([[[0.0000, 0.0000, 1.0000],\n",
      "         [0.5000, 0.5000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000]]], device='mps:0')\n",
      "  Efficiency Loss: 1.0\n"
     ]
    }
   ],
   "source": [
    "preference_list_p = list(itertools.product(normalize_tuples(no_tie_patterns), repeat=3))\n",
    "preference_list_q = list(itertools.product(normalize_tuples(one_tie_2_2), repeat=3))\n",
    "\n",
    "for i in range(500):\n",
    "    random.seed(i)\n",
    "    p=random.choice(preference_list_p)\n",
    "    q=random.choice(preference_list_q)\n",
    "\n",
    "    p = torch.tensor([p]).to(device)\n",
    "    q = torch.tensor([q]).to(device)\n",
    "\n",
    "    model_output = model(p, q)\n",
    "    model_efficiency_loss = compute_efficiency_loss(cfg, model_output, p, q)\n",
    "    #model_stability_loss = compute_t(model_output, p, q).mean()\n",
    "    #model_sp_loss = compute_spv_w(cfg, model, model_output, p, q).mean()\n",
    "\n",
    "    da_output = da_with_t(p, q)\n",
    "    da_efficiency_loss = compute_efficiency_loss(cfg, da_output, p, q)\n",
    "    #da_stability_loss = compute_t(da_output, p, q).mean()\n",
    "    #da_sp_loss = compute_spv_w(cfg, da_with_t, da_output, p, q).mean()\n",
    "\n",
    "    is_fract = False\n",
    "    for pi in range(2):\n",
    "        for qi in range(2):\n",
    "            if model_output[0][pi][qi] > 0.1 and model_output[0][pi][qi]<0.9 :\n",
    "                is_fract = True\n",
    "                break\n",
    "    \n",
    "    is_not_efficient= False\n",
    "    if da_efficiency_loss > 0:\n",
    "        is_not_efficient= True\n",
    "\n",
    "    is_different = False\n",
    "    for pi in range(2):\n",
    "        for qi in range(2):\n",
    "            difference = model_output[0][pi][qi] - da_output[0][pi][qi]\n",
    "            if difference < -0.7 or difference>0.7 :\n",
    "                is_different = True\n",
    "                break\n",
    "\n",
    "    \n",
    "    if is_not_efficient and is_different:\n",
    "        print(f\"p:{p}\")\n",
    "        print(f\"q:{q}\")\n",
    "        print(\"\\nModel Results:\")\n",
    "        print(f\"  Output: {model_output}\")\n",
    "        print(f\"  Efficiency Loss: {model_efficiency_loss}\")\n",
    "        #print(f\"  Stability Loss: {model_stability_loss}\")\n",
    "        #print(f\"  SP Loss: {model_sp_loss}\")\n",
    "\n",
    "        print(\"\\nDA Results:\")\n",
    "        print(f\"  Output: {da_output}\")\n",
    "        print(f\"  Efficiency Loss: {da_efficiency_loss}\")\n",
    "        #print(f\"  Stability Loss: {da_stability_loss}\")\n",
    "        #print(f\"  SP Loss: {da_sp_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
